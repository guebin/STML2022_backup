{
  
    
        "post0": {
            "title": "(6주차) 10월12일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . imports . import torch import torchvision from fastai.data.all import * import matplotlib.pyplot as plt . import graphviz def gv(s): return graphviz.Source(&#39;digraph G{ rankdir=&quot;LR&quot;&#39;+s + &#39;; }&#39;); . &#49884;&#48292;&#53076;&#51221;&#47532; . &#51648;&#45212;&#49884;&#44036; &#45436;&#47532;&#51204;&#44060; . - 아이디어: linear -&gt; relu -&gt; linear (-&gt; sigmoid) 조합으로 꺽은선으로 표현되는 underlying 을 표현할 수 있었다. . 아이디어의 실용성: 실제자료에서 꺽은선으로 표현되는 underlying은 몇개 없을 것 같음. 그건 맞는데 꺽이는 점을 많이 설정하면 얼추 비슷하게는 &quot;근사&quot; 시킬 수 있음. | 아이디어의 확장성: 이러한 논리전개는 X:(n,2)인 경우도 가능했음. (이 경우 꺽인선은 꺽인평면이 된다) | 아이디어에 해당하는 용어정리: : 이 구조가 x-&gt;y 로 바로 가는 것이 아니라 x-&gt;(u1-&gt;v1)-&gt;(u2-&gt;v2)=y 의 구조인데 이러한 네트워크를 하나의 은닉층을 포함하는 네트워크라고 표현한다. | . &#49884;&#48292;&#53076;&#51221;&#47532; . universal approximation thm: (범용근사정리,보편근사정리,시벤코정리), 1989 . 하나의 은닉층을 가지는 &quot;linear -&gt; sigmoid -&gt; linear&quot; 꼴의 네트워크를 이용하여 세상에 존재하는 모든 (다차원) 연속함수를 원하는 정확도로 근사시킬 수 있다. (계수를 잘 추정한다면) . - 사실 엄청 이해안되는 정리임. 왜냐햐면, . 그렇게 잘 맞추면 1989년에 세상의 모든 문제를 다 풀어야 한거 아니야? | 요즘은 &quot;linear -&gt; sigmoid -&gt; linear&quot; 가 아니라 &quot;linear -&gt; relu -&gt; linear&quot; 조합으로 많이 쓰던데? | 요즘은 하나의 은닉층을 포함하는 네트워크는 잘 안쓰지 않나? 은닉층이 여러개일수록 좋다고 어디서 본 것 같은데? | . - 약간의 의구심이 있지만 아무튼 universal approximation thm에 따르면 우리는 아래와 같은 무기를 가진 꼴이 된다. . 우리의 무기: ${ bf X}: (n,p)$ 꼴의 입력에서 ${ bf y}:(n,1)$ 꼴의 출력으로 향하는 맵핑을 &quot;linear -&gt; relu -&gt; linear&quot;와 같은 네트워크를 이용해서 &quot;근사&quot;시킬 수 있다. | . &#49884;&#48292;&#53076;&#51221;&#47532; proof . &#44536;&#47548;&#51004;&#47196; &#48372;&#45716; &#51613;&#47749;&#44284;&#51221; . - 데이터 . x = torch.linspace(-10,10,200).reshape(-1,1) . - 아래와 같은 네트워크를 고려하자. . l1 = torch.nn.Linear(in_features=1,out_features=2) a1 = torch.nn.Sigmoid() l2 = torch.nn.Linear(in_features=2,out_features=1) . - 직관1: $l_1$,$l_2$의 가중치를 잘 결합하다보면 우연히 아래와 같이 만들 수 있다. . l1.weight.data = torch.tensor([[-5.00],[5.00]]) l1.bias.data = torch.tensor([+10.00,+10.00]) . l2.weight.data = torch.tensor([[1.00,1.00]]) l2.bias.data = torch.tensor([-1.00]) . fig,ax = plt.subplots(1,3,figsize=(9,3)) ax[0].plot(x,l1(x).data); ax[0].set_title(&#39;$l_1(x)$&#39;) ax[1].plot(x,a1(l1(x)).data); ax[1].set_title(&#39;$(a_1 circ l_1)(x)$&#39;) ax[2].plot(x,l2(a1(l1(x))).data,color=&#39;C2&#39;); ax[2].set_title(&#39;$(l_2 circ a_1 circ l_1)(x)$&#39;) . Text(0.5, 1.0, &#39;$(l_2 circ a_1 circ l_1)(x)$&#39;) . - 직관2: 아래들도 가능할듯? . l1.weight.data = torch.tensor([[-5.00],[5.00]]) l1.bias.data = torch.tensor([+0.00,+20.00]) l2.weight.data = torch.tensor([[1.00,1.00]]) l2.bias.data = torch.tensor([-1.00]) fig,ax = plt.subplots(1,3,figsize=(9,3)) ax[0].plot(x,l1(x).data,&#39;--&#39;,color=&#39;C0&#39;); ax[0].set_title(&#39;$l_1(x)$&#39;) ax[1].plot(x,a1(l1(x)).data,&#39;--&#39;,color=&#39;C0&#39;); ax[1].set_title(&#39;$(a_1 circ l_1)(x)$&#39;) ax[2].plot(x,l2(a1(l1(x))).data,&#39;--&#39;,color=&#39;C0&#39;); ax[2].set_title(&#39;$(l_2 circ a_1 circ l_1)(x)$&#39;); . l1.weight.data = torch.tensor([[-5.00],[5.00]]) l1.bias.data = torch.tensor([+20.00,+0.00]) l2.weight.data = torch.tensor([[2.50,2.50]]) l2.bias.data = torch.tensor([-2.50]) ax[0].plot(x,l1(x).data,&#39;--&#39;,color=&#39;C1&#39;); ax[0].set_title(&#39;$l_1(x)$&#39;) ax[1].plot(x,a1(l1(x)).data,&#39;--&#39;,color=&#39;C1&#39;); ax[1].set_title(&#39;$(a_1 circ l_1)(x)$&#39;) ax[2].plot(x,l2(a1(l1(x))).data,&#39;--&#39;,color=&#39;C1&#39;); ax[2].set_title(&#39;$(l_2 circ a_1 circ l_1)(x)$&#39;); fig . - 은닉층의노드수=4로 하고 적당한 가중치를 조정하면 $(l_2 circ a_1 circ l_1)(x)$의 결과로 주황색선 + 파란색선도 가능할 것 같다. $ to$ 실제로 가능함 . l1 = torch.nn.Linear(in_features=1,out_features=4) a1 = torch.nn.Sigmoid() l2 = torch.nn.Linear(in_features=4,out_features=1) . l1.weight.data = torch.tensor([[-5.00],[5.00],[-5.00],[5.00]]) l1.bias.data = torch.tensor([0.00, 20.00, 20.00, 0]) l2.weight.data = torch.tensor([[1.00, 1.00, 2.50, 2.50]]) l2.bias.data = torch.tensor([-1.0-2.5]) . plt.plot(l2(a1(l1(x))).data) . [&lt;matplotlib.lines.Line2D at 0x7f62d815a690&gt;] . - 2개의 시그모이드를 우연히 잘 결합하면 아래와 같은 함수 $h$를 만들 수 있다. . h = lambda x: torch.sigmoid(200*(x+0.5))+torch.sigmoid(-200*(x-0.5))-1.0 . plt.plot(x,h(x)) plt.title(&quot;$h(x)$&quot;) . Text(0.5, 1.0, &#39;$h(x)$&#39;) . - 위와 같은 함수 $h$를 활성화함수로 하고 $m$개의 노드를 가지는 은닉층을 생각해보자. 이러한 은닉층을 사용한다면 전체 네트워크를 아래와 같이 표현할 수 있다. . $ underset{(n,1)}{ bf X} overset{l_1}{ to} underset{(n,m)}{ boldsymbol u^{(1)}} overset{h}{ to} underset{(n,m)}{ boldsymbol v^{(1)}} overset{l_2}{ to} underset{(n,1)}{ hat{ boldsymbol y}}$ . 그리고 위의 네트워크와 동일한 효과를 주는 아래의 네트워크가 항상 존재함. . $ underset{(n,1)}{ bf X} overset{l_1}{ to} underset{(n,2m)}{ boldsymbol u^{(1)}} overset{sig}{ to} underset{(n,2m)}{ boldsymbol v^{(1)}} overset{l_2}{ to} underset{(n,1)}{ hat{ boldsymbol y}}$ . - $h(x)$를 활성화함수로 가지는 네트워크를 설계하여 보자. . class MyActivation(torch.nn.Module): ## 사용자정의 활성화함수를 선언하는 방법 def __init__(self): super().__init__() def forward(self, input): return h(input) # activation 의 출력 . a1=MyActivation() # a1 = torch.nn.Sigmoid(), a1 = torch.nn.ReLU() 대신에 a1 = MyActivation() . plt.plot(x,a1(x)) . [&lt;matplotlib.lines.Line2D at 0x7f62d8043b10&gt;] . 히든레이어가 1개의 노드를 가지는 경우 . torch.manual_seed(43052) fig, ax = plt.subplots(4,4,figsize=(12,12)) for i in range(4): for j in range(4): net = torch.nn.Sequential( torch.nn.Linear(1,1), MyActivation(), torch.nn.Linear(1,1) ) ax[i,j].plot(x,net(x).data,&#39;--&#39;) . 히든레이어가 2개의 노드를 가지는 경우 . torch.manual_seed(43052) fig, ax = plt.subplots(4,4,figsize=(12,12)) for i in range(4): for j in range(4): net = torch.nn.Sequential( torch.nn.Linear(1,2), MyActivation(), torch.nn.Linear(2,1) ) ax[i,j].plot(x,net(x).data,&#39;--&#39;) . 히든레이어가 3개의 노드를 가지는 경우 . torch.manual_seed(43052) fig, ax = plt.subplots(4,4,figsize=(12,12)) for i in range(4): for j in range(4): net = torch.nn.Sequential( torch.nn.Linear(1,3), MyActivation(), torch.nn.Linear(3,1) ) ax[i,j].plot(x,net(x).data,&#39;--&#39;) . 히든레이어가 1024개의 노드를 가지는 경우 . torch.manual_seed(43052) fig, ax = plt.subplots(4,4,figsize=(12,12)) for i in range(4): for j in range(4): net = torch.nn.Sequential( torch.nn.Linear(1,1024), MyActivation(), torch.nn.Linear(1024,1) ) ax[i,j].plot(x,net(x).data,&#39;--&#39;) . &#49884;&#48292;&#53076;&#51221;&#47532; &#54876;&#50857; . - 아래와 같이 하나의 은닉층을 가지고 있더라도 많은 노드수만 보장되면 매우 충분한 표현력을 가짐 . $ underset{(n,1)}{ bf X} overset{l_1}{ to} underset{(n,m)}{ boldsymbol u^{(1)}} overset{h}{ to} underset{(n,m)}{ boldsymbol v^{(1)}} overset{l_2}{ to} underset{(n,1)}{ hat{ boldsymbol y}}$ . &#50696;&#51228;1 (sin, exp) . torch.manual_seed(43052) x = torch.linspace(-10,10,200).reshape(-1,1) underlying = torch.sin(2*x) + torch.sin(0.5*x) + torch.exp(-0.2*x) eps = torch.randn(200).reshape(-1,1)*0.1 y = underlying + eps plt.plot(x,y,&#39;o&#39;,alpha=0.5) plt.plot(x,underlying,lw=3) . [&lt;matplotlib.lines.Line2D at 0x7f62d9f82550&gt;] . h = lambda x: torch.sigmoid(200*(x+0.5))+torch.sigmoid(-200*(x-0.5))-1.0 class MyActivation(torch.nn.Module): ## 사용자정의 활성화함수를 선언하는 방법 def __init__(self): super().__init__() def forward(self, input): return h(input) . net= torch.nn.Sequential( torch.nn.Linear(1,2048), MyActivation(), torch.nn.Linear(2048,1) ) loss_fn = torch.nn.MSELoss() optimizr = torch.optim.Adam(net.parameters()) . for epoc in range(200): ## 1 yhat = net(x) ## 2 loss = loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizr.step() optimizr.zero_grad() . plt.plot(x,y,&#39;o&#39;,alpha=0.2) plt.plot(x,underlying,lw=3) plt.plot(x,net(x).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f62d9bc3d90&gt;] . &#50696;&#51228;2 (&#49828;&#54169;&#45458;&#50500;&#46020; &#52712;&#50629;X) . df=pd.read_csv(&#39;https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-10-04-dnnex0.csv&#39;) df . x underlying y . 0 -1.000000 | 0.000045 | 0.0 | . 1 -0.998999 | 0.000046 | 0.0 | . 2 -0.997999 | 0.000047 | 0.0 | . 3 -0.996998 | 0.000047 | 0.0 | . 4 -0.995998 | 0.000048 | 0.0 | . ... ... | ... | ... | . 1995 0.995998 | 0.505002 | 0.0 | . 1996 0.996998 | 0.503752 | 0.0 | . 1997 0.997999 | 0.502501 | 0.0 | . 1998 0.998999 | 0.501251 | 1.0 | . 1999 1.000000 | 0.500000 | 1.0 | . 2000 rows × 3 columns . x = torch.tensor(df.x).reshape(-1,1).float() y = torch.tensor(df.y).reshape(-1,1).float() plt.plot(x,y,&#39;o&#39;,alpha=0.1) plt.plot(df.x,df.underlying,lw=3) . [&lt;matplotlib.lines.Line2D at 0x7f62d97965d0&gt;] . h = lambda x: torch.sigmoid(200*(x+0.5))+torch.sigmoid(-200*(x-0.5))-1.0 class MyActivation(torch.nn.Module): ## 사용자정의 활성화함수를 선언하는 방법 def __init__(self): super().__init__() def forward(self, input): return h(input) . torch.manual_seed(43052) net= torch.nn.Sequential( torch.nn.Linear(1,2048), MyActivation(), torch.nn.Linear(2048,1), torch.nn.Sigmoid() ) loss_fn = torch.nn.BCELoss() optimizr = torch.optim.Adam(net.parameters()) . for epoc in range(100): ## 1 yhat = net(x) ## 2 loss = loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizr.step() optimizr.zero_grad() . plt.plot(x,y,&#39;o&#39;,alpha=0.2) plt.plot(df.x,df.underlying,lw=3) plt.plot(x,net(x).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f62d971fd90&gt;] . &#50696;&#51228;3 (MNIST data with DNN) . # &#50696;&#48708;&#54617;&#49845; . (예비학습1) Path . path = untar_data(URLs.MNIST) path . Path(&#39;/home/cgb4/.fastai/data/mnist_png&#39;) . path 도 오브젝트임 | path 도 정보+기능이 있음 | . - path의 정보 . path._str # 숨겨놓았네? . &#39;/home/cgb4/.fastai/data/mnist_png&#39; . - 기능1 . path.ls() . (#2) [Path(&#39;/home/cgb4/.fastai/data/mnist_png/training&#39;),Path(&#39;/home/cgb4/.fastai/data/mnist_png/testing&#39;)] . - 기능2 . path/&#39;training&#39; . Path(&#39;/home/cgb4/.fastai/data/mnist_png/training&#39;) . path/&#39;testing&#39; . Path(&#39;/home/cgb4/.fastai/data/mnist_png/testing&#39;) . - 기능1과 기능2의 결합 . (path/&#39;training/3&#39;).ls() . (#6131) [Path(&#39;/home/cgb4/.fastai/data/mnist_png/training/3/37912.png&#39;),Path(&#39;/home/cgb4/.fastai/data/mnist_png/training/3/12933.png&#39;),Path(&#39;/home/cgb4/.fastai/data/mnist_png/training/3/3576.png&#39;),Path(&#39;/home/cgb4/.fastai/data/mnist_png/training/3/59955.png&#39;),Path(&#39;/home/cgb4/.fastai/data/mnist_png/training/3/23144.png&#39;),Path(&#39;/home/cgb4/.fastai/data/mnist_png/training/3/40836.png&#39;),Path(&#39;/home/cgb4/.fastai/data/mnist_png/training/3/25536.png&#39;),Path(&#39;/home/cgb4/.fastai/data/mnist_png/training/3/42669.png&#39;),Path(&#39;/home/cgb4/.fastai/data/mnist_png/training/3/7046.png&#39;),Path(&#39;/home/cgb4/.fastai/data/mnist_png/training/3/47380.png&#39;)...] . &#39;/home/cgb4/.fastai/data/mnist_png/training/3/37912.png&#39; 이 파일을 더블클릭하면 이미지가 보인단 말임 | . (예비학습2) plt.imshow . imgtsr = torch.tensor([[1.0,2],[2.0,4.0]]) imgtsr . tensor([[1., 2.], [2., 4.]]) . plt.imshow(imgtsr,cmap=&#39;gray&#39;) plt.colorbar() . &lt;matplotlib.colorbar.Colorbar at 0x7fceac108e50&gt; . (예비학습3) torchvision . - &#39;/home/cgb4/.fastai/data/mnist_png/training/3/37912.png&#39;의 이미지파일을 torchvision.io.read_image 를 이용하여 텐서로 만듬 . imgtsr = torchvision.io.read_image(&#39;/home/cgb4/.fastai/data/mnist_png/training/3/37912.png&#39;) imgtsr . tensor([[[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17, 66, 138, 149, 180, 138, 138, 86, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 22, 162, 161, 228, 252, 252, 253, 252, 252, 252, 252, 74, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 116, 253, 252, 252, 252, 189, 184, 110, 119, 252, 252, 32, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 74, 161, 160, 77, 45, 4, 0, 0, 70, 252, 210, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 22, 205, 252, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 162, 253, 245, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 36, 219, 252, 139, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 222, 252, 202, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 43, 253, 252, 89, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 85, 240, 253, 157, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 160, 253, 231, 42, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 142, 252, 252, 42, 30, 78, 161, 36, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 184, 252, 252, 185, 228, 252, 252, 168, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 184, 252, 252, 253, 252, 252, 252, 116, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 101, 179, 252, 253, 252, 252, 210, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 22, 255, 253, 215, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 34, 89, 244, 253, 223, 98, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 116, 123, 142, 234, 252, 252, 184, 67, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 230, 253, 252, 252, 252, 168, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 126, 253, 252, 168, 43, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]], dtype=torch.uint8) . - 이 텐서는 (1,28,28)의 shape을 가짐 . imgtsr.shape . torch.Size([1, 28, 28]) . - imgtsr를 plt.imshow 로 시각화 . plt.imshow(imgtsr.reshape(28,28),cmap=&#39;gray&#39;) . &lt;matplotlib.image.AxesImage at 0x7fceabd49a90&gt; . 진짜 숫자3이 있음 | . # &#45936;&#51060;&#53552;&#51221;&#47532; . - 데이터정리 . threes = (path/&#39;training/3&#39;).ls() sevens = (path/&#39;training/7&#39;).ls() len(threes),len(sevens) . (6131, 6265) . X3 = torch.stack([torchvision.io.read_image(str(threes[i])) for i in range(6131)]) X7 = torch.stack([torchvision.io.read_image(str(sevens[i])) for i in range(6265)]) . X3.shape,X7.shape . (torch.Size([6131, 1, 28, 28]), torch.Size([6265, 1, 28, 28])) . X=torch.concat([X3,X7]) X.shape . torch.Size([12396, 1, 28, 28]) . Xnp = X.reshape(-1,1*28*28).float() Xnp.shape . torch.Size([12396, 784]) . y = torch.tensor([0.0]*6131 + [1.0]*6265).reshape(-1,1) y.shape . torch.Size([12396, 1]) . plt.plot(y,&#39;o&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fceab1cde50&gt;] . &quot;y=0&quot;은 숫자3을 의미, &quot;y=1&quot;은 숫자7을 의미 | 숫자3은 6131개, 숫자7은 6265개 있음 | . # &#54617;&#49845; . - 네트워크의 설계 . torch.manual_seed(43052) net = torch.nn.Sequential( torch.nn.Linear(in_features=1*28*28,out_features=30), torch.nn.ReLU(), torch.nn.Linear(in_features=30,out_features=1), torch.nn.Sigmoid() ) . $ underset{(n,784)}{ bf X} overset{l_1}{ to} underset{(n,30)}{ boldsymbol u^{(1)}} overset{a_1}{ to} underset{(n,30)}{ boldsymbol v^{(1)}} overset{l_1}{ to} underset{(n,1)}{ boldsymbol u^{(2)}} overset{a_2}{ to} underset{(n,1)}{ boldsymbol v^{(2)}}= underset{(n,1)}{ hat{ boldsymbol y}}$ | . loss_fn = torch.nn.BCELoss() . optimizr = torch.optim.Adam(net.parameters()) . for epoc in range(200): ## 1 yhat = net(Xnp) ## 2 loss = loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizr.step() optimizr.zero_grad() . plt.plot(y,&#39;o&#39;) plt.plot(net(Xnp).data,&#39;.&#39;,alpha=0.2) . [&lt;matplotlib.lines.Line2D at 0x7fceab06af50&gt;] . 대부분 잘 적합되었음 | . &#49888;&#44221;&#47581;&#51032; &#54364;&#54788; (${ boldsymbol x} to hat{ boldsymbol y}$ &#47196; &#44032;&#45716; &#44284;&#51221;&#51012; &#44536;&#47548;&#51004;&#47196; &#54364;&#54788;) . &#50696;&#51228;1: $ underset{(n,1)}{ bf X} overset{l_1}{ to} underset{(n,1)}{ boldsymbol u^{(1)}} overset{sig}{ to} underset{(n,1)}{ boldsymbol v^{(1)}} = underset{(n,1)}{ hat{ boldsymbol y}}$ . - 모든 observation과 가중치를 명시한 버전 . (표현1) . gv(&#39;&#39;&#39; &quot;1&quot; -&gt; &quot;ŵ₀ + xₙ*ŵ₁, bias=False&quot;[label=&quot;* ŵ₀&quot;] &quot;xₙ&quot; -&gt; &quot;ŵ₀ + xₙ*ŵ₁, bias=False&quot;[label=&quot;* ŵ₁&quot;] &quot;ŵ₀ + xₙ*ŵ₁, bias=False&quot; -&gt; &quot;ŷₙ&quot;[label=&quot;sigmoid&quot;] &quot;.&quot; -&gt; &quot;....................................&quot;[label=&quot;* ŵ₀&quot;] &quot;..&quot; -&gt; &quot;....................................&quot;[label=&quot;* ŵ₁&quot;] &quot;....................................&quot; -&gt; &quot;...&quot;[label=&quot; &quot;] &quot;1 &quot; -&gt; &quot;ŵ₀ + x₂*ŵ₁, bias=False&quot;[label=&quot;* ŵ₀&quot;] &quot;x₂&quot; -&gt; &quot;ŵ₀ + x₂*ŵ₁, bias=False&quot;[label=&quot;* ŵ₁&quot;] &quot;ŵ₀ + x₂*ŵ₁, bias=False&quot; -&gt; &quot;ŷ₂&quot;[label=&quot;sigmoid&quot;] &quot;1 &quot; -&gt; &quot;ŵ₀ + x₁*ŵ₁, bias=False&quot;[label=&quot;* ŵ₀&quot;] &quot;x₁&quot; -&gt; &quot;ŵ₀ + x₁*ŵ₁, bias=False&quot;[label=&quot;* ŵ₁&quot;] &quot;ŵ₀ + x₁*ŵ₁, bias=False&quot; -&gt; &quot;ŷ₁&quot;[label=&quot;sigmoid&quot;] &#39;&#39;&#39;) . . 단점: 똑같은 그림의 반복이 너무 많음 | . - observation 반복을 생략한 버전들 . (표현2) 모든 $i$에 대하여 아래의 그림을 반복한다고 하면 (표현1)과 같다. . gv(&#39;&#39;&#39; &quot;1&quot; -&gt; &quot;ŵ₀ + xᵢ*ŵ₁, bias=False&quot;[label=&quot;* ŵ₀&quot;] &quot;xᵢ&quot; -&gt; &quot;ŵ₀ + xᵢ*ŵ₁, bias=False&quot;[label=&quot;* ŵ₁&quot;] &quot;ŵ₀ + xᵢ*ŵ₁, bias=False&quot; -&gt; &quot;ŷᵢ&quot;[label=&quot;sigmoid&quot;] &#39;&#39;&#39;) . . (표현3) 그런데 (표현2)에서 아래와 같이 $x_i$, $y_i$ 대신에 간단히 $x$, $y$로 쓰는 경우도 많음 . gv(&#39;&#39;&#39; &quot;1&quot; -&gt; &quot;ŵ₀ + x*ŵ₁, bias=False&quot;[label=&quot;* ŵ₀&quot;] &quot;x&quot; -&gt; &quot;ŵ₀ + x*ŵ₁, bias=False&quot;[label=&quot;* ŵ₁&quot;] &quot;ŵ₀ + x*ŵ₁, bias=False&quot; -&gt; &quot;ŷ&quot;[label=&quot;sigmoid&quot;] &#39;&#39;&#39;) . . - 1을 생략한 버전들 . (표현4) bais=False 대신에 bias=True를 주면 1을 생략할 수 있음 . gv(&#39;&#39;&#39; &quot;x&quot; -&gt; &quot;x*ŵ₁, bias=True&quot;[label=&quot;*ŵ₁&quot;] ; &quot;x*ŵ₁, bias=True&quot; -&gt; &quot;ŷ&quot;[label=&quot;sigmoid&quot;] &#39;&#39;&#39;) . . (표현4의 수정) $ hat{w}_1$대신에 $ hat{w}$를 쓰는 것이 더 자연스러움 . gv(&#39;&#39;&#39; &quot;x&quot; -&gt; &quot;x*ŵ, bias=True&quot;[label=&quot;*ŵ&quot;] ; &quot;x*ŵ, bias=True&quot; -&gt; &quot;ŷ&quot;[label=&quot;sigmoid&quot;] &#39;&#39;&#39;) . . (표현5) 선형변환의 결과는 아래와 같이 $u$로 표현하기도 한다. . gv(&#39;&#39;&#39; &quot;x&quot; -&gt; &quot;u&quot;; &quot;u&quot; -&gt; &quot;y&quot;[label=&quot;sigmoid&quot;] &#39;&#39;&#39;) . . 다이어그램은 그리는 사람의 취향에 따라 그리는 방법이 조금씩 다릅니다. 즉 교재마다 달라요. . &#50696;&#51228;2: $ underset{(n,1)}{ bf X} overset{l_1}{ to} underset{(n,2)}{ boldsymbol u^{(1)}} overset{relu}{ to} underset{(n,2)}{ boldsymbol v^{(1)}} overset{l_2}{ to} underset{(n,1)}{ boldsymbol u^{(2)}} overset{sig}{ to} underset{(n,1)}{ boldsymbol v^{(2)}} = underset{(n,1)}{ hat{ boldsymbol y}}$ . 참고: 코드로 표현 . torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=2), torch.nn.ReLU(), torch.nn.Linear(in_features=2,out_features=1), torch.nn.Sigmoid() ) . - 이해를 위해서 10월4일 강의노트에서 다루었던 아래의 상황을 고려하자. . . (강의노트의 표현) . gv(&#39;&#39;&#39; &quot;x&quot; -&gt; &quot; -x&quot;[label=&quot;*(-1)&quot;]; &quot;x&quot; -&gt; &quot; x&quot;[label=&quot;*1&quot;] &quot; x&quot; -&gt; &quot;rlu(x)&quot;[label=&quot;relu&quot;] &quot; -x&quot; -&gt; &quot;rlu(-x)&quot;[label=&quot;relu&quot;] &quot;rlu(x)&quot; -&gt; &quot;u&quot;[label=&quot;*(-4.5)&quot;] &quot;rlu(-x)&quot; -&gt; &quot;u&quot;[label=&quot;*(-9.0)&quot;] &quot;u&quot; -&gt; &quot;sig(u)=yhat&quot;[label=&quot;sig&quot;] &#39;&#39;&#39; ) . . (좀 더 일반화된 표현) 10월4일 강의노트 상황을 일반화하면 아래와 같다. . gv(&#39;&#39;&#39; &quot;x&quot; -&gt; &quot;u1[:,0]&quot;[label=&quot;*(-1)&quot;]; &quot;x&quot; -&gt; &quot;u1[:,1]&quot;[label=&quot;*1&quot;] &quot;u1[:,0]&quot; -&gt; &quot;v1[:,0]&quot;[label=&quot;relu&quot;] &quot;u1[:,1]&quot; -&gt; &quot;v1[:,1]&quot;[label=&quot;relu&quot;] &quot;v1[:,0]&quot; -&gt; &quot;u2&quot;[label=&quot;*(-9.0)&quot;] &quot;v1[:,1]&quot; -&gt; &quot;u2&quot;[label=&quot;*(-4.5)&quot;] &quot;u2&quot; -&gt; &quot;v2=yhat&quot;[label=&quot;sig&quot;] &#39;&#39;&#39; ) . . * Layer의 개념: ${ bf X}$에서 $ hat{ boldsymbol y}$로 가는 과정은 &quot;선형변환+비선형변환&quot;이 반복되는 구조이다. &quot;선형변환+비선형변환&quot;을 하나의 세트로 보면 아래와 같이 표현할 수 있다. . $ underset{(n,1)}{ bf X} overset{l_1}{ to} left( underset{(n,2)}{ boldsymbol u^{(1)}} overset{relu}{ to} underset{(n,2)}{ boldsymbol v^{(1)}} right) overset{l_2}{ to} left( underset{(n,1)}{ boldsymbol u^{(2)}} overset{sig}{ to} underset{(n,1)}{ boldsymbol v^{(2)}} right), quad underset{(n,1)}{ boldsymbol v^{(2)}}= underset{(n,1)}{net({ bf X})}= underset{(n,1)}{ hat{ boldsymbol y}}$ . 이것을 다이어그램으로 표현한다면 아래와 같다. . (선형+비선형을 하나의 Layer로 묶은 표현) . gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; &quot;X&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;X&quot; -&gt; &quot;u1[:,0]&quot; &quot;X&quot; -&gt; &quot;u1[:,1]&quot; &quot;u1[:,0]&quot; -&gt; &quot;v1[:,0]&quot;[label=&quot;relu&quot;] &quot;u1[:,1]&quot; -&gt; &quot;v1[:,1]&quot;[label=&quot;relu&quot;] label = &quot;Layer 1&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;v1[:,0]&quot; -&gt; &quot;u2&quot; &quot;v1[:,1]&quot; -&gt; &quot;u2&quot; &quot;u2&quot; -&gt; &quot;v2=yhat&quot;[label=&quot;sigmoid&quot;] label = &quot;Layer 2&quot; } &#39;&#39;&#39;) . . Layer를 세는 방법 . 정석: 학습가능한 파라메터가 몇층으로 있는지... | 일부 교재 설명: 입력층은 계산하지 않음, activation layer는 계산하지 않음. | 위의 예제의 경우 number of layer = 2 이다. | . 사실 input layer, activation layer 등의 표현을 자주 사용해서 layer를 세는 방법이 처음에는 헷갈립니다.. . Hidden Layer의 수를 세는 방법 . Layer의 수 = Hidden Layer의 수 + 출력층의 수 = Hidden Layer의 수 + 1 | 위의 예제의 경우 number of hidden layer = 1 이다. | . * node의 개념: $u to v$로 가는 쌍을 간단히 노드라는 개념을 이용하여 나타낼 수 있음. . (노드의 개념이 포함된 그림) . gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; &quot;X&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;X&quot; -&gt; &quot;node1&quot; &quot;X&quot; -&gt; &quot;node2&quot; label = &quot;Layer 1:relu&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;node1&quot; -&gt; &quot;yhat &quot; &quot;node2&quot; -&gt; &quot;yhat &quot; label = &quot;Layer 2:sigmoid&quot; } &#39;&#39;&#39;) . . 여기에서 node의 숫자 = feature의 숫자와 같이 이해할 수 있다. 즉 아래와 같이 이해할 수 있다. . (&quot;number of nodes = number of features&quot;로 이해한 그림) . gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; &quot;X&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;X&quot; -&gt; &quot;feature1&quot; &quot;X&quot; -&gt; &quot;feature2&quot; label = &quot;Layer 1:relu&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;feature1&quot; -&gt; &quot;yhat &quot; &quot;feature2&quot; -&gt; &quot;yhat &quot; label = &quot;Layer 2:sigmoid&quot; } &#39;&#39;&#39;) . . 다이어그램의 표현방식은 교재마다 달라서 모든 예시를 달달 외울 필요는 없습니다. 다만 임의의 다이어그램을 보고 대응하는 네트워크를 pytorch로 구현하는 능력은 매우 중요합니다. . &#50696;&#51228;3: $ underset{(n,784)}{ bf X} overset{l_1}{ to} underset{(n,32)}{ boldsymbol u^{(1)}} overset{relu}{ to} underset{(n,32)}{ boldsymbol v^{(1)}} overset{l_1}{ to} underset{(n,1)}{ boldsymbol u^{(2)}} overset{sig}{ to} underset{(n,1)}{ boldsymbol v^{(2)}}= underset{(n,1)}{ hat{ boldsymbol y}}$ . (다이어그램표현) . gv(&#39;&#39;&#39; splines=line subgraph cluster_1{ style=filled; color=lightgrey; &quot;x1&quot; &quot;x2&quot; &quot;..&quot; &quot;x784&quot; label = &quot;Input Layer&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;x1&quot; -&gt; &quot;node1&quot; &quot;x2&quot; -&gt; &quot;node1&quot; &quot;..&quot; -&gt; &quot;node1&quot; &quot;x784&quot; -&gt; &quot;node1&quot; &quot;x1&quot; -&gt; &quot;node2&quot; &quot;x2&quot; -&gt; &quot;node2&quot; &quot;..&quot; -&gt; &quot;node2&quot; &quot;x784&quot; -&gt; &quot;node2&quot; &quot;x1&quot; -&gt; &quot;...&quot; &quot;x2&quot; -&gt; &quot;...&quot; &quot;..&quot; -&gt; &quot;...&quot; &quot;x784&quot; -&gt; &quot;...&quot; &quot;x1&quot; -&gt; &quot;node32&quot; &quot;x2&quot; -&gt; &quot;node32&quot; &quot;..&quot; -&gt; &quot;node32&quot; &quot;x784&quot; -&gt; &quot;node32&quot; label = &quot;Hidden Layer: relu&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;node1&quot; -&gt; &quot;yhat&quot; &quot;node2&quot; -&gt; &quot;yhat&quot; &quot;...&quot; -&gt; &quot;yhat&quot; &quot;node32&quot; -&gt; &quot;yhat&quot; label = &quot;Outplut Layer: sigmoid&quot; } &#39;&#39;&#39;) . . Layer0,1,2 대신에 Input Layer, Hidden Layer, Output Layer로 표현함 | . - 위의 다이어그램에 대응하는 코드 . net = torch.nn.Sequential( torch.nn.Linear(in_features=28*28*1,out_features=32), torch.nn.ReLU(), torch.nn.Linear(in_features=32,out_features=1), torch.nn.Sigmoid() ) . CPU vs GPU . - 파이토치에서 GPU를 쓰는 방법을 알아보자. (사실 지금까지 우리는 CPU만 쓰고 있었음) . GPU &#49324;&#50857;&#48169;&#48277; . - cpu 연산이 가능한 메모리에 데이터 저장 . torch.manual_seed(43052) x_cpu = torch.tensor([0.0,0.1,0.2]).reshape(-1,1) y_cpu = torch.tensor([0.0,0.2,0.4]).reshape(-1,1) net_cpu = torch.nn.Linear(1,1) . - gpu 연산이 가능한 메모리에 데이터 저장 . torch.manual_seed(43052) x_gpu = x_cpu.to(&quot;cuda:0&quot;) y_gpu = y_cpu.to(&quot;cuda:0&quot;) net_gpu = torch.nn.Linear(1,1).to(&quot;cuda:0&quot;) . - cpu 혹은 gpu 연산이 가능한 메모리에 저장된 값들을 확인 . x_cpu, y_cpu, net_cpu.weight, net_cpu.bias . (tensor([[0.0000], [0.1000], [0.2000]]), tensor([[0.0000], [0.2000], [0.4000]]), Parameter containing: tensor([[-0.3467]], requires_grad=True), Parameter containing: tensor([-0.8470], requires_grad=True)) . x_gpu, y_gpu, net_gpu.weight, net_gpu.bias . (tensor([[0.0000], [0.1000], [0.2000]], device=&#39;cuda:0&#39;), tensor([[0.0000], [0.2000], [0.4000]], device=&#39;cuda:0&#39;), Parameter containing: tensor([[-0.3467]], device=&#39;cuda:0&#39;, requires_grad=True), Parameter containing: tensor([-0.8470], device=&#39;cuda:0&#39;, requires_grad=True)) . - gpu는 gpu끼리 연산가능하고 cpu는 cpu끼리 연산가능함 . (예시1) . net_cpu(x_cpu) . tensor([[-0.8470], [-0.8817], [-0.9164]], grad_fn=&lt;AddmmBackward0&gt;) . (예시2) . net_gpu(x_gpu) . tensor([[-0.8470], [-0.8817], [-0.9164]], device=&#39;cuda:0&#39;, grad_fn=&lt;AddmmBackward0&gt;) . (예시3) . net_cpu(x_gpu) . RuntimeError Traceback (most recent call last) /tmp/ipykernel_1392421/955438640.py in &lt;module&gt; -&gt; 1 net_cpu(x_gpu) ~/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs) 1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks 1129 or _global_forward_hooks or _global_forward_pre_hooks): -&gt; 1130 return forward_call(*input, **kwargs) 1131 # Do not call functions when jit is used 1132 full_backward_hooks, non_full_backward_hooks = [], [] ~/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/linear.py in forward(self, input) 112 113 def forward(self, input: Tensor) -&gt; Tensor: --&gt; 114 return F.linear(input, self.weight, self.bias) 115 116 def extra_repr(self) -&gt; str: RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm) . (예시4) . net_gpu(x_cpu) . RuntimeError Traceback (most recent call last) /tmp/ipykernel_1392421/413131407.py in &lt;module&gt; -&gt; 1 net_gpu(x_cpu) ~/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs) 1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks 1129 or _global_forward_hooks or _global_forward_pre_hooks): -&gt; 1130 return forward_call(*input, **kwargs) 1131 # Do not call functions when jit is used 1132 full_backward_hooks, non_full_backward_hooks = [], [] ~/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/linear.py in forward(self, input) 112 113 def forward(self, input: Tensor) -&gt; Tensor: --&gt; 114 return F.linear(input, self.weight, self.bias) 115 116 def extra_repr(self) -&gt; str: RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm) . (예시5) . torch.mean((y_cpu-net_cpu(x_cpu))**2) . tensor(1.2068, grad_fn=&lt;MeanBackward0&gt;) . (예시6) . torch.mean((y_gpu-net_gpu(x_gpu))**2) . tensor(1.2068, device=&#39;cuda:0&#39;, grad_fn=&lt;MeanBackward0&gt;) . (예시7) . torch.mean((y_gpu-net_cpu(x_cpu))**2) . RuntimeError Traceback (most recent call last) /tmp/ipykernel_1392421/2449708258.py in &lt;module&gt; -&gt; 1 torch.mean((y_gpu-net_cpu(x_cpu))**2) RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! . (예시8) . torch.mean((y_cpu-net_gpu(x_gpu))**2) . RuntimeError Traceback (most recent call last) /tmp/ipykernel_1392421/2827309378.py in &lt;module&gt; -&gt; 1 torch.mean((y_cpu-net_gpu(x_gpu))**2) RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! . &#49884;&#44036;&#52769;&#51221; (&#50696;&#48708;&#54617;&#49845;) . import time . t1 = time.time() . t2 = time.time() . t2-t1 . 4.9920783042907715 . CPU (512) . - 데이터준비 . torch.manual_seed(5) x=torch.linspace(0,1,100).reshape(-1,1) y=torch.randn(100).reshape(-1,1)*0.01 . - for문 준비 . net = torch.nn.Sequential( torch.nn.Linear(1,512), torch.nn.ReLU(), torch.nn.Linear(512,1) ) loss_fn = torch.nn.MSELoss() optimizr = torch.optim.Adam(net.parameters()) . - for문 + 학습시간측정 . t1= time.time() for epoc in range(1000): ## 1 yhat = net(x) ## 2 loss = loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizr.step() optimizr.zero_grad() t2 = time.time() t2-t1 . 0.28586554527282715 . GPU (512) . - 데이터준비 . torch.manual_seed(5) x=torch.linspace(0,1,100).reshape(-1,1).to(&quot;cuda:0&quot;) y=(torch.randn(100).reshape(-1,1)*0.01).to(&quot;cuda:0&quot;) . - for문돌릴준비 . net = torch.nn.Sequential( torch.nn.Linear(1,512), torch.nn.ReLU(), torch.nn.Linear(512,1) ).to(&quot;cuda:0&quot;) loss_fn = torch.nn.MSELoss() optimizr = torch.optim.Adam(net.parameters()) . - for문 + 학습시간측정 . t1= time.time() for epoc in range(1000): ## 1 yhat = net(x) ## 2 loss = loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizr.step() optimizr.zero_grad() t2 = time.time() t2-t1 . 0.5355696678161621 . !! CPU가 더 빠르다? | . CPU vs GPU (20480) . - CPU (20480) . torch.manual_seed(5) x=torch.linspace(0,1,100).reshape(-1,1) y=torch.randn(100).reshape(-1,1)*0.01 net = torch.nn.Sequential( torch.nn.Linear(1,20480), torch.nn.ReLU(), torch.nn.Linear(20480,1) ) loss_fn = torch.nn.MSELoss() optimizr = torch.optim.Adam(net.parameters()) t1= time.time() for epoc in range(1000): ## 1 yhat = net(x) ## 2 loss = loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizr.step() optimizr.zero_grad() t2 = time.time() t2-t1 . 2.380666494369507 . - GPU (20480) . torch.manual_seed(5) x=torch.linspace(0,1,100).reshape(-1,1).to(&quot;cuda:0&quot;) y=(torch.randn(100).reshape(-1,1)*0.01).to(&quot;cuda:0&quot;) net = torch.nn.Sequential( torch.nn.Linear(1,20480), torch.nn.ReLU(), torch.nn.Linear(20480,1) ).to(&quot;cuda:0&quot;) loss_fn = torch.nn.MSELoss() optimizr = torch.optim.Adam(net.parameters()) t1= time.time() for epoc in range(1000): ## 1 yhat = net(x) ## 2 loss = loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizr.step() optimizr.zero_grad() t2 = time.time() t2-t1 . 0.5442469120025635 . - 왜 이런 차이가 나는가? 연산을 하는 주체는 코어인데 CPU는 수는 적지만 일을 잘하는 코어들을 가지고 있고 GPU는 일은 못하지만 다수의 코어를 가지고 있기 때문 . CPU vs GPU (204800) . - CPU (204800) . torch.manual_seed(5) x=torch.linspace(0,1,100).reshape(-1,1) y=torch.randn(100).reshape(-1,1)*0.01 net = torch.nn.Sequential( torch.nn.Linear(1,204800), torch.nn.ReLU(), torch.nn.Linear(204800,1) ) loss_fn = torch.nn.MSELoss() optimizr = torch.optim.Adam(net.parameters()) t1= time.time() for epoc in range(1000): ## 1 yhat = net(x) ## 2 loss = loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizr.step() optimizr.zero_grad() t2 = time.time() t2-t1 . 51.95550894737244 . - GPU (204800) . torch.manual_seed(5) x=torch.linspace(0,1,100).reshape(-1,1).to(&quot;cuda:0&quot;) y=(torch.randn(100).reshape(-1,1)*0.01).to(&quot;cuda:0&quot;) net = torch.nn.Sequential( torch.nn.Linear(1,204800), torch.nn.ReLU(), torch.nn.Linear(204800,1) ).to(&quot;cuda:0&quot;) loss_fn = torch.nn.MSELoss() optimizr = torch.optim.Adam(net.parameters()) t1= time.time() for epoc in range(1000): ## 1 yhat = net(x) ## 2 loss = loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizr.step() optimizr.zero_grad() t2 = time.time() t2-t1 . 1.3824031352996826 . &#54869;&#47456;&#51201;&#44221;&#49324;&#54616;&#44053;&#48277;, &#48176;&#52824;, &#50640;&#54253; . &#51328; &#51060;&#49345;&#54616;&#51648; &#50506;&#50500;&#50836;? . - 우리가 쓰는 GPU: 다나와 PC견적 . GPU 메모리 끽해봐야 24GB | . - 우리가 분석하는 데이터: 빅데이터..? . - 데이터의 크기가 커지는순간 X.to(&quot;cuda:0&quot;), y.to(&quot;cuda:0&quot;) 쓰면 난리나겠는걸? . x = torch.linspace(-10,10,100000).reshape(-1,1) eps = torch.randn(100000).reshape(-1,1) y = x*2 + eps . plt.plot(x,y,&#39;o&#39;,alpha=0.05) plt.plot(x,2*x) . [&lt;matplotlib.lines.Line2D at 0x7f62d96f2c10&gt;] . - 데이터를 100개중에 1개만 꼴로만 쓰면 어떨까? . plt.plot(x[::100],y[::100],&#39;o&#39;,alpha=0.05) plt.plot(x,2*x) . [&lt;matplotlib.lines.Line2D at 0x7f62db305310&gt;] . 대충 이거만 가지고 적합해도 충분히 정확할것 같은데 | . X,y &#45936;&#51060;&#53552;&#47484; &#44403;&#51060; &#47784;&#46160; GPU&#50640; &#45336;&#44200;&#50556; &#54616;&#45716;&#44032;? . - 데이터셋을 짝홀로 나누어서 번갈아가면서 GPU에 올렸다 내렸다하면 안되나? . - 아래의 알고리즘을 생각해보자. . 데이터를 반으로 나눈다. | 짝수obs의 x,y 그리고 net의 모든 파라메터를 GPU에 올린다. | yhat, loss, grad, update 수행 | 짝수obs의 x,y를 GPU메모리에서 내린다. 그리고 홀수obs의 x,y를 GPU메모리에 올린다. | yhat, loss, grad, update 수행 | 홀수obs의 x,y를 GPU메모리에서 내린다. 그리고 짝수obs의 x,y를 GPU메모리에 올린다. | 반복 | &#44221;&#49324;&#54616;&#44053;&#48277;, &#54869;&#47456;&#51201;&#44221;&#49324;&#54616;&#44053;&#48277;, &#48120;&#45768;&#48176;&#52824; &#44221;&#49324;&#54616;&#44053;&#48277; . 10개의 샘플이 있다고 가정. $ {(x_i,y_i) }_{i=1}^{10}$ . - ver1: 모든 샘플을 이용하여 slope 계산 . (epoch1) $loss= sum_{i=1}^{10}(y_i- beta_0- beta_1x_i)^2 to slope to update$ . (epoch2) $loss= sum_{i=1}^{10}(y_i- beta_0- beta_1x_i)^2 to slope to update$ . ... . - ver2: 하나의 샘플만을 이용하여 slope 계산 . (epoch1) . $loss=(y_1- beta_0- beta_1x_1)^2 to slope to update$ | $loss=(y_2- beta_0- beta_1x_2)^2 to slope to update$ | ... | $loss=(y_{10}- beta_0- beta_1x_{10})^2 to slope to update$ | . (epoch2) . $loss=(y_1- beta_0- beta_1x_1)^2 to slope to update$ | $loss=(y_2- beta_0- beta_1x_2)^2 to slope to update$ | ... | $loss=(y_{10}- beta_0- beta_1x_{10})^2 to slope to update$ | . ... . - ver3: $m ( leq n)$ 개의 샘플을 이용하여 slope 계산 . $m=3$이라고 하자. . (epoch1) . $loss= sum_{i=1}^{3}(y_i- beta_0- beta_1x_i)^2 to slope to update$ | $loss= sum_{i=4}^{6}(y_i- beta_0- beta_1x_i)^2 to slope to update$ | $loss= sum_{i=7}^{9}(y_i- beta_0- beta_1x_i)^2 to slope to update$ | $loss=(y_{10}- beta_0- beta_1x_{10})^2 to slope to update$ | . (epoch2) . $loss= sum_{i=1}^{3}(y_i- beta_0- beta_1x_i)^2 to slope to update$ | $loss= sum_{i=4}^{6}(y_i- beta_0- beta_1x_i)^2 to slope to update$ | $loss= sum_{i=7}^{9}(y_i- beta_0- beta_1x_i)^2 to slope to update$ | $loss=(y_{10}- beta_0- beta_1x_{10})^2 to slope to update$ | . ... . &#50857;&#50612;&#51032; &#51221;&#47532; . 옛날 . - ver1: gradient descent, batch gradient descent . - ver2: stochastic gradient descent . - ver3: mini-batch gradient descent, mini-batch stochastic gradient descent . 요즘 . - ver1: gradient descent . - ver2: stochastic gradient descent with batch size = 1 . - ver3: stochastic gradient descent . https://www.deeplearningbook.org/contents/optimization.html, 알고리즘 8-1 참고. | . ds, dl . - ds . x=torch.tensor(range(10)).float()#.reshape(-1,1) y=torch.tensor([1.0]*5+[0.0]*5)#.reshape(-1,1) . ds=torch.utils.data.TensorDataset(x,y) ds . &lt;torch.utils.data.dataset.TensorDataset at 0x7f62db294710&gt; . ds.tensors # 그냥 (x,y)의 튜플 . (tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]), tensor([1., 1., 1., 1., 1., 0., 0., 0., 0., 0.])) . - dl . dl=torch.utils.data.DataLoader(ds,batch_size=3) #set(dir(dl)) &amp; {&#39;__iter__&#39;} . for xx,yy in dl: print(xx,yy) . tensor([0., 1., 2.]) tensor([1., 1., 1.]) tensor([3., 4., 5.]) tensor([1., 1., 0.]) tensor([6., 7., 8.]) tensor([0., 0., 0.]) tensor([9.]) tensor([0.]) . ds, dl&#51012; &#51060;&#50857;&#54620; MNIST &#44396;&#54788; . - 데이터정리 . path = untar_data(URLs.MNIST) . zero_fnames = (path/&#39;training/0&#39;).ls() one_fnames = (path/&#39;training/1&#39;).ls() . X0 = torch.stack([torchvision.io.read_image(str(zf)) for zf in zero_fnames]) X1 = torch.stack([torchvision.io.read_image(str(of)) for of in one_fnames]) X = torch.concat([X0,X1],axis=0).reshape(-1,1*28*28)/255 y = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1) . X.shape,y.shape . (torch.Size([12665, 784]), torch.Size([12665, 1])) . - ds $ to$ dl . ds = torch.utils.data.TensorDataset(X,y) dl = torch.utils.data.DataLoader(ds,batch_size=2048) . 12665/2048 . 6.18408203125 . i = 0 for xx,yy in dl: # 총 7번 돌아가는 for문 print(i) i=i+1 . 0 1 2 3 4 5 6 . - 미니배치 안쓰는 학습 . torch.manual_seed(43052) net = torch.nn.Sequential( torch.nn.Linear(784,32), torch.nn.ReLU(), torch.nn.Linear(32,1), torch.nn.Sigmoid() ) loss_fn = torch.nn.BCELoss() optimizr = torch.optim.Adam(net.parameters()) . for epoc in range(70): ## 1 yhat = net(X) ## 2 loss= loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizr.step() optimizr.zero_grad() . torch.sum((yhat&gt;0.5) == y) / len(y) . tensor(0.9981) . - 미니배치 쓰는 학습 (GPU 올리고 내리는 과정은 생략) . torch.manual_seed(43052) net = torch.nn.Sequential( torch.nn.Linear(784,32), torch.nn.ReLU(), torch.nn.Linear(32,1), torch.nn.Sigmoid() ) loss_fn = torch.nn.BCELoss() optimizr = torch.optim.Adam(net.parameters()) . for epoc in range(10): for xx,yy in dl: ## 7번 ## 1 #yhat = net(xx) ## 2 loss = loss_fn(net(xx),yy) ## 3 loss.backward() ## 4 optimizr.step() optimizr.zero_grad() . torch.mean(((net(X)&gt;0.5) == y)*1.0) . tensor(0.9950) . &#50724;&#48260;&#54588;&#54021; . - 오버피팅이란? . 위키: In mathematical modeling, overfitting is &quot;the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit to additional data or predict future observations reliably&quot;. | 제 개념: 데이터를 &quot;데이터 = 언더라잉 + 오차&quot;라고 생각할때 우리가 데이터로부터 적합할 것은 언더라잉인데 오차항을 적합하고 있는 현상. | . &#50724;&#48260;&#54588;&#54021; &#50696;&#49884; . - $m$이 매우 클때 아래의 네트워크 거의 무엇이든 맞출수 있다고 보면 된다. . $ underset{(n,1)}{ bf X} overset{l_1}{ to} underset{(n,m)}{ boldsymbol u^{(1)}} overset{h}{ to} underset{(n,m)}{ boldsymbol v^{(1)}} overset{l_2}{ to} underset{(n,1)}{ hat{ boldsymbol y}}$ | $ underset{(n,1)}{ bf X} overset{l_1}{ to} underset{(n,m)}{ boldsymbol u^{(1)}} overset{sig}{ to} underset{(n,m)}{ boldsymbol v^{(1)}} overset{l_2}{ to} underset{(n,1)}{ hat{ boldsymbol y}}$ | $ underset{(n,1)}{ bf X} overset{l_1}{ to} underset{(n,m)}{ boldsymbol u^{(1)}} overset{relu}{ to} underset{(n,m)}{ boldsymbol v^{(1)}} overset{l_2}{ to} underset{(n,1)}{ hat{ boldsymbol y}}$ | . - 그런데 종종 맞추지 말아야 할 것들도 맞춘다. . model: $y_i = (0 times x_i) + epsilon_i$, where $ epsilon_i sim N(0,0.01^2)$ . torch.manual_seed(5) x=torch.linspace(0,1,100).reshape(100,1) y=torch.randn(100).reshape(100,1)*0.01 plt.plot(x,y) . [&lt;matplotlib.lines.Line2D at 0x7f62e48fc190&gt;] . y는 그냥 정규분포에서 생성한 오차이므로 $X to y$ 로 항햐는 규칙따위는 없음 | . torch.manual_seed(1) net=torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=512), torch.nn.ReLU(), torch.nn.Linear(in_features=512,out_features=1)) optimizer= torch.optim.Adam(net.parameters()) loss_fn= torch.nn.MSELoss() for epoc in range(1000): ## 1 yhat=net(x) ## 2 loss=loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . plt.plot(x,y) plt.plot(x,net(x).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f62e4933310&gt;] . 우리는 데이터를 랜덤에서 뽑았는데, 데이터의 추세를 따라간다 $ to$ 오버피팅 (underlying이 아니라 오차항을 따라가고 있음) | . &#50724;&#48260;&#54588;&#54021;&#51060;&#46972;&#45716; &#46748;&#47159;&#54620; &#51613;&#44144;! (train / test) . - 데이터의 분리하여 보자. . torch.manual_seed(5) x=torch.linspace(0,1,100).reshape(100,1) y=torch.randn(100).reshape(100,1)*0.01 xtr = x[:80] ytr = y[:80] xtest = x[80:] ytest = y[80:] plt.plot(xtr,ytr) plt.plot(xtest,ytest) plt.title(&#39;train: blue / test: orange&#39;); . - train만 학습 . torch.manual_seed(1) net1=torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=512), torch.nn.ReLU(), torch.nn.Linear(in_features=512,out_features=1)) optimizr1= torch.optim.Adam(net1.parameters()) loss_fn= torch.nn.MSELoss() for epoc in range(1000): ## 1 # net(xtr) ## 2 loss=loss_fn(net1(xtr),ytr) ## 3 loss.backward() ## 4 optimizr1.step() optimizr1.zero_grad() . - training data로 학습한 net를 training data 에 적용 . plt.plot(x,y,alpha=0.5) plt.plot(xtr,net1(xtr).data,&#39;--&#39;) # prediction (train) . [&lt;matplotlib.lines.Line2D at 0x7f62de905190&gt;] . train에서는 잘 맞추는듯이 보인다. | . - training data로 학습한 net를 test data 에 적용 . plt.plot(x,y,alpha=0.5) plt.plot(xtr,net1(xtr).data,&#39;--&#39;) # prediction (train) plt.plot(xtest,net1(xtest).data,&#39;--&#39;) # prediction with unseen data (test) . [&lt;matplotlib.lines.Line2D at 0x7f62de084550&gt;] . train은 그럭저럭 따라가지만 test에서는 엉망이다. $ to$ overfit | . &#49689;&#51228; (&#54644;&#49444; &#48143; &#54400;&#51060;&#45716; &#50668;&#44592;-10%EC%9B%946%EC%9D%BC.html#%EC%88%99%EC%A0%9C)&#52280;&#44256;) . (1) 숫자0과 숫자1을 구분하는 네트워크를 아래와 같은 구조로 설계하라 . $$ underset{(n,784)}{ bf X} overset{l_1}{ to} underset{(n,64)}{ boldsymbol u^{(1)}} overset{a_1}{ to} underset{(n,64)}{ boldsymbol v^{(1)}} overset{l_1}{ to} underset{(n,1)}{ boldsymbol u^{(2)}} overset{a_2}{ to} underset{(n,1)}{ boldsymbol v^{(2)}}= underset{(n,1)}{ hat{ boldsymbol y}}$$ . 위에서 $a_1$은 relu를, $a_2$는 sigmoid를 의미한다. . &quot;y=0&quot;은 숫자0을 의미하도록 하고 &quot;y=1&quot;은 숫자1을 의미하도록 설정하라. | . (2) 아래의 지침에 따라 200 epoch 학습을 진행하라. . 손실함수는 BECLoss를 이용할 것. torch.nn.BCELoss() 를 이용할 것. | 옵티마이저는 아담으로 설정할 것. 학습률은 lr=0.002로 설정할 것. | . (3) 아래의 지침에 따라 200 epoch 학습을 진행하라. 학습이 잘 되는가? . 손실함수는 BECLoss를 이용할 것. torch.nn.BCELoss()를 사용하지 않고 수식을 직접 입력할 것. | 옵티마이저는 아담으로 설정할 것. 학습률은 lr=0.002로 설정할 것. | . (4) 아래의 지침에 따라 200 epoch 학습을 진행하라. 학습이 잘 되는가? . 이미지의 값을 0과 1사이로 규격화 하라. (Xnp = Xnp/255 를 이용하세요!) | 손실함수는 BECLoss를 이용할 것. torch.nn.BCELoss()를 사용하지 않고 수식을 직접 입력할 것. | 옵티마이저는 아담으로 설정할 것. 학습률은 lr=0.002로 설정할 것. | . (5) 아래와 같은 수식을 이용하여 accuracy를 계산하라. . $ text{accuracy}= frac{1}{n} sum_{i=1}^n I( tilde{y}_i=y_i)$ . $ tilde{y}_i = begin{cases} 1 &amp; hat{y}_i &gt; 0.5 0 &amp; hat{y}_i leq 0.5 end{cases}$ | $I( tilde{y}_i=y_i) = begin{cases} 1 &amp; tilde{y}_i=y_i 0 &amp; tilde{y}_i neq y_i end{cases}$ | . 단, $n$은 0과 1을 의미하는 이미지의 수 .",
            "url": "https://guebin.github.io/STML2022/2022/10/12/(6%EC%A3%BC%EC%B0%A8)-10%EC%9B%9412%EC%9D%BC.html",
            "relUrl": "/2022/10/12/(6%EC%A3%BC%EC%B0%A8)-10%EC%9B%9412%EC%9D%BC.html",
            "date": " • Oct 12, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "(6주차) 10월11일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . imports . import torch import torchvision from fastai.data.all import * . import graphviz def gv(s): return graphviz.Source(&#39;digraph G{ rankdir=&quot;LR&quot;&#39;+s + &#39;; }&#39;); . &#49888;&#44221;&#47581;&#51032; &#54364;&#54788; (${ boldsymbol x} to hat{ boldsymbol y}$ &#47196; &#44032;&#45716; &#44284;&#51221;&#51012; &#44536;&#47548;&#51004;&#47196; &#54364;&#54788;) . &#50696;&#51228;1: $ underset{(n,1)}{ bf X} overset{l_1}{ to} underset{(n,1)}{ boldsymbol u^{(1)}} overset{sig}{ to} underset{(n,1)}{ boldsymbol v^{(1)}} = underset{(n,1)}{ hat{ boldsymbol y}}$ . - 모든 observation과 가중치를 명시한 버전 . (표현1) . gv(&#39;&#39;&#39; &quot;1&quot; -&gt; &quot;ŵ₀ + xₙ*ŵ₁, bias=False&quot;[label=&quot;* ŵ₀&quot;] &quot;xₙ&quot; -&gt; &quot;ŵ₀ + xₙ*ŵ₁, bias=False&quot;[label=&quot;* ŵ₁&quot;] &quot;ŵ₀ + xₙ*ŵ₁, bias=False&quot; -&gt; &quot;ŷₙ&quot;[label=&quot;sigmoid&quot;] &quot;.&quot; -&gt; &quot;....................................&quot;[label=&quot;* ŵ₀&quot;] &quot;..&quot; -&gt; &quot;....................................&quot;[label=&quot;* ŵ₁&quot;] &quot;....................................&quot; -&gt; &quot;...&quot;[label=&quot; &quot;] &quot;1 &quot; -&gt; &quot;ŵ₀ + x₂*ŵ₁, bias=False&quot;[label=&quot;* ŵ₀&quot;] &quot;x₂&quot; -&gt; &quot;ŵ₀ + x₂*ŵ₁, bias=False&quot;[label=&quot;* ŵ₁&quot;] &quot;ŵ₀ + x₂*ŵ₁, bias=False&quot; -&gt; &quot;ŷ₂&quot;[label=&quot;sigmoid&quot;] &quot;1 &quot; -&gt; &quot;ŵ₀ + x₁*ŵ₁, bias=False&quot;[label=&quot;* ŵ₀&quot;] &quot;x₁&quot; -&gt; &quot;ŵ₀ + x₁*ŵ₁, bias=False&quot;[label=&quot;* ŵ₁&quot;] &quot;ŵ₀ + x₁*ŵ₁, bias=False&quot; -&gt; &quot;ŷ₁&quot;[label=&quot;sigmoid&quot;] &#39;&#39;&#39;) . . 단점: 똑같은 그림의 반복이 너무 많음 | . - observation 반복을 생략한 버전들 . (표현2) 모든 $i$에 대하여 아래의 그림을 반복한다고 하면 (표현1)과 같다. . gv(&#39;&#39;&#39; &quot;1&quot; -&gt; &quot;ŵ₀ + xᵢ*ŵ₁, bias=False&quot;[label=&quot;* ŵ₀&quot;] &quot;xᵢ&quot; -&gt; &quot;ŵ₀ + xᵢ*ŵ₁, bias=False&quot;[label=&quot;* ŵ₁&quot;] &quot;ŵ₀ + xᵢ*ŵ₁, bias=False&quot; -&gt; &quot;ŷᵢ&quot;[label=&quot;sigmoid&quot;] &#39;&#39;&#39;) . . (표현3) 그런데 (표현2)에서 아래와 같이 $x_i$, $y_i$ 대신에 간단히 $x$, $y$로 쓰는 경우도 많음 . gv(&#39;&#39;&#39; &quot;1&quot; -&gt; &quot;ŵ₀ + x*ŵ₁, bias=False&quot;[label=&quot;* ŵ₀&quot;] &quot;x&quot; -&gt; &quot;ŵ₀ + x*ŵ₁, bias=False&quot;[label=&quot;* ŵ₁&quot;] &quot;ŵ₀ + x*ŵ₁, bias=False&quot; -&gt; &quot;ŷ&quot;[label=&quot;sigmoid&quot;] &#39;&#39;&#39;) . . - 1을 생략한 버전들 . (표현4) bais=False 대신에 bias=True를 주면 1을 생략할 수 있음 . gv(&#39;&#39;&#39; &quot;x&quot; -&gt; &quot;x*ŵ₁, bias=True&quot;[label=&quot;*ŵ₁&quot;] ; &quot;x*ŵ₁, bias=True&quot; -&gt; &quot;ŷ&quot;[label=&quot;sigmoid&quot;] &#39;&#39;&#39;) . . (표현4의 수정) $ hat{w}_1$대신에 $ hat{w}$를 쓰는 것이 더 자연스러움 . gv(&#39;&#39;&#39; &quot;x&quot; -&gt; &quot;x*ŵ, bias=True&quot;[label=&quot;*ŵ&quot;] ; &quot;x*ŵ, bias=True&quot; -&gt; &quot;ŷ&quot;[label=&quot;sigmoid&quot;] &#39;&#39;&#39;) . . (표현5) 선형변환의 결과는 아래와 같이 $u$로 표현하기도 한다. . gv(&#39;&#39;&#39; &quot;x&quot; -&gt; &quot;u&quot;; &quot;u&quot; -&gt; &quot;y&quot;[label=&quot;sigmoid&quot;] &#39;&#39;&#39;) . . 다이어그램은 그리는 사람의 취향에 따라 그리는 방법이 조금씩 다릅니다. 즉 교재마다 달라요. . &#50696;&#51228;2: $ underset{(n,1)}{ bf X} overset{l_1}{ to} underset{(n,2)}{ boldsymbol u^{(1)}} overset{relu}{ to} underset{(n,2)}{ boldsymbol v^{(1)}} overset{l_2}{ to} underset{(n,1)}{ boldsymbol u^{(2)}} overset{sig}{ to} underset{(n,1)}{ boldsymbol v^{(2)}} = underset{(n,1)}{ hat{ boldsymbol y}}$ . 참고: 코드로 표현 . torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=2), torch.nn.ReLU(), torch.nn.Linear(in_features=2,out_features=1), torch.nn.Sigmoid() ) . - 이해를 위해서 10월4일 강의노트에서 다루었던 아래의 상황을 고려하자. . . (강의노트의 표현) . gv(&#39;&#39;&#39; &quot;x&quot; -&gt; &quot; -x&quot;[label=&quot;*(-1)&quot;]; &quot;x&quot; -&gt; &quot; x&quot;[label=&quot;*1&quot;] &quot; x&quot; -&gt; &quot;rlu(x)&quot;[label=&quot;relu&quot;] &quot; -x&quot; -&gt; &quot;rlu(-x)&quot;[label=&quot;relu&quot;] &quot;rlu(x)&quot; -&gt; &quot;u&quot;[label=&quot;*(-4.5)&quot;] &quot;rlu(-x)&quot; -&gt; &quot;u&quot;[label=&quot;*(-9.0)&quot;] &quot;u&quot; -&gt; &quot;sig(u)=yhat&quot;[label=&quot;sig&quot;] &#39;&#39;&#39; ) . . (좀 더 일반화된 표현) 10월4일 강의노트 상황을 일반화하면 아래와 같다. . gv(&#39;&#39;&#39; &quot;x&quot; -&gt; &quot;u1[:,0]&quot;[label=&quot;*(-1)&quot;]; &quot;x&quot; -&gt; &quot;u1[:,1]&quot;[label=&quot;*1&quot;] &quot;u1[:,0]&quot; -&gt; &quot;v1[:,0]&quot;[label=&quot;relu&quot;] &quot;u1[:,1]&quot; -&gt; &quot;v1[:,1]&quot;[label=&quot;relu&quot;] &quot;v1[:,0]&quot; -&gt; &quot;u2&quot;[label=&quot;*(-9.0)&quot;] &quot;v1[:,1]&quot; -&gt; &quot;u2&quot;[label=&quot;*(-4.5)&quot;] &quot;u2&quot; -&gt; &quot;v2=yhat&quot;[label=&quot;sig&quot;] &#39;&#39;&#39; ) . . * Layer의 개념: ${ bf X}$에서 $ hat{ boldsymbol y}$로 가는 과정은 &quot;선형변환+비선형변환&quot;이 반복되는 구조이다. &quot;선형변환+비선형변환&quot;을 하나의 세트로 보면 아래와 같이 표현할 수 있다. . $ underset{(n,1)}{ bf X} overset{l_1}{ to} left( underset{(n,2)}{ boldsymbol u^{(1)}} overset{relu}{ to} underset{(n,2)}{ boldsymbol v^{(1)}} right) overset{l_2}{ to} left( underset{(n,1)}{ boldsymbol u^{(2)}} overset{sig}{ to} underset{(n,1)}{ boldsymbol v^{(2)}} right), quad underset{(n,1)}{ boldsymbol v^{(2)}}= underset{(n,1)}{net({ bf X})}= underset{(n,1)}{ hat{ boldsymbol y}}$ . 이것을 다이어그램으로 표현한다면 아래와 같다. . (선형+비선형을 하나의 Layer로 묶은 표현) . gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; &quot;X&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;X&quot; -&gt; &quot;u1[:,0]&quot; &quot;X&quot; -&gt; &quot;u1[:,1]&quot; &quot;u1[:,0]&quot; -&gt; &quot;v1[:,0]&quot;[label=&quot;relu&quot;] &quot;u1[:,1]&quot; -&gt; &quot;v1[:,1]&quot;[label=&quot;relu&quot;] label = &quot;Layer 1&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;v1[:,0]&quot; -&gt; &quot;u2&quot; &quot;v1[:,1]&quot; -&gt; &quot;u2&quot; &quot;u2&quot; -&gt; &quot;v2=yhat&quot;[label=&quot;sigmoid&quot;] label = &quot;Layer 2&quot; } &#39;&#39;&#39;) . . Layer를 세는 방법 . 정석: 학습가능한 파라메터가 몇층으로 있는지... | 일부 교재 설명: 입력층은 계산하지 않음, activation layer는 계산하지 않음. | 위의 예제의 경우 number of layer = 2 이다. | . 사실 input layer, activation layer 등의 표현을 자주 사용해서 layer를 세는 방법이 처음에는 헷갈립니다.. . Hidden Layer의 수를 세는 방법 . Layer의 수 = Hidden Layer의 수 + 출력층의 수 = Hidden Layer의 수 + 1 | 위의 예제의 경우 number of hidden layer = 1 이다. | . * node의 개념: $u to v$로 가는 쌍을 간단히 노드라는 개념을 이용하여 나타낼 수 있음. . (노드의 개념이 포함된 그림) . gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; &quot;X&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;X&quot; -&gt; &quot;node1&quot; &quot;X&quot; -&gt; &quot;node2&quot; label = &quot;Layer 1:relu&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;node1&quot; -&gt; &quot;yhat &quot; &quot;node2&quot; -&gt; &quot;yhat &quot; label = &quot;Layer 2:sigmoid&quot; } &#39;&#39;&#39;) . . 여기에서 node의 숫자 = feature의 숫자와 같이 이해할 수 있다. 즉 아래와 같이 이해할 수 있다. . (&quot;number of nodes = number of features&quot;로 이해한 그림) . gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; &quot;X&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;X&quot; -&gt; &quot;feature1&quot; &quot;X&quot; -&gt; &quot;feature2&quot; label = &quot;Layer 1:relu&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;feature1&quot; -&gt; &quot;yhat &quot; &quot;feature2&quot; -&gt; &quot;yhat &quot; label = &quot;Layer 2:sigmoid&quot; } &#39;&#39;&#39;) . . 다이어그램의 표현방식은 교재마다 달라서 모든 예시를 달달 외울 필요는 없습니다. 다만 임의의 다이어그램을 보고 대응하는 네트워크를 pytorch로 구현하는 능력은 매우 중요합니다. . &#50696;&#51228;3: $ underset{(n,784)}{ bf X} overset{l_1}{ to} underset{(n,32)}{ boldsymbol u^{(1)}} overset{relu}{ to} underset{(n,32)}{ boldsymbol v^{(1)}} overset{l_1}{ to} underset{(n,1)}{ boldsymbol u^{(2)}} overset{sig}{ to} underset{(n,1)}{ boldsymbol v^{(2)}}= underset{(n,1)}{ hat{ boldsymbol y}}$ . (다이어그램표현) . gv(&#39;&#39;&#39; splines=line subgraph cluster_1{ style=filled; color=lightgrey; &quot;x1&quot; &quot;x2&quot; &quot;..&quot; &quot;x784&quot; label = &quot;Input Layer&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;x1&quot; -&gt; &quot;node1&quot; &quot;x2&quot; -&gt; &quot;node1&quot; &quot;..&quot; -&gt; &quot;node1&quot; &quot;x784&quot; -&gt; &quot;node1&quot; &quot;x1&quot; -&gt; &quot;node2&quot; &quot;x2&quot; -&gt; &quot;node2&quot; &quot;..&quot; -&gt; &quot;node2&quot; &quot;x784&quot; -&gt; &quot;node2&quot; &quot;x1&quot; -&gt; &quot;...&quot; &quot;x2&quot; -&gt; &quot;...&quot; &quot;..&quot; -&gt; &quot;...&quot; &quot;x784&quot; -&gt; &quot;...&quot; &quot;x1&quot; -&gt; &quot;node32&quot; &quot;x2&quot; -&gt; &quot;node32&quot; &quot;..&quot; -&gt; &quot;node32&quot; &quot;x784&quot; -&gt; &quot;node32&quot; label = &quot;Hidden Layer: relu&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;node1&quot; -&gt; &quot;yhat&quot; &quot;node2&quot; -&gt; &quot;yhat&quot; &quot;...&quot; -&gt; &quot;yhat&quot; &quot;node32&quot; -&gt; &quot;yhat&quot; label = &quot;Outplut Layer: sigmoid&quot; } &#39;&#39;&#39;) . . Layer0,1,2 대신에 Input Layer, Hidden Layer, Output Layer로 표현함 | . - 위의 다이어그램에 대응하는 코드 . net = torch.nn.Sequential( torch.nn.Linear(in_features=28*28*1,out_features=32), torch.nn.ReLU(), torch.nn.Linear(in_features=32,out_features=1), torch.nn.Sigmoid() ) . &#49884;&#48292;&#53076;&#51221;&#47532;&#44032; &#49457;&#47549;&#54616;&#45716; &#51060;&#50976;? (&#50628;&#48128;&#54620; &#51613;&#47749; X) . &#44536;&#47548;&#51004;&#47196; &#48372;&#45716; &#51613;&#47749;&#44284;&#51221; . - 데이터 . x = torch.linspace(-10,10,200).reshape(-1,1) . - 아래와 같은 네트워크를 고려하자. . l1 = torch.nn.Linear(in_features=1,out_features=2) a1 = torch.nn.Sigmoid() l2 = torch.nn.Linear(in_features=2,out_features=1) . - 직관1: $l_1$,$l_2$의 가중치를 잘 결합하다보면 우연히 아래와 같이 만들 수 있다. . l1.weight.data = torch.tensor([[-5.00],[5.00]]) l1.bias.data = torch.tensor([+10.00,+10.00]) . l2.weight.data = torch.tensor([[1.00,1.00]]) l2.bias.data = torch.tensor([-1.00]) . fig,ax = plt.subplots(1,3,figsize=(9,3)) ax[0].plot(x,l1(x).data); ax[0].set_title(&#39;$l_1(x)$&#39;) ax[1].plot(x,a1(l1(x)).data); ax[1].set_title(&#39;$(a_1 circ l_1)(x)$&#39;) ax[2].plot(x,l2(a1(l1(x))).data,color=&#39;C2&#39;); ax[2].set_title(&#39;$(l_2 circ a_1 circ l_1)(x)$&#39;) . Text(0.5, 1.0, &#39;$(l_2 circ a_1 circ l_1)(x)$&#39;) . - 직관2: 아래들도 가능할듯? . l1.weight.data = torch.tensor([[-5.00],[5.00]]) l1.bias.data = torch.tensor([+0.00,+20.00]) l2.weight.data = torch.tensor([[1.00,1.00]]) l2.bias.data = torch.tensor([-1.00]) fig,ax = plt.subplots(1,3,figsize=(9,3)) ax[0].plot(x,l1(x).data,&#39;--&#39;,color=&#39;C0&#39;); ax[0].set_title(&#39;$l_1(x)$&#39;) ax[1].plot(x,a1(l1(x)).data,&#39;--&#39;,color=&#39;C0&#39;); ax[1].set_title(&#39;$(a_1 circ l_1)(x)$&#39;) ax[2].plot(x,l2(a1(l1(x))).data,&#39;--&#39;,color=&#39;C0&#39;); ax[2].set_title(&#39;$(l_2 circ a_1 circ l_1)(x)$&#39;); . l1.weight.data = torch.tensor([[-5.00],[5.00]]) l1.bias.data = torch.tensor([+20.00,+0.00]) l2.weight.data = torch.tensor([[2.50,2.50]]) l2.bias.data = torch.tensor([-2.50]) ax[0].plot(x,l1(x).data,&#39;--&#39;,color=&#39;C1&#39;); ax[0].set_title(&#39;$l_1(x)$&#39;) ax[1].plot(x,a1(l1(x)).data,&#39;--&#39;,color=&#39;C1&#39;); ax[1].set_title(&#39;$(a_1 circ l_1)(x)$&#39;) ax[2].plot(x,l2(a1(l1(x))).data,&#39;--&#39;,color=&#39;C1&#39;); ax[2].set_title(&#39;$(l_2 circ a_1 circ l_1)(x)$&#39;); fig . - 은닉층의노드수=4로 하고 적당한 가중치를 조정하면 $(l_2 circ a_1 circ l_1)(x)$의 결과로 주황색선 + 파란색선도 가능할 것 같다. $ to$ 실제로 가능함 . l1 = torch.nn.Linear(in_features=1,out_features=4) a1 = torch.nn.Sigmoid() l2 = torch.nn.Linear(in_features=4,out_features=1) . l1.weight.data = torch.tensor([[-5.00],[5.00],[-5.00],[5.00]]) l1.bias.data = torch.tensor([0.00, 20.00, 20.00, 0]) l2.weight.data = torch.tensor([[1.00, 1.00, 2.50, 2.50]]) l2.bias.data = torch.tensor([-1.0-2.5]) . plt.plot(l2(a1(l1(x))).data) . [&lt;matplotlib.lines.Line2D at 0x7f62d815a690&gt;] . - 2개의 시그모이드를 우연히 잘 결합하면 아래와 같은 함수 $h$를 만들 수 있다. . h = lambda x: torch.sigmoid(200*(x+0.5))+torch.sigmoid(-200*(x-0.5))-1.0 . plt.plot(x,h(x)) plt.title(&quot;$h(x)$&quot;) . Text(0.5, 1.0, &#39;$h(x)$&#39;) . - 위와 같은 함수 $h$를 활성화함수로 하고 $m$개의 노드를 가지는 은닉층을 생각해보자. 이러한 은닉층을 사용한다면 전체 네트워크를 아래와 같이 표현할 수 있다. . $ underset{(n,1)}{ bf X} overset{l_1}{ to} underset{(n,m)}{ boldsymbol u^{(1)}} overset{h}{ to} underset{(n,m)}{ boldsymbol v^{(1)}} overset{l_2}{ to} underset{(n,1)}{ hat{ boldsymbol y}}$ . 그리고 위의 네트워크와 동일한 효과를 주는 아래의 네트워크가 항상 존재함. . $ underset{(n,1)}{ bf X} overset{l_1}{ to} underset{(n,2m)}{ boldsymbol u^{(1)}} overset{sig}{ to} underset{(n,2m)}{ boldsymbol v^{(1)}} overset{l_2}{ to} underset{(n,1)}{ hat{ boldsymbol y}}$ . - $h(x)$를 활성화함수로 가지는 네트워크를 설계하여 보자. . class MyActivation(torch.nn.Module): ## 사용자정의 활성화함수를 선언하는 방법 def __init__(self): super().__init__() def forward(self, input): return h(input) # activation 의 출력 . a1=MyActivation() # a1 = torch.nn.Sigmoid(), a1 = torch.nn.ReLU() 대신에 a1 = MyActivation() . plt.plot(x,a1(x)) . [&lt;matplotlib.lines.Line2D at 0x7f62d8043b10&gt;] . 히든레이어가 1개의 노드를 가지는 경우 . torch.manual_seed(43052) fig, ax = plt.subplots(4,4,figsize=(12,12)) for i in range(4): for j in range(4): net = torch.nn.Sequential( torch.nn.Linear(1,1), MyActivation(), torch.nn.Linear(1,1) ) ax[i,j].plot(x,net(x).data,&#39;--&#39;) . 히든레이어가 2개의 노드를 가지는 경우 . torch.manual_seed(43052) fig, ax = plt.subplots(4,4,figsize=(12,12)) for i in range(4): for j in range(4): net = torch.nn.Sequential( torch.nn.Linear(1,2), MyActivation(), torch.nn.Linear(2,1) ) ax[i,j].plot(x,net(x).data,&#39;--&#39;) . 히든레이어가 3개의 노드를 가지는 경우 . torch.manual_seed(43052) fig, ax = plt.subplots(4,4,figsize=(12,12)) for i in range(4): for j in range(4): net = torch.nn.Sequential( torch.nn.Linear(1,3), MyActivation(), torch.nn.Linear(3,1) ) ax[i,j].plot(x,net(x).data,&#39;--&#39;) . 히든레이어가 1024개의 노드를 가지는 경우 . torch.manual_seed(43052) fig, ax = plt.subplots(4,4,figsize=(12,12)) for i in range(4): for j in range(4): net = torch.nn.Sequential( torch.nn.Linear(1,1024), MyActivation(), torch.nn.Linear(1024,1) ) ax[i,j].plot(x,net(x).data,&#39;--&#39;) . &#54616;&#45208;&#51032; &#51008;&#45769;&#52789;&#50640; &#47566;&#51008; &#45432;&#46300;&#49688;&#44032; &#51080;&#45716; &#49888;&#44221;&#47581; . - 아래와 같이 하나의 은닉층을 가지고 있더라도 많은 노드수만 보장되면 매우 충분한 표현력을 가짐 . $ underset{(n,1)}{ bf X} overset{l_1}{ to} underset{(n,m)}{ boldsymbol u^{(1)}} overset{h}{ to} underset{(n,m)}{ boldsymbol v^{(1)}} overset{l_2}{ to} underset{(n,1)}{ hat{ boldsymbol y}}$ . (예시1) . torch.manual_seed(43052) x = torch.linspace(-10,10,200).reshape(-1,1) underlying = torch.sin(2*x) + torch.sin(0.5*x) + torch.exp(-0.2*x) eps = torch.randn(200).reshape(-1,1)*0.1 y = underlying + eps plt.plot(x,y,&#39;o&#39;,alpha=0.5) plt.plot(x,underlying,lw=3) . [&lt;matplotlib.lines.Line2D at 0x7f62d9f82550&gt;] . h = lambda x: torch.sigmoid(200*(x+0.5))+torch.sigmoid(-200*(x-0.5))-1.0 class MyActivation(torch.nn.Module): ## 사용자정의 활성화함수를 선언하는 방법 def __init__(self): super().__init__() def forward(self, input): return h(input) . net= torch.nn.Sequential( torch.nn.Linear(1,2048), MyActivation(), torch.nn.Linear(2048,1) ) loss_fn = torch.nn.MSELoss() optimizr = torch.optim.Adam(net.parameters()) . for epoc in range(200): ## 1 yhat = net(x) ## 2 loss = loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizr.step() optimizr.zero_grad() . plt.plot(x,y,&#39;o&#39;,alpha=0.2) plt.plot(x,underlying,lw=3) plt.plot(x,net(x).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f62d9bc3d90&gt;] . (예시2) . df=pd.read_csv(&#39;https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-10-04-dnnex0.csv&#39;) df . x underlying y . 0 -1.000000 | 0.000045 | 0.0 | . 1 -0.998999 | 0.000046 | 0.0 | . 2 -0.997999 | 0.000047 | 0.0 | . 3 -0.996998 | 0.000047 | 0.0 | . 4 -0.995998 | 0.000048 | 0.0 | . ... ... | ... | ... | . 1995 0.995998 | 0.505002 | 0.0 | . 1996 0.996998 | 0.503752 | 0.0 | . 1997 0.997999 | 0.502501 | 0.0 | . 1998 0.998999 | 0.501251 | 1.0 | . 1999 1.000000 | 0.500000 | 1.0 | . 2000 rows × 3 columns . x = torch.tensor(df.x).reshape(-1,1).float() y = torch.tensor(df.y).reshape(-1,1).float() plt.plot(x,y,&#39;o&#39;,alpha=0.1) plt.plot(df.x,df.underlying,lw=3) . [&lt;matplotlib.lines.Line2D at 0x7f62d97965d0&gt;] . h = lambda x: torch.sigmoid(200*(x+0.5))+torch.sigmoid(-200*(x-0.5))-1.0 class MyActivation(torch.nn.Module): ## 사용자정의 활성화함수를 선언하는 방법 def __init__(self): super().__init__() def forward(self, input): return h(input) . torch.manual_seed(43052) net= torch.nn.Sequential( torch.nn.Linear(1,2048), MyActivation(), torch.nn.Linear(2048,1), torch.nn.Sigmoid() ) loss_fn = torch.nn.BCELoss() optimizr = torch.optim.Adam(net.parameters()) . for epoc in range(100): ## 1 yhat = net(x) ## 2 loss = loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizr.step() optimizr.zero_grad() . plt.plot(x,y,&#39;o&#39;,alpha=0.2) plt.plot(df.x,df.underlying,lw=3) plt.plot(x,net(x).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f62d971fd90&gt;] . CPU vs GPU . - 파이토치에서 GPU를 쓰는 방법을 알아보자. (사실 지금까지 우리는 CPU만 쓰고 있었음) . GPU &#49324;&#50857;&#48169;&#48277; . - cpu 연산이 가능한 메모리에 데이터 저장 . torch.manual_seed(43052) x_cpu = torch.tensor([0.0,0.1,0.2]).reshape(-1,1) y_cpu = torch.tensor([0.0,0.2,0.4]).reshape(-1,1) net_cpu = torch.nn.Linear(1,1) . - gpu 연산이 가능한 메모리에 데이터 저장 . torch.manual_seed(43052) x_gpu = x_cpu.to(&quot;cuda:0&quot;) y_gpu = y_cpu.to(&quot;cuda:0&quot;) net_gpu = torch.nn.Linear(1,1).to(&quot;cuda:0&quot;) . - cpu 혹은 gpu 연산이 가능한 메모리에 저장된 값들을 확인 . x_cpu, y_cpu, net_cpu.weight, net_cpu.bias . (tensor([[0.0000], [0.1000], [0.2000]]), tensor([[0.0000], [0.2000], [0.4000]]), Parameter containing: tensor([[-0.3467]], requires_grad=True), Parameter containing: tensor([-0.8470], requires_grad=True)) . x_gpu, y_gpu, net_gpu.weight, net_gpu.bias . (tensor([[0.0000], [0.1000], [0.2000]], device=&#39;cuda:0&#39;), tensor([[0.0000], [0.2000], [0.4000]], device=&#39;cuda:0&#39;), Parameter containing: tensor([[-0.3467]], device=&#39;cuda:0&#39;, requires_grad=True), Parameter containing: tensor([-0.8470], device=&#39;cuda:0&#39;, requires_grad=True)) . - gpu는 gpu끼리 연산가능하고 cpu는 cpu끼리 연산가능함 . (예시1) . net_cpu(x_cpu) . tensor([[-0.8470], [-0.8817], [-0.9164]], grad_fn=&lt;AddmmBackward0&gt;) . (예시2) . net_gpu(x_gpu) . tensor([[-0.8470], [-0.8817], [-0.9164]], device=&#39;cuda:0&#39;, grad_fn=&lt;AddmmBackward0&gt;) . (예시3) . net_cpu(x_gpu) . RuntimeError Traceback (most recent call last) /tmp/ipykernel_1392421/955438640.py in &lt;module&gt; -&gt; 1 net_cpu(x_gpu) ~/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs) 1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks 1129 or _global_forward_hooks or _global_forward_pre_hooks): -&gt; 1130 return forward_call(*input, **kwargs) 1131 # Do not call functions when jit is used 1132 full_backward_hooks, non_full_backward_hooks = [], [] ~/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/linear.py in forward(self, input) 112 113 def forward(self, input: Tensor) -&gt; Tensor: --&gt; 114 return F.linear(input, self.weight, self.bias) 115 116 def extra_repr(self) -&gt; str: RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm) . (예시4) . net_gpu(x_cpu) . RuntimeError Traceback (most recent call last) /tmp/ipykernel_1392421/413131407.py in &lt;module&gt; -&gt; 1 net_gpu(x_cpu) ~/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs) 1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks 1129 or _global_forward_hooks or _global_forward_pre_hooks): -&gt; 1130 return forward_call(*input, **kwargs) 1131 # Do not call functions when jit is used 1132 full_backward_hooks, non_full_backward_hooks = [], [] ~/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/linear.py in forward(self, input) 112 113 def forward(self, input: Tensor) -&gt; Tensor: --&gt; 114 return F.linear(input, self.weight, self.bias) 115 116 def extra_repr(self) -&gt; str: RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm) . (예시5) . torch.mean((y_cpu-net_cpu(x_cpu))**2) . tensor(1.2068, grad_fn=&lt;MeanBackward0&gt;) . (예시6) . torch.mean((y_gpu-net_gpu(x_gpu))**2) . tensor(1.2068, device=&#39;cuda:0&#39;, grad_fn=&lt;MeanBackward0&gt;) . (예시7) . torch.mean((y_gpu-net_cpu(x_cpu))**2) . RuntimeError Traceback (most recent call last) /tmp/ipykernel_1392421/2449708258.py in &lt;module&gt; -&gt; 1 torch.mean((y_gpu-net_cpu(x_cpu))**2) RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! . (예시8) . torch.mean((y_cpu-net_gpu(x_gpu))**2) . RuntimeError Traceback (most recent call last) /tmp/ipykernel_1392421/2827309378.py in &lt;module&gt; -&gt; 1 torch.mean((y_cpu-net_gpu(x_gpu))**2) RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! . &#49884;&#44036;&#52769;&#51221; (&#50696;&#48708;&#54617;&#49845;) . import time . t1 = time.time() . t2 = time.time() . t2-t1 . 4.9920783042907715 . CPU (512) . - 데이터준비 . torch.manual_seed(5) x=torch.linspace(0,1,100).reshape(-1,1) y=torch.randn(100).reshape(-1,1)*0.01 . - for문 준비 . net = torch.nn.Sequential( torch.nn.Linear(1,512), torch.nn.ReLU(), torch.nn.Linear(512,1) ) loss_fn = torch.nn.MSELoss() optimizr = torch.optim.Adam(net.parameters()) . - for문 + 학습시간측정 . t1= time.time() for epoc in range(1000): ## 1 yhat = net(x) ## 2 loss = loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizr.step() optimizr.zero_grad() t2 = time.time() t2-t1 . 0.28586554527282715 . GPU (512) . - 데이터준비 . torch.manual_seed(5) x=torch.linspace(0,1,100).reshape(-1,1).to(&quot;cuda:0&quot;) y=(torch.randn(100).reshape(-1,1)*0.01).to(&quot;cuda:0&quot;) . - for문돌릴준비 . net = torch.nn.Sequential( torch.nn.Linear(1,512), torch.nn.ReLU(), torch.nn.Linear(512,1) ).to(&quot;cuda:0&quot;) loss_fn = torch.nn.MSELoss() optimizr = torch.optim.Adam(net.parameters()) . - for문 + 학습시간측정 . t1= time.time() for epoc in range(1000): ## 1 yhat = net(x) ## 2 loss = loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizr.step() optimizr.zero_grad() t2 = time.time() t2-t1 . 0.5355696678161621 . !! CPU가 더 빠르다? | . CPU vs GPU (20480) . - CPU (20480) . torch.manual_seed(5) x=torch.linspace(0,1,100).reshape(-1,1) y=torch.randn(100).reshape(-1,1)*0.01 net = torch.nn.Sequential( torch.nn.Linear(1,20480), torch.nn.ReLU(), torch.nn.Linear(20480,1) ) loss_fn = torch.nn.MSELoss() optimizr = torch.optim.Adam(net.parameters()) t1= time.time() for epoc in range(1000): ## 1 yhat = net(x) ## 2 loss = loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizr.step() optimizr.zero_grad() t2 = time.time() t2-t1 . 2.380666494369507 . - GPU (20480) . torch.manual_seed(5) x=torch.linspace(0,1,100).reshape(-1,1).to(&quot;cuda:0&quot;) y=(torch.randn(100).reshape(-1,1)*0.01).to(&quot;cuda:0&quot;) net = torch.nn.Sequential( torch.nn.Linear(1,20480), torch.nn.ReLU(), torch.nn.Linear(20480,1) ).to(&quot;cuda:0&quot;) loss_fn = torch.nn.MSELoss() optimizr = torch.optim.Adam(net.parameters()) t1= time.time() for epoc in range(1000): ## 1 yhat = net(x) ## 2 loss = loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizr.step() optimizr.zero_grad() t2 = time.time() t2-t1 . 0.5442469120025635 . - 왜 이런 차이가 나는가? 연산을 하는 주체는 코어인데 CPU는 수는 적지만 일을 잘하는 코어들을 가지고 있고 GPU는 일은 못하지만 다수의 코어를 가지고 있기 때문 . CPU vs GPU (204800) . - CPU (204800) . torch.manual_seed(5) x=torch.linspace(0,1,100).reshape(-1,1) y=torch.randn(100).reshape(-1,1)*0.01 net = torch.nn.Sequential( torch.nn.Linear(1,204800), torch.nn.ReLU(), torch.nn.Linear(204800,1) ) loss_fn = torch.nn.MSELoss() optimizr = torch.optim.Adam(net.parameters()) t1= time.time() for epoc in range(1000): ## 1 yhat = net(x) ## 2 loss = loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizr.step() optimizr.zero_grad() t2 = time.time() t2-t1 . 51.95550894737244 . - GPU (204800) . torch.manual_seed(5) x=torch.linspace(0,1,100).reshape(-1,1).to(&quot;cuda:0&quot;) y=(torch.randn(100).reshape(-1,1)*0.01).to(&quot;cuda:0&quot;) net = torch.nn.Sequential( torch.nn.Linear(1,204800), torch.nn.ReLU(), torch.nn.Linear(204800,1) ).to(&quot;cuda:0&quot;) loss_fn = torch.nn.MSELoss() optimizr = torch.optim.Adam(net.parameters()) t1= time.time() for epoc in range(1000): ## 1 yhat = net(x) ## 2 loss = loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizr.step() optimizr.zero_grad() t2 = time.time() t2-t1 . 1.3824031352996826 . &#54869;&#47456;&#51201;&#44221;&#49324;&#54616;&#44053;&#48277;, &#48176;&#52824;, &#50640;&#54253; . &#51328; &#51060;&#49345;&#54616;&#51648; &#50506;&#50500;&#50836;? . - 우리가 쓰는 GPU: 다나와 PC견적 . GPU 메모리 끽해봐야 24GB | . - 우리가 분석하는 데이터: 빅데이터..? . - 데이터의 크기가 커지는순간 X.to(&quot;cuda:0&quot;), y.to(&quot;cuda:0&quot;) 쓰면 난리나겠는걸? . x = torch.linspace(-10,10,100000).reshape(-1,1) eps = torch.randn(100000).reshape(-1,1) y = x*2 + eps . plt.plot(x,y,&#39;o&#39;,alpha=0.05) plt.plot(x,2*x) . [&lt;matplotlib.lines.Line2D at 0x7f62d96f2c10&gt;] . - 데이터를 100개중에 1개만 꼴로만 쓰면 어떨까? . plt.plot(x[::100],y[::100],&#39;o&#39;,alpha=0.05) plt.plot(x,2*x) . [&lt;matplotlib.lines.Line2D at 0x7f62db305310&gt;] . 대충 이거만 가지고 적합해도 충분히 정확할것 같은데 | . X,y &#45936;&#51060;&#53552;&#47484; &#44403;&#51060; &#47784;&#46160; GPU&#50640; &#45336;&#44200;&#50556; &#54616;&#45716;&#44032;? . - 데이터셋을 짝홀로 나누어서 번갈아가면서 GPU에 올렸다 내렸다하면 안되나? . - 아래의 알고리즘을 생각해보자. . 데이터를 반으로 나눈다. | 짝수obs의 x,y 그리고 net의 모든 파라메터를 GPU에 올린다. | yhat, loss, grad, update 수행 | 짝수obs의 x,y를 GPU메모리에서 내린다. 그리고 홀수obs의 x,y를 GPU메모리에 올린다. | yhat, loss, grad, update 수행 | 홀수obs의 x,y를 GPU메모리에서 내린다. 그리고 짝수obs의 x,y를 GPU메모리에 올린다. | 반복 | &#44221;&#49324;&#54616;&#44053;&#48277;, &#54869;&#47456;&#51201;&#44221;&#49324;&#54616;&#44053;&#48277;, &#48120;&#45768;&#48176;&#52824; &#44221;&#49324;&#54616;&#44053;&#48277; . 10개의 샘플이 있다고 가정. $ {(x_i,y_i) }_{i=1}^{10}$ . - ver1: 모든 샘플을 이용하여 slope 계산 . (epoch1) $loss= sum_{i=1}^{10}(y_i- beta_0- beta_1x_i)^2 to slope to update$ . (epoch2) $loss= sum_{i=1}^{10}(y_i- beta_0- beta_1x_i)^2 to slope to update$ . ... . - ver2: 하나의 샘플만을 이용하여 slope 계산 . (epoch1) . $loss=(y_1- beta_0- beta_1x_1)^2 to slope to update$ | $loss=(y_2- beta_0- beta_1x_2)^2 to slope to update$ | ... | $loss=(y_{10}- beta_0- beta_1x_{10})^2 to slope to update$ | . (epoch2) . $loss=(y_1- beta_0- beta_1x_1)^2 to slope to update$ | $loss=(y_2- beta_0- beta_1x_2)^2 to slope to update$ | ... | $loss=(y_{10}- beta_0- beta_1x_{10})^2 to slope to update$ | . ... . - ver3: $m ( leq n)$ 개의 샘플을 이용하여 slope 계산 . $m=3$이라고 하자. . (epoch1) . $loss= sum_{i=1}^{3}(y_i- beta_0- beta_1x_i)^2 to slope to update$ | $loss= sum_{i=4}^{6}(y_i- beta_0- beta_1x_i)^2 to slope to update$ | $loss= sum_{i=7}^{9}(y_i- beta_0- beta_1x_i)^2 to slope to update$ | $loss=(y_{10}- beta_0- beta_1x_{10})^2 to slope to update$ | . (epoch2) . $loss= sum_{i=1}^{3}(y_i- beta_0- beta_1x_i)^2 to slope to update$ | $loss= sum_{i=4}^{6}(y_i- beta_0- beta_1x_i)^2 to slope to update$ | $loss= sum_{i=7}^{9}(y_i- beta_0- beta_1x_i)^2 to slope to update$ | $loss=(y_{10}- beta_0- beta_1x_{10})^2 to slope to update$ | . ... . &#50857;&#50612;&#51032; &#51221;&#47532; . 옛날 . - ver1: gradient descent, batch gradient descent . - ver2: stochastic gradient descent . - ver3: mini-batch gradient descent, mini-batch stochastic gradient descent . 요즘 . - ver1: gradient descent . - ver2: stochastic gradient descent with batch size = 1 . - ver3: stochastic gradient descent . https://www.deeplearningbook.org/contents/optimization.html, 알고리즘 8-1 참고. | . ds, dl . - ds . x=torch.tensor(range(10)).float()#.reshape(-1,1) y=torch.tensor([1.0]*5+[0.0]*5)#.reshape(-1,1) . ds=torch.utils.data.TensorDataset(x,y) ds . &lt;torch.utils.data.dataset.TensorDataset at 0x7f62db294710&gt; . ds.tensors # 그냥 (x,y)의 튜플 . (tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]), tensor([1., 1., 1., 1., 1., 0., 0., 0., 0., 0.])) . - dl . dl=torch.utils.data.DataLoader(ds,batch_size=3) #set(dir(dl)) &amp; {&#39;__iter__&#39;} . for xx,yy in dl: print(xx,yy) . tensor([0., 1., 2.]) tensor([1., 1., 1.]) tensor([3., 4., 5.]) tensor([1., 1., 0.]) tensor([6., 7., 8.]) tensor([0., 0., 0.]) tensor([9.]) tensor([0.]) . ds, dl&#51012; &#51060;&#50857;&#54620; MNIST &#44396;&#54788; . - 데이터정리 . path = untar_data(URLs.MNIST) . zero_fnames = (path/&#39;training/0&#39;).ls() one_fnames = (path/&#39;training/1&#39;).ls() . X0 = torch.stack([torchvision.io.read_image(str(zf)) for zf in zero_fnames]) X1 = torch.stack([torchvision.io.read_image(str(of)) for of in one_fnames]) X = torch.concat([X0,X1],axis=0).reshape(-1,1*28*28)/255 y = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1) . X.shape,y.shape . (torch.Size([12665, 784]), torch.Size([12665, 1])) . - ds $ to$ dl . ds = torch.utils.data.TensorDataset(X,y) dl = torch.utils.data.DataLoader(ds,batch_size=2048) . 12665/2048 . 6.18408203125 . i = 0 for xx,yy in dl: # 총 7번 돌아가는 for문 print(i) i=i+1 . 0 1 2 3 4 5 6 . - 미니배치 안쓰는 학습 . torch.manual_seed(43052) net = torch.nn.Sequential( torch.nn.Linear(784,32), torch.nn.ReLU(), torch.nn.Linear(32,1), torch.nn.Sigmoid() ) loss_fn = torch.nn.BCELoss() optimizr = torch.optim.Adam(net.parameters()) . for epoc in range(70): ## 1 yhat = net(X) ## 2 loss= loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizr.step() optimizr.zero_grad() . torch.sum((yhat&gt;0.5) == y) / len(y) . tensor(0.9981) . - 미니배치 쓰는 학습 (GPU 올리고 내리는 과정은 생략) . torch.manual_seed(43052) net = torch.nn.Sequential( torch.nn.Linear(784,32), torch.nn.ReLU(), torch.nn.Linear(32,1), torch.nn.Sigmoid() ) loss_fn = torch.nn.BCELoss() optimizr = torch.optim.Adam(net.parameters()) . for epoc in range(10): for xx,yy in dl: ## 7번 ## 1 #yhat = net(xx) ## 2 loss = loss_fn(net(xx),yy) ## 3 loss.backward() ## 4 optimizr.step() optimizr.zero_grad() . torch.mean(((net(X)&gt;0.5) == y)*1.0) . tensor(0.9950) . &#50724;&#48260;&#54588;&#54021; . - 오버피팅이란? . 위키: In mathematical modeling, overfitting is &quot;the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit to additional data or predict future observations reliably&quot;. | 제 개념: 데이터를 &quot;데이터 = 언더라잉 + 오차&quot;라고 생각할때 우리가 데이터로부터 적합할 것은 언더라잉인데 오차항을 적합하고 있는 현상. | . &#50724;&#48260;&#54588;&#54021; &#50696;&#49884; . - $m$이 매우 클때 아래의 네트워크 거의 무엇이든 맞출수 있다고 보면 된다. . $ underset{(n,1)}{ bf X} overset{l_1}{ to} underset{(n,m)}{ boldsymbol u^{(1)}} overset{h}{ to} underset{(n,m)}{ boldsymbol v^{(1)}} overset{l_2}{ to} underset{(n,1)}{ hat{ boldsymbol y}}$ | $ underset{(n,1)}{ bf X} overset{l_1}{ to} underset{(n,m)}{ boldsymbol u^{(1)}} overset{sig}{ to} underset{(n,m)}{ boldsymbol v^{(1)}} overset{l_2}{ to} underset{(n,1)}{ hat{ boldsymbol y}}$ | $ underset{(n,1)}{ bf X} overset{l_1}{ to} underset{(n,m)}{ boldsymbol u^{(1)}} overset{relu}{ to} underset{(n,m)}{ boldsymbol v^{(1)}} overset{l_2}{ to} underset{(n,1)}{ hat{ boldsymbol y}}$ | . - 그런데 종종 맞추지 말아야 할 것들도 맞춘다. . model: $y_i = (0 times x_i) + epsilon_i$, where $ epsilon_i sim N(0,0.01^2)$ . torch.manual_seed(5) x=torch.linspace(0,1,100).reshape(100,1) y=torch.randn(100).reshape(100,1)*0.01 plt.plot(x,y) . [&lt;matplotlib.lines.Line2D at 0x7f62e48fc190&gt;] . y는 그냥 정규분포에서 생성한 오차이므로 $X to y$ 로 항햐는 규칙따위는 없음 | . torch.manual_seed(1) net=torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=512), torch.nn.ReLU(), torch.nn.Linear(in_features=512,out_features=1)) optimizer= torch.optim.Adam(net.parameters()) loss_fn= torch.nn.MSELoss() for epoc in range(1000): ## 1 yhat=net(x) ## 2 loss=loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . plt.plot(x,y) plt.plot(x,net(x).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f62e4933310&gt;] . 우리는 데이터를 랜덤에서 뽑았는데, 데이터의 추세를 따라간다 $ to$ 오버피팅 (underlying이 아니라 오차항을 따라가고 있음) | . &#50724;&#48260;&#54588;&#54021;&#51060;&#46972;&#45716; &#46748;&#47159;&#54620; &#51613;&#44144;! (train / test) . - 데이터의 분리하여 보자. . torch.manual_seed(5) x=torch.linspace(0,1,100).reshape(100,1) y=torch.randn(100).reshape(100,1)*0.01 xtr = x[:80] ytr = y[:80] xtest = x[80:] ytest = y[80:] plt.plot(xtr,ytr) plt.plot(xtest,ytest) plt.title(&#39;train: blue / test: orange&#39;); . - train만 학습 . torch.manual_seed(1) net1=torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=512), torch.nn.ReLU(), torch.nn.Linear(in_features=512,out_features=1)) optimizr1= torch.optim.Adam(net1.parameters()) loss_fn= torch.nn.MSELoss() for epoc in range(1000): ## 1 # net(xtr) ## 2 loss=loss_fn(net1(xtr),ytr) ## 3 loss.backward() ## 4 optimizr1.step() optimizr1.zero_grad() . - training data로 학습한 net를 training data 에 적용 . plt.plot(x,y,alpha=0.5) plt.plot(xtr,net1(xtr).data,&#39;--&#39;) # prediction (train) . [&lt;matplotlib.lines.Line2D at 0x7f62de905190&gt;] . train에서는 잘 맞추는듯이 보인다. | . - training data로 학습한 net를 test data 에 적용 . plt.plot(x,y,alpha=0.5) plt.plot(xtr,net1(xtr).data,&#39;--&#39;) # prediction (train) plt.plot(xtest,net1(xtest).data,&#39;--&#39;) # prediction with unseen data (test) . [&lt;matplotlib.lines.Line2D at 0x7f62de084550&gt;] . train은 그럭저럭 따라가지만 test에서는 엉망이다. $ to$ overfit | .",
            "url": "https://guebin.github.io/STML2022/2022/10/11/(6%EC%A3%BC%EC%B0%A8)-10%EC%9B%9411%EC%9D%BC.html",
            "relUrl": "/2022/10/11/(6%EC%A3%BC%EC%B0%A8)-10%EC%9B%9411%EC%9D%BC.html",
            "date": " • Oct 11, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "(5주차) 10월5일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . imports . import torch import pandas as pd import numpy as np import matplotlib.pyplot as plt . &#49884;&#44033;&#54868;&#47484; &#50948;&#54620; &#51456;&#48708;&#54632;&#49688;&#46308; . 준비1 loss_fn을 plot하는 함수 . def plot_loss(loss_fn,ax=None): if ax==None: fig = plt.figure() ax=fig.add_subplot(1,1,1,projection=&#39;3d&#39;) ax.elev=15;ax.azim=75 w0hat,w1hat =torch.meshgrid(torch.arange(-10,3,0.15),torch.arange(-1,10,0.15),indexing=&#39;ij&#39;) w0hat = w0hat.reshape(-1) w1hat = w1hat.reshape(-1) def l(w0hat,w1hat): yhat = torch.exp(w0hat+w1hat*x)/(1+torch.exp(w0hat+w1hat*x)) return loss_fn(yhat,y) loss = list(map(l,w0hat,w1hat)) ax.scatter(w0hat,w1hat,loss,s=0.1,alpha=0.2) ax.scatter(-1,5,l(-1,5),s=200,marker=&#39;*&#39;) # 실제로 -1,5에서 최소값을 가지는건 아님.. . $y_i sim Ber( pi_i), quad $ where $ pi_i = frac{ exp(-1+5x_i)}{1+ exp(-1+5x_i)}$ 에서 생성된 데이터 한정하여 손실함수가 그려지게 되어있음. | . 준비2: for문 대신 돌려주고 epoch마다 필요한 정보를 기록하는 함수 . def learn_and_record(net, loss_fn, optimizr): yhat_history = [] loss_history = [] what_history = [] for epoc in range(1000): ## step1 yhat = net(x) ## step2 loss = loss_fn(yhat,y) ## step3 loss.backward() ## step4 optimizr.step() optimizr.zero_grad() ## record if epoc % 20 ==0: yhat_history.append(yhat.reshape(-1).data.tolist()) loss_history.append(loss.item()) what_history.append([net[0].bias.data.item(), net[0].weight.data.item()]) return yhat_history, loss_history, what_history . 20에폭마다 yhat, loss, what을 기록 | . 준비3: 애니메이션을 만들어주는 함수 . from matplotlib import animation plt.rcParams[&quot;animation.html&quot;] = &quot;jshtml&quot; . def show_lrpr2(net,loss_fn,optimizr,suptitle=&#39;&#39;): yhat_history,loss_history,what_history = learn_and_record(net,loss_fn,optimizr) fig = plt.figure(figsize=(7,2.5)) ax1 = fig.add_subplot(1, 2, 1) ax2 = fig.add_subplot(1, 2, 2, projection=&#39;3d&#39;) ax1.set_xticks([]);ax1.set_yticks([]) ax2.set_xticks([]);ax2.set_yticks([]);ax2.set_zticks([]) ax2.elev = 15; ax2.azim = 75 ## ax1: 왼쪽그림 ax1.plot(x,v,&#39;--&#39;) ax1.scatter(x,y,alpha=0.05) line, = ax1.plot(x,yhat_history[0],&#39;--&#39;) plot_loss(loss_fn,ax2) fig.suptitle(suptitle) fig.tight_layout() def animate(epoc): line.set_ydata(yhat_history[epoc]) ax2.scatter(np.array(what_history)[epoc,0],np.array(what_history)[epoc,1],loss_history[epoc],color=&#39;grey&#39;) return line ani = animation.FuncAnimation(fig, animate, frames=30) plt.close() return ani . 준비1에서 그려진 loss 함수위에, 준비2의 정보를 조합하여 애니메이션을 만들어주는 함수 | . Logistic intro (review + $ alpha$) . - 모델: $x$가 커질수록 $y=1$이 잘나오는 모형은 아래와 같이 설계할 수 있음 &lt; 외우세요!!! . $y_i sim Ber( pi_i), quad $ where $ pi_i = frac{ exp(w_0+w_1x_i)}{1+ exp(w_0+w_1x_i)}$ . | $ hat{y}_i= frac{ exp( hat{w}_0+ hat{w}_1x_i)}{1+ exp( hat{w}_0+ hat{w}_1x_i)}= frac{1}{1+ exp(- hat{w}_0- hat{w}_1x_i)}$ . | $loss= - sum_{i=1}^{n} big(y_i log( hat{y}_i)+(1-y_i) log(1- hat{y}_i) big)$ &lt; 외우세요!! . | . - toy example . x=torch.linspace(-1,1,2000).reshape(2000,1) w0= -1 w1= 5 u = w0+x*w1 v = torch.exp(u)/(1+torch.exp(u)) # v=πi, 즉 확률을 의미함 y = torch.bernoulli(v) . plt.plot(x,y,&#39;o&#39;,alpha=0.05) plt.plot(x,v,&#39;--b&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f9e75d255d0&gt;] . - step1: yhat을 만들기 . (방법1) . torch.manual_seed(43052) l1=torch.nn.Linear(1,1) a1=torch.nn.Sigmoid() yhat= a1(l1(x)) # yhat=net(x) 라고 쓰고싶어 사실 yhat . tensor([[0.3775], [0.3774], [0.3773], ..., [0.2327], [0.2327], [0.2326]], grad_fn=&lt;SigmoidBackward0&gt;) . (방법2) . $x overset{l1}{ to} u overset{a1}{ to} v = hat{y}$ . $x overset{net}{ to} hat{y}$ . torch.manual_seed(43052) l1=torch.nn.Linear(1,1) a1=torch.nn.Sigmoid() net = torch.nn.Sequential(l1,a1) yhat= net(x) # a1(l1(x)) yhat . tensor([[0.3775], [0.3774], [0.3773], ..., [0.2327], [0.2327], [0.2326]], grad_fn=&lt;SigmoidBackward0&gt;) . (방법3) . torch.manual_seed(43052) net = torch.nn.Sequential( torch.nn.Linear(1,1), torch.nn.Sigmoid() ) yhat= net(x) # a1(l1(x)) yhat . tensor([[0.3775], [0.3774], [0.3773], ..., [0.2327], [0.2327], [0.2326]], grad_fn=&lt;SigmoidBackward0&gt;) . - step2 . (방법1) . loss = torch.mean((y-yhat)**2) loss . tensor(0.2872, grad_fn=&lt;MeanBackward0&gt;) . (방법2) . loss_fn = torch.nn.MSELoss() loss = loss_fn(yhat,y) loss . tensor(0.2872, grad_fn=&lt;MseLossBackward0&gt;) . - step 1~4 . torch.manual_seed(43052) net = torch.nn.Sequential( torch.nn.Linear(1,1), torch.nn.Sigmoid() ) loss_fn = torch.nn.MSELoss() optimizr = torch.optim.SGD(net.parameters(),lr=0.01) . plt.plot(x,y,&#39;o&#39;,alpha=0.01) plt.plot(x,v,&#39;--b&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f9e737c1810&gt;] . for epoc in range(3000): ## 1 yhat = net(x) ## 2 loss = loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizr.step() optimizr.zero_grad() . plt.plot(x,y,&#39;o&#39;,alpha=0.01) plt.plot(x,v,&#39;--b&#39;) plt.plot(x,net(x).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f9e74169150&gt;] . &#47196;&#51648;&#49828;&#54001;--BCEloss . - step 1~4 . torch.manual_seed(43052) net = torch.nn.Sequential( torch.nn.Linear(1,1), torch.nn.Sigmoid() ) loss_fn = torch.nn.BCELoss() optimizr = torch.optim.SGD(net.parameters(),lr=0.01) . plt.plot(x,y,&#39;o&#39;,alpha=0.01) plt.plot(x,v,&#39;--b&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f9e72ad9f90&gt;] . for epoc in range(3000): ## 1 yhat = net(x) ## 2 loss = loss_fn(yhat,y) # -torch.mean(y*torch.log(yhat) + (1-y)*torch.log(1-yhat)) ## 3 loss.backward() ## 4 optimizr.step() optimizr.zero_grad() . plt.plot(x,y,&#39;o&#39;,alpha=0.01) plt.plot(x,v,&#39;--b&#39;) plt.plot(x,net(x).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f9e7292ad10&gt;] . - 왜?? &quot;linear -&gt; sigmoid&quot; 로 yhat을 구하고 BCELoss로 loss를 계산하면 그 모양이 convex하게 되므로 . plot_loss(torch.nn.MSELoss()) . plot_loss(torch.nn.BCELoss()) . &#49884;&#44033;&#54868;1: MSE, &#51339;&#51008;&#52488;&#44592;&#44050; . net = torch.nn.Sequential( torch.nn.Linear(1,1), torch.nn.Sigmoid() ) loss_fn = torch.nn.MSELoss() optimizr = torch.optim.SGD(net.parameters(),lr=0.05) . l1,a1 = net . l1.weight.data = torch.tensor([[-1.0]]) l1.bias.data = torch.tensor([-3.0]) . show_lrpr2(net,loss_fn,optimizr,&#39;MSELoss, good_init // SGD&#39;) . Once Loop Reflect &#49884;&#44033;&#54868;2: MSE, &#45208;&#49244;&#52488;&#44592;&#44050; . net = torch.nn.Sequential( torch.nn.Linear(1,1), torch.nn.Sigmoid() ) loss_fn = torch.nn.MSELoss() optimizr = torch.optim.SGD(net.parameters(),lr=0.05) . l1,a1 = net . l1.weight.data = torch.tensor([[-1.0]]) l1.bias.data = torch.tensor([-10.0]) . show_lrpr2(net,loss_fn,optimizr,&#39;MSELoss, bad_init // SGD&#39;) . &#49884;&#44033;&#54868;3: BCE, &#51339;&#51008;&#52488;&#44592;&#44050; . net = torch.nn.Sequential( torch.nn.Linear(1,1), torch.nn.Sigmoid() ) loss_fn = torch.nn.BCELoss() optimizr = torch.optim.SGD(net.parameters(),lr=0.05) . l1,a1 = net . l1.weight.data = torch.tensor([[-1.0]]) l1.bias.data = torch.tensor([-3.0]) . show_lrpr2(net,loss_fn,optimizr,&#39;BCELoss, good_init // SGD&#39;) . Once Loop Reflect &#49884;&#44033;&#54868;4: BCE, &#45208;&#49244;&#52488;&#44592;&#44050; . net = torch.nn.Sequential( torch.nn.Linear(1,1), torch.nn.Sigmoid() ) loss_fn = torch.nn.BCELoss() optimizr = torch.optim.SGD(net.parameters(),lr=0.05) . l1,a1 = net . l1.weight.data = torch.tensor([[-1.0]]) l1.bias.data = torch.tensor([-10.0]) . show_lrpr2(net,loss_fn,optimizr,&#39;BCELoss, bad_init // SGD&#39;) . Once Loop Reflect &#47196;&#51648;&#49828;&#54001;--Adam (&#44397;&#48124;&#50741;&#54000;&#47560;&#51060;&#51200;) . - Adam은 SGD에 비하여 2가지 면에서 개선점이 있음. . 몰라도 됩니다.. | 가속도의 개념 | &#49884;&#44033;&#54868;1: MSE, &#51339;&#51008;&#52488;&#44592;&#44050; --&gt; &#51060;&#44152; &#50500;&#45812;&#51004;&#47196;! . net = torch.nn.Sequential( torch.nn.Linear(1,1), torch.nn.Sigmoid() ) loss_fn = torch.nn.MSELoss() optimizr = torch.optim.Adam(net.parameters(),lr=0.05) . l1,a1 = net . l1.weight.data = torch.tensor([[-1.0]]) l1.bias.data = torch.tensor([-3.0]) . show_lrpr2(net,loss_fn,optimizr,&#39;MSELoss, good_init // Adam&#39;) . Once Loop Reflect &#49884;&#44033;&#54868;2: MSE, &#45208;&#49244;&#52488;&#44592;&#44050; --&gt; &#51060;&#44152; &#50500;&#45812;&#51004;&#47196;! (&#54844;&#51088;&#54644;&#48400;&#50836;..) . &#49884;&#44033;&#54868;3: BCE, &#51339;&#51008;&#52488;&#44592;&#44050; --&gt; &#51060;&#44152; &#50500;&#45812;&#51004;&#47196;! (&#54844;&#51088;&#54644;&#48400;&#50836;..) . &#49884;&#44033;&#54868;4: BCE, &#45208;&#49244;&#52488;&#44592;&#44050; --&gt; &#51060;&#44152; &#50500;&#45812;&#51004;&#47196;! (&#54844;&#51088;&#54644;&#48400;&#50836;..) . &#44618;&#51008;&#49888;&#44221;&#47581;--&#47196;&#51648;&#49828;&#54001; &#54924;&#44480;&#51032; &#54620;&#44228; . &#49888;&#47928;&#44592;&#49324; (&#45936;&#51060;&#53552;&#51032; &#47784;&#54000;&#48652;) . - 스펙이 높아도 취업이 안된다고 합니다.. . 중소·지방 기업 &quot;뽑아봤자 그만두니까&quot; . 중소기업 관계자들은 고스펙 지원자를 꺼리는 이유로 높은 퇴직률을 꼽는다. 여건이 좋은 대기업으로 이직하거나 회사를 관두는 경우가 많다는 하소연이다. 고용정보원이 지난 3일 공개한 자료에 따르면 중소기업 청년취업자 가운데 49.5%가 2년 내에 회사를 그만두는 것으로 나타났다. . 중소 IT업체 관계자는 &quot;기업 입장에서 가장 뼈아픈 게 신입사원이 그만둬서 새로 뽑는 일&quot;이라며 &quot;명문대 나온 스펙 좋은 지원자를 뽑아놔도 1년을 채우지 않고 그만두는 사원이 대부분이라 우리도 눈을 낮춰 사람을 뽑는다&quot;고 말했다. . &#44032;&#51676;&#45936;&#51060;&#53552; . - 위의 기사를 모티브로 한 데이터 . df=pd.read_csv(&#39;https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-10-04-dnnex0.csv&#39;) df . x underlying y . 0 -1.000000 | 0.000045 | 0.0 | . 1 -0.998999 | 0.000046 | 0.0 | . 2 -0.997999 | 0.000047 | 0.0 | . 3 -0.996998 | 0.000047 | 0.0 | . 4 -0.995998 | 0.000048 | 0.0 | . ... ... | ... | ... | . 1995 0.995998 | 0.505002 | 0.0 | . 1996 0.996998 | 0.503752 | 0.0 | . 1997 0.997999 | 0.502501 | 0.0 | . 1998 0.998999 | 0.501251 | 1.0 | . 1999 1.000000 | 0.500000 | 1.0 | . 2000 rows × 3 columns . plt.plot(df.x,df.y,&#39;o&#39;,alpha=0.01) plt.plot(df.x,df.underlying, &#39;--b&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f86c20db310&gt;] . &#47196;&#51648;&#49828;&#54001; &#54924;&#44480;&#47196; &#51201;&#54633; . x=torch.tensor(df.x).float().reshape(-1,1) y=torch.tensor(df.y).float().reshape(-1,1) . torch.manual_seed(43052) net = torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=1,bias=True), torch.nn.Sigmoid() ) loss_fn = torch.nn.BCELoss() optimizr = torch.optim.Adam(net.parameters()) . for epoc in range(6000): ## step1 yhat = net(x) ## step2 loss = loss_fn(yhat,y) ## step3 loss.backward() ## step4 optimizr.step() optimizr.zero_grad() . plt.plot(x,y,&#39;o&#39;,alpha=0.01) plt.plot(df.x,df.underlying,&#39;--b&#39;) plt.plot(x,net(x).data, &#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f86c0fc3d50&gt;] . - 이거는 epoc=60억해도 주황색 점선은 절대 파란색 점선이 될 수 없다. . &#54644;&#44208;&#52293; . - sigmoid를 취하기 전의 상태가 꺽인 그래프여야 한다. . sig = torch.nn.Sigmoid() . plt.plot(sig(torch.tensor([-6,-2,2,6.0,3,0])),&#39;--o&#39; ) . [&lt;matplotlib.lines.Line2D at 0x7f86acaf14d0&gt;] . &#44618;&#51008;&#49888;&#44221;&#47581;--DNN&#51012; &#51060;&#50857;&#54620; &#54644;&#44208; . - 목표: 아래와 같은 벡터 ${ boldsymbol u}$를 만들어보자. . ${ boldsymbol u} = [u_1,u_2, dots,u_{2000}], quad u_i = begin{cases} 9x_i +4.5&amp; x_i &lt;0 -4.5x_i + 4.5&amp; x_i &gt;0 end{cases}$ . &#44733;&#51064; &#44536;&#47000;&#54532;&#47484; &#47564;&#46300;&#45716; &#48169;&#48277;1 . u= [9*xi+4.5 if xi&lt;0 else -4.5*xi+4.5 for xi in x.reshape(-1).tolist()] plt.plot(u,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f86ac94f810&gt;] . &#44733;&#51064; &#44536;&#47000;&#54532;&#47484; &#47564;&#46300;&#45716; &#48169;&#48277;2 . - 전략: 선형변환 $ to$ ReLU $ to$ 선형변환 . (예비학습) ReLU 함수란? . $ReLU(x) = max(0,x)$ . rlu =torch.nn.ReLU() . plt.plot(x-0.5) plt.plot(rlu(x-0.5)) . [&lt;matplotlib.lines.Line2D at 0x7f86ac7198d0&gt;] . (선형변환1) . plt.plot(x) plt.plot(-x) . [&lt;matplotlib.lines.Line2D at 0x7f86ac5ca610&gt;] . (렐루) . plt.plot(x,alpha=0.5,color=&#39;C0&#39;) plt.plot(-x,alpha=0.5,color=&#39;C1&#39;) plt.plot(rlu(x),color=&#39;C0&#39;) plt.plot(rlu(-x),color=&#39;C1&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f86ac11c690&gt;] . (선형변환2) . plt.plot(x,alpha=0.2,color=&#39;C0&#39;) plt.plot(-x,alpha=0.2,color=&#39;C1&#39;) plt.plot(rlu(x),color=&#39;C0&#39;,alpha=0.2) plt.plot(rlu(-x),color=&#39;C1&#39;,alpha=0.2) plt.plot(-4.5*rlu(x)+-9*rlu(-x)+4.5,color=&#39;C2&#39;) #plt.plot(u) . [&lt;matplotlib.lines.Line2D at 0x7f86a7838410&gt;] . (시그모이드) . plt.plot(sig(-4.5*rlu(x)+-9*rlu(-x)+4.5),color=&#39;C2&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f86a773d050&gt;] . 정리하면! . fig = plt.figure(figsize=(8, 4)) spec = fig.add_gridspec(4, 4) ax1 = fig.add_subplot(spec[:2,0]); ax1.set_title(&#39;x&#39;); ax1.plot(x,&#39;--&#39;,color=&#39;C0&#39;) ax2 = fig.add_subplot(spec[2:,0]); ax2.set_title(&#39;-x&#39;); ax2.plot(-x,&#39;--&#39;,color=&#39;C1&#39;) ax3 = fig.add_subplot(spec[:2,1]); ax3.set_title(&#39;relu(x)&#39;); ax3.plot(rlu(x),&#39;--&#39;,color=&#39;C0&#39;) ax4 = fig.add_subplot(spec[2:,1]); ax4.set_title(&#39;relu(-x)&#39;); ax4.plot(rlu(-x),&#39;--&#39;,color=&#39;C1&#39;) ax5 = fig.add_subplot(spec[1:3,2]); ax5.set_title(&#39;u&#39;); ax5.plot(-4.5*rlu(x)-9*rlu(-x)+4.5,&#39;--&#39;,color=&#39;C2&#39;) ax6 = fig.add_subplot(spec[1:3,3]); ax6.set_title(&#39;yhat&#39;); ax6.plot(sig(-4.5*rlu(x)-9*rlu(-x)+4.5),&#39;--&#39;,color=&#39;C2&#39;) fig.tight_layout() . 이런느낌으로 $ hat{ boldsymbol y}$을 만들면 된다. | . torch.nn.Linear()&#47484; &#51060;&#50857;&#54620; &#44733;&#51064; &#44536;&#47000;&#54532; &#44396;&#54788; . - 구현 . l1 = torch.nn.Linear(in_features=1,out_features=2,bias=True) a1 = torch.nn.ReLU() l2 = torch.nn.Linear(in_features=2,out_features=1,bias=True) a2 = torch.nn.Sigmoid() . l1.weight.data = torch.tensor([[1.0],[-1.0]]) l1.bias.data = torch.tensor([0.0, 0.0]) . l2.weight.data= torch.tensor([[-4.5,-9.0]]) l2.bias.data = torch.tensor([4.5]) . net = torch.nn.Sequential(l1,a1,l2,a2) . plt.plot(y,&#39;o&#39;,alpha=0.02) plt.plot(df.underlying,&#39;--b&#39;) #plt.plot(a2(l2(a1(l1(x)))).data) plt.plot(net(x).data) . [&lt;matplotlib.lines.Line2D at 0x7f86a70becd0&gt;] . - 수식표현 . ${ bf X}= begin{bmatrix} x_1 dots x_n end{bmatrix}$ . | $l_1({ bf X})={ bf X}{ bf W}^{(1)} overset{bc}{+} { boldsymbol b}^{(1)}= begin{bmatrix} x_1 &amp; -x_1 x_2 &amp; -x_2 dots &amp; dots x_n &amp; -x_n end{bmatrix}$ . ${ bf W}^{(1)}= begin{bmatrix} 1 &amp; -1 end{bmatrix}$ | ${ boldsymbol b}^{(1)}= begin{bmatrix} 0 &amp; 0 end{bmatrix}$ | . | $(a_1 circ l_1)({ bf X})= text{relu} big({ bf X}{ bf W}^{(1)} overset{bc}{+}{ boldsymbol b}^{(1)} big)= begin{bmatrix} text{relu}(x_1) &amp; text{relu}(-x_1) text{relu}(x_2) &amp; text{relu}(-x_2) dots &amp; dots text{relu}(x_n) &amp; text{relu}(-x_n) end{bmatrix}$ . | $(l_2 circ a_1 circ l_1)({ bf X})= text{relu} big({ bf X}{ bf W}^{(1)} overset{bc}{+}{ boldsymbol b}^{(1)} big){ bf W}^{(2)} overset{bc}{+}b^{(2)} = begin{bmatrix} -4.5 times text{relu}(x_1) -9.0 times text{relu}(-x_1) +4.5 -4.5 times text{relu}(x_2) -9.0 times text{relu}(-x_2) + 4.5 dots -4.5 times text{relu}(x_n) -9.0 times text{relu}(-x_n)+4.5 end{bmatrix}$ . ${ bf W}^{(2)}= begin{bmatrix} -4.5 -9 end{bmatrix}$ | $b^{(2)}=4.5$ | . | $net({ bf X})=(a_2 circ l_2 circ a_1 circ l_1)({ bf X})= text{sig} Big( text{relu} big({ bf X}{ bf W}^{(1)} overset{bc}{+}{ boldsymbol b}^{(1)} big){ bf W}^{(2)} overset{bc}{+}b^{(2)} Big) = begin{bmatrix} text{sig} Big(-4.5 times text{relu}(x_1) -9.0 times text{relu}(-x_1) +4.5 Big) text{sig} Big(-4.5 times text{relu}(x_2) -9.0 times text{relu}(-x_2) + 4.5 Big) dots text{sig} Big(-4.5 times text{relu}(x_n) -9.0 times text{relu}(-x_n)+4.5 Big) end{bmatrix}$ . | - 차원만 따지자 . $ underset{(n,1)}{ bf X} overset{l_1}{ to} underset{(n,2)}{ boldsymbol u^{(1)}} overset{a_1}{ to} underset{(n,2)}{ boldsymbol v^{(1)}} overset{l_2}{ to} underset{(n,1)}{ boldsymbol u^{(2)}} overset{a_2}{ to} underset{(n,1)}{ boldsymbol v^{(2)}}= underset{(n,1)}{ hat{ boldsymbol y}}$ . Step1 ~ Step4 . torch.manual_seed(43052) net = torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=2), torch.nn.ReLU(), torch.nn.Linear(in_features=2,out_features=1), torch.nn.Sigmoid() ) loss_fn = torch.nn.BCELoss() optimizr = torch.optim.Adam(net.parameters()) . for epoc in range(3000): ## 1 yhat = net(x) ## 2 loss = loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizr.step() optimizr.zero_grad() . plt.plot(y,&#39;o&#39;,alpha=0.02) plt.plot(df.underlying,&#39;--b&#39;) plt.plot(net(x).data) . [&lt;matplotlib.lines.Line2D at 0x7f86a65d9c90&gt;] . - 추가로 3000번 더 . for epoc in range(3000): ## 1 yhat = net(x) ## 2 loss = loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizr.step() optimizr.zero_grad() . plt.plot(y,&#39;o&#39;,alpha=0.02) plt.plot(df.underlying,&#39;--b&#39;) plt.plot(net(x).data) . [&lt;matplotlib.lines.Line2D at 0x7f86a658e710&gt;] . &#44618;&#51008;&#49888;&#44221;&#47581;--DNN&#51004;&#47196; &#54644;&#44208;&#44032;&#45733;&#54620; &#45796;&#50577;&#54620; &#50696;&#51228; . &#50696;&#51228;1 . df = pd.read_csv(&#39;https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-10-04-dnnex1.csv&#39;) df . x underlying y . 0 -1.000000 | 0.999877 | 1.0 | . 1 -0.998999 | 0.999875 | 1.0 | . 2 -0.997999 | 0.999873 | 1.0 | . 3 -0.996998 | 0.999871 | 1.0 | . 4 -0.995998 | 0.999869 | 1.0 | . ... ... | ... | ... | . 1995 0.995998 | 0.000123 | 0.0 | . 1996 0.996998 | 0.000123 | 0.0 | . 1997 0.997999 | 0.000123 | 0.0 | . 1998 0.998999 | 0.000123 | 0.0 | . 1999 1.000000 | 0.000123 | 0.0 | . 2000 rows × 3 columns . plt.plot(df.x,df.y,&#39;o&#39;,alpha=0.05) plt.plot(df.x,df.underlying,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f5026f5b0d0&gt;] . - 데이터정리 . x= torch.tensor(df.x).float().reshape(-1,1) y= torch.tensor(df.y).float().reshape(-1,1) . - for문을 돌리기 위한 준비 . torch.manual_seed(43052) net = torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=32), torch.nn.ReLU(), torch.nn.Linear(in_features=32,out_features=1), torch.nn.Sigmoid() ) loss_fn = torch.nn.BCELoss() optimizr = torch.optim.Adam(net.parameters()) . for epoc in range(20000): ## 1 yhat = net(x) ## 2 loss = loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizr.step() optimizr.zero_grad() . plt.plot(x,y,&#39;o&#39;,alpha=0.01) plt.plot(df.x,df.underlying,&#39;--b&#39;) plt.plot(x,net(x).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f5024c00c50&gt;] . &#50696;&#51228;2 . df = pd.read_csv(&#39;https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-10-04-dnnex2.csv&#39;) df . x underlying y . 0 -1.000000 | 14.791438 | 14.486265 | . 1 -0.999000 | 14.756562 | 14.832600 | . 2 -0.997999 | 14.721663 | 15.473211 | . 3 -0.996999 | 14.686739 | 14.757734 | . 4 -0.995998 | 14.651794 | 15.042901 | . ... ... | ... | ... | . 1995 0.995998 | 5.299511 | 5.511416 | . 1996 0.996999 | 5.322140 | 6.022263 | . 1997 0.997999 | 5.344736 | 4.989637 | . 1998 0.999000 | 5.367299 | 5.575369 | . 1999 1.000000 | 5.389829 | 5.466730 | . 2000 rows × 3 columns . plt.plot(df.x,df.y,&#39;o&#39;,alpha=0.05) . [&lt;matplotlib.lines.Line2D at 0x7f5025afd750&gt;] . - 데이터준비 . x= torch.tensor(df.x).float().reshape(-1,1) y= torch.tensor(df.y).float().reshape(-1,1) . - for문 돌릴 준비 . torch.manual_seed(43052) net = torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=32), torch.nn.ReLU(), torch.nn.Linear(in_features=32,out_features=1) ) loss_fn = torch.nn.MSELoss() optimizr = torch.optim.Adam(net.parameters()) . - step1~4 . for epoc in range(20000): ## 1 yhat = net(x) ## 2 loss = loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizr.step() optimizr.zero_grad() . plt.plot(df.x,df.y,&#39;o&#39;,alpha=0.05) plt.plot(df.x,df.underlying,&#39;--b&#39;) plt.plot(x,net(x).data,&#39;--&#39;,lw=5) . [&lt;matplotlib.lines.Line2D at 0x7f5020796850&gt;] . - for문 돌릴 준비 . torch.manual_seed(5) net = torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=32), torch.nn.ReLU(), torch.nn.Linear(in_features=32,out_features=1) ) loss_fn = torch.nn.MSELoss() optimizr = torch.optim.Adam(net.parameters()) . - step1~4 . for epoc in range(20000): ## 1 yhat = net(x) ## 2 loss = loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizr.step() optimizr.zero_grad() . plt.plot(df.x,df.y,&#39;o&#39;,alpha=0.05) plt.plot(df.x,df.underlying,&#39;--b&#39;) plt.plot(x,net(x).data,&#39;--&#39;,lw=5) . [&lt;matplotlib.lines.Line2D at 0x7f5020774790&gt;] . &#50696;&#51228;3 . import seaborn as sns . df = pd.read_csv(&#39;https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-10-04-dnnex3.csv&#39;) df . x1 x2 y . 0 -0.874139 | 0.210035 | 0.0 | . 1 -1.143622 | -0.835728 | 1.0 | . 2 -0.383906 | -0.027954 | 0.0 | . 3 2.131652 | 0.748879 | 1.0 | . 4 2.411805 | 0.925588 | 1.0 | . ... ... | ... | ... | . 1995 -0.002797 | -0.040410 | 0.0 | . 1996 -1.003506 | 1.182736 | 0.0 | . 1997 1.388121 | 0.079317 | 0.0 | . 1998 0.080463 | 0.816024 | 1.0 | . 1999 -0.416859 | 0.067907 | 0.0 | . 2000 rows × 3 columns . sns.scatterplot(data=df,x=&#39;x1&#39;,y=&#39;x2&#39;,hue=&#39;y&#39;) . &lt;AxesSubplot:xlabel=&#39;x1&#39;, ylabel=&#39;x2&#39;&gt; . - 데이터준비 . x1= torch.tensor(df.x1).float().reshape(-1,1) x2= torch.tensor(df.x2).float().reshape(-1,1) X = torch.concat([x1,x2],axis=1) y= torch.tensor(df.y).float().reshape(-1,1) . - for문 돌릴 준비 . torch.manual_seed(43052) net = torch.nn.Sequential( torch.nn.Linear(in_features=2,out_features=64), torch.nn.ReLU(), torch.nn.Linear(in_features=64,out_features=1), torch.nn.Sigmoid() ) loss_fn = torch.nn.BCELoss() optimizr = torch.optim.Adam(net.parameters()) . for epoc in range(20000): ## 1 yhat = net(X) ## 2 loss = loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizr.step() optimizr.zero_grad() . df2= df.assign(yhat=yhat.data.reshape(-1)) . sns.scatterplot(data=df2,x=&#39;x1&#39;,y=&#39;x2&#39;,hue=&#39;yhat&#39;) . &lt;AxesSubplot:xlabel=&#39;x1&#39;, ylabel=&#39;x2&#39;&gt; .",
            "url": "https://guebin.github.io/STML2022/2022/10/05/(5%EC%A3%BC%EC%B0%A8)-10%EC%9B%945%EC%9D%BC.html",
            "relUrl": "/2022/10/05/(5%EC%A3%BC%EC%B0%A8)-10%EC%9B%945%EC%9D%BC.html",
            "date": " • Oct 5, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "(4주차) 9월28일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . imports . import numpy as np import matplotlib.pyplot as plt import pandas as pd import torch . numpy, torch (&#49440;&#53469;&#54617;&#49845;) . numpy, torch&#45716; &#50628;&#52397; &#48708;&#49847;&#54644;&#50836; . - torch.tensor() = np.array() 처럼 생각해도 무방 . np.array([1,2,3]), torch.tensor([1,2,3]) . (array([1, 2, 3]), tensor([1, 2, 3])) . - 소수점의 정밀도에서 차이가 있음 (torch가 좀 더 쪼잔함) . np.array([3.123456789]) . array([3.12345679]) . torch.tensor([3.123456789]) . tensor([3.1235]) . - 기본적인 numpy 문법은 np 대신에 torch를 써도 무방 // 완전 같지는 않음 . np.arange(10), torch.arange(10) . (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])) . np.linspace(0,1,10), torch.linspace(0,1,10) . (array([0. , 0.11111111, 0.22222222, 0.33333333, 0.44444444, 0.55555556, 0.66666667, 0.77777778, 0.88888889, 1. ]), tensor([0.0000, 0.1111, 0.2222, 0.3333, 0.4444, 0.5556, 0.6667, 0.7778, 0.8889, 1.0000])) . np.random.randn(10) . array([ 0.68732684, -0.53367188, 0.27916096, 0.28236708, 0.03800702, -0.66236923, 1.32472364, -0.11671166, -0.77019834, -1.14755872]) . torch.randn(10) . tensor([ 0.8525, 0.2257, 0.3406, -0.4713, 1.5393, -2.0060, -0.4257, 3.0482, -0.7659, 0.3265]) . length $n$ vector, $n times 1$ col-vector, $1 times n$ row-vector . - 길이가 3인 벡터 선언방법 . a = torch.tensor([1,2,3]) a.shape . torch.Size([3]) . - 3x1 col-vec 선언방법 . (방법1) . a = torch.tensor([[1],[2],[3]]) a.shape . torch.Size([3, 1]) . (방법2) . a = torch.tensor([1,2,3]).reshape(3,1) a.shape . torch.Size([3, 1]) . - 1x3 row-vec 선언방법 . (방법1) . a = torch.tensor([[1,2,3]]) a.shape . torch.Size([1, 3]) . (방법2) . a = torch.tensor([1,2,3]).reshape(1,3) a.shape . torch.Size([1, 3]) . - 3x1 col-vec 선언방법, 1x3 row-vec 선언방법에서 [[1],[2],[3]] 혹은 [[1,2,3]] 와 같은 표현이 이해안되면 아래링크로 가셔서 . https://guebin.github.io/STBDA2022/2022/03/14/(2주차)-3월14일.html . 첫번째 동영상 12:15 - 22:45 에 해당하는 분량을 학습하시길 바랍니다. . torch&#51032; dtype . - 기본적으로 torch는 소수점으로 저장되면 dtype=torch.float32 가 된다. (이걸로 맞추는게 편리함) . tsr = torch.tensor([1.23,2.34]) tsr . tensor([1.2300, 2.3400]) . tsr.dtype . torch.float32 . - 정수로 선언하더라도 dtype를 torch.float32로 바꾸는게 유리함 . (안 좋은 선언예시) . tsr = torch.tensor([1,2]) tsr . tensor([1, 2]) . tsr.dtype . torch.int64 . (좋은 선언예시1) . tsr = torch.tensor([1,2],dtype=torch.float32) tsr . tensor([1., 2.]) . tsr.dtype . torch.float32 . (좋은 선언예시2) . tsr = torch.tensor([1,2.0]) tsr . tensor([1., 2.]) . tsr.dtype . torch.float32 . (사실 int로 선언해도 나중에 float으로 바꾸면 큰 문제없음) . tsr = torch.tensor([1,2]).float() tsr . tensor([1., 2.]) . tsr.dtype . torch.float32 . - 왜 정수만으로 torch.tensor를 만들때에도 torch.float32로 바꾸는게 유리할까? $ to$ torch.tensor끼리의 연산에서 문제가 될 수 있음 . 별 문제 없을수도 있지만 . torch.tensor([1,2])-torch.tensor([1.0,2.0]) . tensor([0., 0.]) . 아래와 같이 에러가 날수도 있다 . (에러1) . torch.tensor([[1.0,0.0],[0.0,1.0]]) @ torch.tensor([[1],[2]]) . RuntimeError Traceback (most recent call last) /tmp/ipykernel_1083902/281683190.py in &lt;module&gt; -&gt; 1 torch.tensor([[1.0,0.0],[0.0,1.0]]) @ torch.tensor([[1],[2]]) RuntimeError: expected scalar type Float but found Long . (에러2) . torch.tensor([[1,0],[0,1]]) @ torch.tensor([[1.0],[2.0]]) . RuntimeError Traceback (most recent call last) /tmp/ipykernel_1083902/837094206.py in &lt;module&gt; -&gt; 1 torch.tensor([[1,0],[0,1]]) @ torch.tensor([[1.0],[2.0]]) RuntimeError: expected scalar type Long but found Float . (해결1) 둘다 정수로 통일 . torch.tensor([[1,0],[0,1]]) @ torch.tensor([[1],[2]]) . tensor([[1], [2]]) . (해결2) 둘다 소수로 통일 &lt;-- 더 좋은 방법임 . torch.tensor([[1.0,0.0],[0.0,1.0]]) @ torch.tensor([[1.0],[2.0]]) . tensor([[1.], [2.]]) . shape of vector . - 행렬곱셈에 대한 shape 조심 . A = torch.tensor([[2.00,0.00],[0.00,3.00]]) b1 = torch.tensor([[-1.0,-5.0]]) b2 = torch.tensor([[-1.0],[-5.0]]) b3 = torch.tensor([-1.0,-5.0]) . A.shape,b1.shape,b2.shape,b3.shape . (torch.Size([2, 2]), torch.Size([1, 2]), torch.Size([2, 1]), torch.Size([2])) . - A@b1: 계산불가, b1@A: 계산가능 . A@b1 . RuntimeError Traceback (most recent call last) /tmp/ipykernel_1083902/615827064.py in &lt;module&gt; -&gt; 1 A@b1 RuntimeError: mat1 and mat2 shapes cannot be multiplied (2x2 and 1x2) . b1@A . tensor([[ -2., -15.]]) . - A@b2: 계산가능, b2@A: 계산불가 . A@b2 . tensor([[ -2.], [-15.]]) . b2@A . RuntimeError Traceback (most recent call last) /tmp/ipykernel_1083902/926932629.py in &lt;module&gt; -&gt; 1 b2@A RuntimeError: mat1 and mat2 shapes cannot be multiplied (2x1 and 2x2) . - A@b3: 계산가능, b3@A: 계산가능 . (A@b3).shape ## b3를 마치 col-vec 처럼 해석 . torch.Size([2]) . (b3@A).shape ## b3를 마지 row-vec 처럼 해석 . torch.Size([2]) . - 브로드캐스팅 . a = torch.tensor([1,2,3]) a - 1 . tensor([0, 1, 2]) . b = torch.tensor([[1],[2],[3]]) b - 1 . tensor([[0], [1], [2]]) . a - b # a를 row-vec 로 해석 . tensor([[ 0, 1, 2], [-1, 0, 1], [-2, -1, 0]]) . Review: step1~4 . df = pd.read_csv(&quot;https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-09-22-regression.csv&quot;) df . x y . 0 -2.482113 | -8.542024 | . 1 -2.362146 | -6.576713 | . 2 -1.997295 | -5.949576 | . 3 -1.623936 | -4.479364 | . 4 -1.479192 | -4.251570 | . ... ... | ... | . 95 2.244400 | 10.325987 | . 96 2.393501 | 12.266493 | . 97 2.605604 | 13.098280 | . 98 2.605658 | 12.546793 | . 99 2.663240 | 13.834002 | . 100 rows × 2 columns . plt.plot(df.x,df.y,&#39;o&#39;) . [&lt;matplotlib.lines.Line2D at 0x7efce8331450&gt;] . x = torch.tensor(df.x).float().reshape(100,1) y = torch.tensor(df.y).float().reshape(100,1) _1 = torch.ones([100,1]) X = torch.concat([_1,x],axis=1) . What = torch.tensor([[-5.0],[10.0]],requires_grad=True) What . tensor([[-5.], [10.]], requires_grad=True) . plt.plot(x,y,&#39;o&#39;) plt.plot(x,(X@What).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7efce8205e90&gt;] . ver1: loss = sum of squares error . for epoc in range(30): ## step1: yhat yhat = X@What ## step2: loss loss = torch.sum((y-yhat)**2) ## step3: 미분 loss.backward() ## step4: update What.data = What.data - 1/1000* What.grad What.grad = None . plt.plot(x,y,&#39;o&#39;) plt.plot(x,(X@What).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7efce810dd10&gt;] . ver1: loss = sum of squares error . What = torch.tensor([[-5.0],[10.0]],requires_grad=True) What . tensor([[-5.], [10.]], requires_grad=True) . plt.plot(x,y,&#39;o&#39;) plt.plot(x,(X@What).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7efce8082bd0&gt;] . for epoc in range(30): ## step1: yhat yhat = X@What ## step2: loss loss = torch.mean((y-yhat)**2) ## step3: 미분 loss.backward() ## step4: update What.data = What.data - 1/10* What.grad What.grad = None . plt.plot(x,y,&#39;o&#39;) plt.plot(x,(X@What).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7efce3f73510&gt;] . step1&#51032; &#45796;&#47480;&#48260;&#51204; -- net &#49444;&#44228;&#47564; . ver1: net = torch.nn.Linear(1,1,bias=True) . torch.manual_seed(43052) net = torch.nn.Linear(in_features=1,out_features=1,bias=True) . net.bias,net.weight . (Parameter containing: tensor([-0.8470], requires_grad=True), Parameter containing: tensor([[-0.3467]], requires_grad=True)) . plt.plot(x,y,&#39;o&#39;) plt.plot(x,net(x).data,&#39;--&#39;) w0hat=-0.8470 w1hat=-0.3467 plt.plot(x,w0hat+w1hat*x ,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7efce31ce790&gt;] . - net에서 $ hat{w}_0, hat{w}_1$ 의 값은? . net.weight # w1 . Parameter containing: tensor([[-0.3467]], requires_grad=True) . net.bias # w0 . Parameter containing: tensor([-0.8470], requires_grad=True) . _yhat = -0.8470 + -0.3467*x . plt.plot(x,y,&#39;o&#39;) plt.plot(x, _yhat,&#39;--&#39;) plt.plot(x,net(x).data,&#39;-.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7efce8b05b10&gt;] . - 수식표현: $ hat{y}_i = hat{w}_0 + hat{w}_1 x_i = hat{b} + hat{w}x_i = -0.8470 + -0.3467 x_i$ for all $i=1,2, dots,100$. . ver2: net = torch.nn.Linear(2,1,bias=False) . torch.manual_seed(43052) net = torch.nn.Linear(in_features=2,out_features=1,bias=False) . net.weight . Parameter containing: tensor([[-0.2451, -0.5989]], requires_grad=True) . net.bias . plt.plot(x,y,&#39;o&#39;) plt.plot(x,net(X).data,&#39;--&#39;) plt.plot(x,X@torch.tensor([[-0.2451], [-0.5989]]),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7efce2c79b90&gt;] . - 수식표현: $ hat{ bf y} = { bf X} { bf hat W} = begin{bmatrix} 1 &amp; x_1 1 &amp; x_2 dots &amp; dots 1 &amp; x_{100} end{bmatrix} begin{bmatrix} -0.2451 -0.5989 end{bmatrix}$ . &#51096;&#47803;&#46108;&#49324;&#50857;1 . _x = x.reshape(-1) . _x . tensor([-2.4821, -2.3621, -1.9973, -1.6239, -1.4792, -1.4635, -1.4509, -1.4435, -1.3722, -1.3079, -1.1904, -1.1092, -1.1054, -1.0875, -0.9469, -0.9319, -0.8643, -0.7858, -0.7549, -0.7421, -0.6948, -0.6103, -0.5830, -0.5621, -0.5506, -0.5058, -0.4806, -0.4738, -0.4710, -0.4676, -0.3874, -0.3719, -0.3688, -0.3159, -0.2775, -0.2772, -0.2734, -0.2721, -0.2668, -0.2155, -0.2000, -0.1816, -0.1708, -0.1565, -0.1448, -0.1361, -0.1057, -0.0603, -0.0559, -0.0214, 0.0655, 0.0684, 0.1195, 0.1420, 0.1521, 0.1568, 0.2646, 0.2656, 0.3157, 0.3220, 0.3461, 0.3984, 0.4190, 0.5443, 0.5579, 0.5913, 0.6148, 0.6469, 0.6469, 0.6523, 0.6674, 0.7059, 0.7141, 0.7822, 0.8154, 0.8668, 0.9291, 0.9804, 0.9853, 0.9941, 1.0376, 1.0393, 1.0697, 1.1024, 1.1126, 1.1532, 1.2289, 1.3403, 1.3494, 1.4279, 1.4994, 1.5031, 1.5437, 1.6789, 2.0832, 2.2444, 2.3935, 2.6056, 2.6057, 2.6632]) . torch.manual_seed(43052) net = torch.nn.Linear(in_features=1,out_features=1) . net(_x) . RuntimeError Traceback (most recent call last) /tmp/ipykernel_1083902/3314203086.py in &lt;module&gt; -&gt; 1 net(_x) ~/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs) 1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks 1129 or _global_forward_hooks or _global_forward_pre_hooks): -&gt; 1130 return forward_call(*input, **kwargs) 1131 # Do not call functions when jit is used 1132 full_backward_hooks, non_full_backward_hooks = [], [] ~/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/linear.py in forward(self, input) 112 113 def forward(self, input: Tensor) -&gt; Tensor: --&gt; 114 return F.linear(input, self.weight, self.bias) 115 116 def extra_repr(self) -&gt; str: RuntimeError: size mismatch, got 1, 1x1,100 . &#51096;&#47803;&#46108;&#49324;&#50857;2 . torch.manual_seed(43052) net = torch.nn.Linear(in_features=2,out_features=1) # bias=False를 깜빡.. . net.weight . Parameter containing: tensor([[-0.2451, -0.5989]], requires_grad=True) . net.bias . Parameter containing: tensor([0.2549], requires_grad=True) . plt.plot(x,y,&#39;o&#39;) plt.plot(x,net(X).data,&#39;--&#39;) plt.plot(x,X@torch.tensor([[-0.2451],[-0.5989]])+0.2549,&#39;-.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7efce2bfa310&gt;] . 수식표현: $ hat{ bf y} = { bf X} { bf hat W} + hat{b}= begin{bmatrix} 1 &amp; x_1 1 &amp; x_2 dots &amp; dots 1 &amp; x_{100} end{bmatrix} begin{bmatrix} -0.2451 -0.5989 end{bmatrix} + 0.2549$ | . step1&#51032; &#45796;&#47480;&#48260;&#51204; -- &#45149;&#44620;&#51648; . ver1: net = torch.nn.Linear(1,1,bias=True) . - 준비 . net = torch.nn.Linear(1,1) net.bias.data = torch.tensor([-5.0]) net.weight.data = torch.tensor([[10.00]]) net.bias,net.weight . (Parameter containing: tensor([-5.], requires_grad=True), Parameter containing: tensor([[10.]], requires_grad=True)) . plt.plot(x,y,&#39;o&#39;) plt.plot(x,net(x).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7efce249d210&gt;] . - step1 . yhat = net(x) . - step2 . loss = torch.mean((y-yhat)**2) . - step3 . (미분전) . net.bias,net.weight . (Parameter containing: tensor([-5.], requires_grad=True), Parameter containing: tensor([[10.]], requires_grad=True)) . net.bias.grad,net.weight.grad . (None, None) . (미분) . loss.backward() . (미분후) . net.bias,net.weight . (Parameter containing: tensor([-5.], requires_grad=True), Parameter containing: tensor([[10.]], requires_grad=True)) . net.bias.grad,net.weight.grad . (tensor([-13.4225]), tensor([[11.8893]])) . - step4 . (업데이트전) . net.bias,net.weight . (Parameter containing: tensor([-5.], requires_grad=True), Parameter containing: tensor([[10.]], requires_grad=True)) . net.bias.grad,net.weight.grad . (tensor([-13.4225]), tensor([[11.8893]])) . (업데이트) . net.bias.data = net.bias.data - 0.1*net.bias.grad net.weight.data = net.weight.data - 0.1*net.weight.grad . net.bias.grad = None net.weight.grad = None . (업데이트후) . net.bias,net.weight . (Parameter containing: tensor([-3.6577], requires_grad=True), Parameter containing: tensor([[8.8111]], requires_grad=True)) . net.bias.grad,net.weight.grad . (None, None) . - 반복하자. . net = torch.nn.Linear(1,1) net.bias.data = torch.tensor([-5.0]) net.weight.data = torch.tensor([[10.00]]) . plt.plot(x,y,&#39;o&#39;) plt.plot(x,net(x).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7efce263ced0&gt;] . for epoc in range(30): ## step1 yhat = net(x) ## step2 loss = torch.mean((y-yhat)**2) ## step3 loss.backward() ## step4 net.bias.data = net.bias.data - 0.1*net.bias.grad net.weight.data = net.weight.data - 0.1*net.weight.grad net.bias.grad = None net.weight.grad = None . plt.plot(x,y,&#39;o&#39;) plt.plot(x,net(x).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7efce1349050&gt;] . ver2: net = torch.nn.Linear(2,1,bias=False) . - 준비 . net = torch.nn.Linear(in_features=2,out_features=1,bias=False) net.weight.data = torch.tensor([[-5.0, 10.0]]) net.weight . Parameter containing: tensor([[-5., 10.]], requires_grad=True) . - step1 . yhat= net(X) . - step2 . loss = torch.mean((y-yhat)**2) . - step3 . (미분전) . net.weight . Parameter containing: tensor([[-5., 10.]], requires_grad=True) . net.weight.grad . (미분) . loss.backward() . (미분후) . net.weight . Parameter containing: tensor([[-5., 10.]], requires_grad=True) . net.weight.grad . tensor([[-13.4225, 11.8893]]) . - step4 . (업데이트전) . net.weight . Parameter containing: tensor([[-5., 10.]], requires_grad=True) . net.weight.grad . tensor([[-13.4225, 11.8893]]) . (업데이트) . net.weight.data = net.weight.data - 0.1 * net.weight.grad net.weight.grad = None . (업데이트후) . net.weight . Parameter containing: tensor([[-3.6577, 8.8111]], requires_grad=True) . net.weight.grad . - 반복하면 . net = torch.nn.Linear(in_features=2,out_features=1,bias=False) net.weight.data = torch.tensor([[-5.0, 10.0]]) . for epoc in range(30): ## step1 yhat = net(X) ## step2 loss = torch.mean((y-yhat)**2) ## step3 loss.backward() ## step4 net.weight.data = net.weight.data - 0.1 * net.weight.grad net.weight.grad = None . plt.plot(x,y,&#39;o&#39;) plt.plot(x,net(X).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7efce1438b10&gt;] . step4&#51032; &#45796;&#47480;&#48260;&#51204;: &#50741;&#54000;&#47560;&#51060;&#51200;! . ver1: net = torch.nn.Linear(1,1,bias=True) . - 준비단계 . net = torch.nn.Linear(1,1) net.bias.data = torch.tensor([-5.0]) net.weight.data = torch.tensor([[10.0]]) net.bias,net.weight . (Parameter containing: tensor([-5.], requires_grad=True), Parameter containing: tensor([[10.]], requires_grad=True)) . optimizr = torch.optim.SGD(net.parameters(),lr=0.1) . - step1~3 . yhat = net(x) loss = torch.mean((y-yhat)**2) loss.backward() . - step4 . (업데이트전) . net.bias,net.weight . (Parameter containing: tensor([-5.], requires_grad=True), Parameter containing: tensor([[10.]], requires_grad=True)) . net.bias.grad, net.weight.grad . (tensor([-13.4225]), tensor([[11.8893]])) . (업데이트) . optimizr.step() . optimizr.zero_grad() . (업데이트후) . net.bias,net.weight . (Parameter containing: tensor([-3.6577], requires_grad=True), Parameter containing: tensor([[8.8111]], requires_grad=True)) . net.bias.grad, net.weight.grad . (tensor([0.]), tensor([[0.]])) . - 반복하자. . net = torch.nn.Linear(1,1) net.bias.data = torch.tensor([-5.0]) net.weight.data = torch.tensor([[10.0]]) . optimizr = torch.optim.SGD(net.parameters(),lr=0.1) . plt.plot(x,y,&#39;o&#39;) plt.plot(x,net(x).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7efce1713f90&gt;] . for epoc in range(30): ## step1 yhat = net(x) ## step2 loss = torch.mean((y-yhat)**2) ## step3 loss.backward() ## step4 optimizr.step() optimizr.zero_grad() . plt.plot(x,y,&#39;o&#39;) plt.plot(x,net(x).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7efce1779690&gt;] . ver2: net = torch.nn.Linear(2,1,bias=False) . - 바로 반복하자.. . net = torch.nn.Linear(2,1,bias=False) net.weight.data = torch.tensor([[-5.0, 10.0]]) net.weight . Parameter containing: tensor([[-5., 10.]], requires_grad=True) . optimizr = torch.optim.SGD(net.parameters(),lr=0.1) . plt.plot(x,y,&#39;o&#39;) plt.plot(x,net(X).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7efce06c7650&gt;] . for epoc in range(30): ## step1 yhat = net(X) ## step2 loss = torch.mean((y-yhat)**2) ## step3 loss.backward() ## step4 optimizr.step() optimizr.zero_grad() . plt.plot(x,y,&#39;o&#39;) plt.plot(x,net(X).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7efce0718950&gt;] . . Appendix: net.parameters()&#51032; &#51032;&#48120;? (&#49440;&#53469;&#54617;&#49845;) . - iterator, generator의 개념필요 . https://guebin.github.io/IP2022/2022/06/06/(14주차)-6월6일.html, 클래스공부 8단계 참고 | . - 탐구시작: 네트워크 생성 . net = torch.nn.Linear(in_features=1,out_features=1) net.weight . Parameter containing: tensor([[-0.1656]], requires_grad=True) . net.bias . Parameter containing: tensor([0.8529], requires_grad=True) . - torch.optim.SGD? 를 확인하면 params에 대한설명에 아래와 같이 되어있음 . params (iterable): iterable of parameters to optimize or dicts defining parameter groups . - 설명을 읽어보면 params에 iterable object를 넣으라고 되어있음 (iterable object는 숨겨진 명령어로 __iter__를 가지고 있는 오브젝트를 의미) . set(dir(net.parameters)) &amp; {&#39;__iter__&#39;} . set() . set(dir(net.parameters())) &amp; {&#39;__iter__&#39;} . {&#39;__iter__&#39;} . - 무슨의미? . _generator = net.parameters() . _generator.__next__() . Parameter containing: tensor([[-0.1656]], requires_grad=True) . _generator.__next__() . Parameter containing: tensor([0.8529], requires_grad=True) . _generator.__next__() . StopIteration Traceback (most recent call last) /tmp/ipykernel_1083902/3395526306.py in &lt;module&gt; -&gt; 1 _generator.__next__() StopIteration: . - 이건 이런느낌인데? . _generator2 = iter([net.weight,net.bias]) . _generator2 . &lt;list_iterator at 0x7efce86d5dd0&gt; . _generator2.__next__() . Parameter containing: tensor([[-0.1656]], requires_grad=True) . _generator2.__next__() . Parameter containing: tensor([0.8529], requires_grad=True) . _generator2.__next__() . StopIteration Traceback (most recent call last) /tmp/ipykernel_1083902/2722541531.py in &lt;module&gt; -&gt; 1 _generator2.__next__() StopIteration: . - 즉 아래는 같은코드이다. . ### 코드1 _generator = net.parameters() torch.optim.SGD(_generator,lr=1/10) ### 코드2 _generator = iter([net.weight,net.bias]) torch.optim.SGD(_generator,lr=1/10) ### 코드3 (이렇게 써도 코드2가 실행된다고 이해할 수 있음) _iterator = [net.weight,net.bias] torch.optim.SGD(_iterator,lr=1/10) . 결론: net.parameters()는 net오브젝트에서 학습할 파라메터를 모두 모아 리스트(iterable object)로 만드는 함수라 이해할 수 있다. . - 응용예제1 . What = torch.tensor([[-5.0],[10.0]],requires_grad=True) optimizr = torch.optim.SGD([What],lr=1/10) . plt.plot(x,y,&#39;o&#39;) plt.plot(x,(X@What).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7efce8696410&gt;] . for epoc in range(30): yhat = X@What loss = torch.mean((y-yhat)**2) loss.backward() optimizr.step();optimizr.zero_grad() . plt.plot(x,y,&#39;o&#39;) plt.plot(x,(X@What).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7efce86124d0&gt;] . - 응용예제2 . b = torch.tensor(-5.0,requires_grad=True) w = torch.tensor(10.0,requires_grad=True) optimizr = torch.optim.SGD([b,w],lr=1/10) . plt.plot(x,y,&#39;o&#39;) plt.plot(x,(w*x+b).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7efce85ccad0&gt;] . for epoc in range(30): yhat = b+ w*x loss = torch.mean((y-yhat)**2) loss.backward() optimizr.step(); optimizr.zero_grad() . plt.plot(x,y,&#39;o&#39;) plt.plot(x,(w*x+b).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7efce8505590&gt;] . Logistic regression . motive . - 현실에서 이런 경우가 많음 . $x$가 커질수록 (혹은 작아질수록) 성공확률이 증가함. | . - (X,y)는 어떤모양? . _df = pd.DataFrame({&#39;x&#39;:range(-6,7),&#39;y&#39;:[0,0,0,0,0,0,1,0,1,1,1,1,1]}) _df . x y . 0 -6 | 0 | . 1 -5 | 0 | . 2 -4 | 0 | . 3 -3 | 0 | . 4 -2 | 0 | . 5 -1 | 0 | . 6 0 | 1 | . 7 1 | 0 | . 8 2 | 1 | . 9 3 | 1 | . 10 4 | 1 | . 11 5 | 1 | . 12 6 | 1 | . plt.plot(_df.x,_df.y,&#39;o&#39;) . [&lt;matplotlib.lines.Line2D at 0x7efce0fac7d0&gt;] . - (예비학습) 시그모이드라는 함수가 있음 . xx = torch.linspace(-6,6,100) def f(x): return torch.exp(x)/(1+torch.exp(x)) . plt.plot(_df.x,_df.y,&#39;o&#39;) plt.plot(xx,f(xx)) . [&lt;matplotlib.lines.Line2D at 0x7efce8403450&gt;] . model . - $x$가 커질수록 $y=1$이 잘나오는 모형은 아래와 같이 설계할 수 있음 &lt; 외우세요!!! . $y_i sim Ber( pi_i), quad $ where $ pi_i = frac{ exp(w_0+w_1x_i)}{1+ exp(w_0+w_1x_i)}$ . | $ hat{y}_i= hat{ pi}_i= frac{ exp( hat{w}_0+ hat{w}_1x_i)}{1+ exp( hat{w}_0+ hat{w}_1x_i)}= frac{1}{1+ exp(- hat{w}_0- hat{w}_1x_i)}$ . | $loss= - sum_{i=1}^{n} big(y_i log( hat{y}_i)+(1-y_i) log(1- hat{y}_i) big)$ &lt; 외우세요!! . | . toy example . x =torch.linspace(-1,1,2000).reshape(2000,1) w0 = -1 w1 = 5 u = w0 + w1*x v = torch.exp(u) / (torch.exp(u)+1) # v는 성공할확률 y=torch.bernoulli(v) . plt.plot(x,y,&#39;o&#39;,alpha=0.05,ms=4) plt.plot(x,v,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7efcce6bbc90&gt;] . - 최초의 곡선 . w0hat= -1 w1hat = 1 yhat = f(w0hat+x*w1hat) plt.plot(x,y,&#39;o&#39;,alpha=0.05,ms=4) plt.plot(x,v,&#39;--&#39;) plt.plot(x,yhat,&#39;--r&#39;) . [&lt;matplotlib.lines.Line2D at 0x7efcce471b10&gt;] . - step1: yhat . l1=torch.nn.Linear(1,1) . l1.bias.data=torch.tensor([-1.0]) l1.weight.data = torch.tensor([[1.0]]) . a1=torch.nn.Sigmoid() . w0hat= -1 w1hat = 1 yhat = a1(l1(x)) plt.plot(x,y,&#39;o&#39;,alpha=0.05,ms=4) plt.plot(x,v,&#39;--&#39;) plt.plot(x,yhat.data,&#39;--r&#39;) . [&lt;matplotlib.lines.Line2D at 0x7efce0b03290&gt;] . - step1~4 반복 . for epoc in range(6000): ## step1 yhat = a1(l1(x)) ## step2 loss = torch.mean((y-yhat)**2) ## loss 를 원래 이렇게 하는건 아니에요.. ## step3 loss.backward() ## step4 l1.bias.data = l1.bias.data - 0.1 * l1.bias.grad l1.weight.data = l1.weight.data - 0.1 * l1.weight.grad l1.bias.grad = None l1.weight.grad = None . plt.plot(x,y,&#39;o&#39;,alpha=0.05,ms=4) plt.plot(x,v,&#39;--&#39;) plt.plot(x,a1(l1(x)).data,&#39;--r&#39;) . [&lt;matplotlib.lines.Line2D at 0x7efce08e5610&gt;] .",
            "url": "https://guebin.github.io/STML2022/2022/09/28/(4%EC%A3%BC%EC%B0%A8)-9%EC%9B%9428%EC%9D%BC.html",
            "relUrl": "/2022/09/28/(4%EC%A3%BC%EC%B0%A8)-9%EC%9B%9428%EC%9D%BC.html",
            "date": " • Sep 28, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "(3주차) 9월21일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . imports . import torch import numpy as np import matplotlib.pyplot as plt . &#47196;&#46300;&#47605; . - 회귀분석 $ to$ 로지스틱 $ to$ 심층신경망(DNN) $ to$ 합성곱신경망(CNN) . - 강의계획서 . ref . - 넘파이 문법이 약하다면? (reshape, concatenate, stack) . (1) reshape: 아래 링크의 넘파이공부 2단계 reshape 참고 . https://guebin.github.io/IP2022/2022/04/06/(6%EC%A3%BC%EC%B0%A8)-4%EC%9B%946%EC%9D%BC.html . (2) concatenate, stack: 아래 링크의 넘파이공부 4단계 참고 . https://guebin.github.io/IP2022/2022/04/11/(6%EC%A3%BC%EC%B0%A8)-4%EC%9B%9411%EC%9D%BC.html . &#54924;&#44480;&#47784;&#54805; &#49548;&#44060; . - model: $y_i= w_0+w_1 x_i + epsilon_i = 2.5 + 4x_i + epsilon_i, quad i=1,2, dots,n$ . - model: ${ bf y}={ bf X}{ bf W} + boldsymbol{ epsilon}$ . ${ bf y}= begin{bmatrix} y_1 y_2 dots y_n end{bmatrix}, quad { bf X}= begin{bmatrix} 1 &amp; x_1 1 &amp; x_2 dots 1 &amp; x_n end{bmatrix}, quad { bf W}= begin{bmatrix} 2.5 4 end{bmatrix}, quad boldsymbol{ epsilon}= begin{bmatrix} epsilon_1 dots epsilon_n end{bmatrix}$ | . &#54924;&#44480;&#47784;&#54805;&#50640;&#49436; &#45936;&#51060;&#53552; &#49373;&#49457; . torch.manual_seed(43052) ones= torch.ones(100) x,_ = torch.randn(100).sort() X = torch.stack([ones,x]).T # torch.stack([ones,x],axis=1) W = torch.tensor([2.5,4]) ϵ = torch.randn(100)*0.5 y = X@W + ϵ . plt.plot(x,y,&#39;o&#39;) plt.plot(x,2.5+4*x,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fa7444cd5d0&gt;] . &#54924;&#44480;&#47784;&#54805;&#50640;&#49436; &#54617;&#49845;&#51060;&#46976;? . - 파란점만 주어졌을때, 주황색 점선을 추정하는것. 좀 더 정확하게 말하면 given data로 $ begin{bmatrix} hat{w}_0 hat{w}_1 end{bmatrix}$를 최대한 $ begin{bmatrix} 2.5 4 end{bmatrix}$와 비슷하게 찾는것. . given data : $ big {(x_i,y_i) big }_{i=1}^{n}$ . | parameter: ${ bf W}= begin{bmatrix} w_0 w_1 end{bmatrix}$ . | estimated parameter: ${ bf hat{W}}= begin{bmatrix} hat{w}_0 hat{w}_1 end{bmatrix}$ . | . - 더 쉽게 말하면 아래의 그림을 보고 적당한 추세선을 찾는것이다. . plt.plot(x,y,&#39;o&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fa7444e7350&gt;] . - 시도: $( hat{w}_0, hat{w}_1)=(-5,10)$을 선택하여 선을 그려보고 적당한지 판단. . plt.plot(x,y,&#39;o&#39;) plt.plot(x,-5 + 10*x, &#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fa7443a97d0&gt;] . $ hat{y}_i=-5 +10 x_i$ 와 같이 $y_i$의 값을 적합시키겠다는 의미 | . - 벡터표현으로 주황색점선을 계산 . What = torch.tensor([-5.0,10.0]) . tensor([-5., 10.]) . plt.plot(x,y,&#39;o&#39;) plt.plot(x,X@What, &#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fa744321ed0&gt;] . &#54028;&#46972;&#47700;&#53552;&#47484; &#54617;&#49845;&#54616;&#45716; &#48169;&#48277; (&#51201;&#45817;&#54620; &#49440;&#51004;&#47196; &#50629;&#45936;&#51060;&#53944; &#54616;&#45716; &#48169;&#48277;) . - 이론적으로 추론 &lt;- 회귀분석시간에 배운것 . - 컴퓨터의 반복계산을 이용하여 추론 (손실함수도입 + 경사하강법) &lt;- 우리가 오늘 파이토치로 실습해볼 내용. . - 전략: 아래와 같은 3단계 전략을 취한다. . stage1: 아무 점선이나 그어본다.. | stage2: stage1에서 그은 점선보다 더 좋은 점선으로 바꾼다. | stage3: stage1 - 2 를 반복한다. | . Stage1: &#52395;&#48264;&#51704; &#51216;&#49440; -- &#51076;&#51032;&#51032; &#49440;&#51012; &#51068;&#45800; &#44536;&#50612;&#48372;&#51088; . - $ hat{w}_0=-5, hat{w}_1 = 10$ 으로 설정하고 (왜? 그냥) 임의의 선을 그어보자. . What = torch.tensor([-5.0,10.0],requires_grad=True) What . tensor([-5., 10.], requires_grad=True) . 처음에는 ${ bf hat{W}}= begin{bmatrix} hat{w}_0 hat{w}_1 end{bmatrix}= begin{bmatrix} -5 10 end{bmatrix} $ 를 대입해서 주황색 점선을 적당히 그려보자는 의미 . | 끝에 requires_grad=True는 나중에 미분을 위한 것 . | . 그려보자! . plt.plot(x,y,&#39;o&#39;) plt.plot(x,X@What.detach(), &#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fa743230bd0&gt;] . Stage2: &#52395;&#48264;&#51704; &#49688;&#51221; -- &#52572;&#52488;&#51032; &#51216;&#49440;&#50640; &#45824;&#54620; &#39;&#51201;&#45817;&#54620; &#51221;&#46020;&#39;&#47484; &#54032;&#45800;&#54616;&#44256; &#45908; &#39;&#51201;&#45817;&#54620;&#39; &#51216;&#49440;&#51004;&#47196; &#50629;&#45936;&#51060;&#53944; &#54620;&#45796;. . - &#39;적당한 정도&#39;를 판단하기 위한 장치: loss function 도입! . $loss= sum_{i=1}^{n}(y_i- hat{y}_i)^2= sum_{i=1}^{n}(y_i-( hat{w}_0+ hat{w}_1x_i))^2$ . $=({ bf y}-{ bf hat{y}})^ top({ bf y}-{ bf hat{y}})=({ bf y}-{ bf X}{ bf hat{W}})^ top({ bf y}-{ bf X}{ bf hat{W}})$ . loss = torch.sum((y-X@What)**2) loss . tensor(8587.6875, grad_fn=&lt;SumBackward0&gt;) . - loss 함수의 특징 . $y_i approx hat{y}_i$ 일수록 loss값이 작다. | $y_i approx hat{y}_i$ 이 되도록 $( hat{w}_0, hat{w}_1)$을 잘 찍으면 loss값이 작다. | (중요) 주황색 점선이 &#39;적당할 수록&#39; loss값이 작다. | . - 우리의 목표: 이 loss(=8587.6875)을 더 줄이자. . 궁극적으로는 아예 모든 조합 $( hat{w}_0, hat{w}_1)$에 대하여 가장 작은 loss를 찾으면 좋겠다. (stage2에서 할일은 아님) | . - 문제의 치환: 생각해보니까 우리의 문제는 아래와 같이 수학적으로 단순화 되었다. . 적당해보이는 주황색 선을 찾자 $ to$ $loss(w_0,w_1)$를 최소로하는 $(w_0,w_1)$의 값을 찾자. | . - 수정된 목표: $loss(w_0,w_1)$를 최소로 하는 $(w_0,w_1)$을 구하라. . 단순한 수학문제가 되었다. 마치 $loss(w)=w^2-2w+3$ 을 최소화하는 $w$를 찾으라는 것과 같음. | 즉 &quot;적당한 선으로 업데이트 하라 = 파라메터를 학습 하라 = 손실함수를 최소화 하라&quot; | . - 우리의 무기: 경사하강법, 벡터미분 . . Stage2&#47484; &#50948;&#54620; &#44221;&#49324;&#54616;&#44053;&#48277; &#48373;&#49845; . 경사하강법 아이디어 (1차원) . (step 1) 임의의 점을 찍는다. . (step 2) 그 점에서 순간기울기를 구한다. (접선) &lt;-- 미분 . (step 3) 순간기울기(=미분계수)의 부호를 살펴보고 부호와 반대방향으로 움직인다. . (팁) 기울기의 절대값 크기와 비례하여 보폭(=움직이는 정도)을 조절한다. . 경사하강법 아이디어 (2차원) . (step 1) 임의의 점을 찍는다. . (step 2) 그 점에서 순간기울기를 구한다. (접평면) &lt;-- 편미분 . (step 3) 순간기울기(=미분계수)의 부호를 살펴보고 부호와 반대방향으로 각각 움직인다. . (팁) 기울기의 절대값 크기와 비례하여 보폭(=움직이는 정도)을 각각 조절한다. . loss를 줄이도록 ${ bf W}$를 개선하는 방법 . - $수정값 leftarrow 원래값 - 기울어진크기(=미분계수) times alpha $ . 여기에서 $ alpha$는 전체적인 보폭의 크기를 결정한다. 즉 $ alpha$값이 클수록 한번의 update에 움직이는 양이 크다. | . - ${ bf W} leftarrow { bf W} - alpha times frac{ partial}{ partial { bf W}}loss(w_0,w_1)$ . 마이너스의 의미: 기울기의 부호를 보고 반대방향으로 움직여라. . | $ frac{ partial}{ partial { bf W}}loss(w_0,w_1):$ 기울기의 절대값 크기와 비례하여 움직이는 정도를 조정하라. . | $ alpha$의 의미: 전체적인 보폭의 속도를 조절, $ alpha$가 크면 전체적으로 빠르게 움직인다. 다리의 길이로 비유할 수 있다. . | . . - 우리의 목표: loss=8587.6875 인데, 이걸 줄이는 것이 목표라고 했었음. 이것을 줄이는 방법이 경사하강법이다. . loss . tensor(8587.6875, grad_fn=&lt;SumBackward0&gt;) . - 경사하강법으로 loss를 줄이기 위해서는 $ frac{ partial}{ partial { bf W}}loss(w_0,w_1)$의 계산이 필요한데, 이를 위해서 벡터미분이 필요하다. (loss.backward()로 하면된다) . loss.backward() . loss.backward()의 의미: loss를 미분해라! 뭘로? requires_grad=True를 가진 텐서로!! . | loss=torch.sum((y-yhat)**2)= torch.sum((y-X@What)**2) # 이었고 What=torch.tensor([-5.0,10.0],requires_grad=True) # 이므로 결국 What으로 미분하라는 의미. # 미분한 식이 나오는 것이 아니고, # 그 식에 (-5.0, 10.0)을 대입한 계수값이 계산됨. . | . - 위에서 loss.backward()의 과정은 미분을 활용하여 $(-5,10)$에서의 순간기울기를 구했다는 의미임. . - (-5,10)에서 loss의 순간기울기 값은 What.grad로 확인가능하다. . What.grad . tensor([-1342.2522, 1188.9305]) . 이것이 의미하는건 $(-5,10)$에서의 $loss(w_0,w_1)$의 순간기울기가 $(-1342.2523, 1188.9307)$ 이라는 의미 | . - (확인1) loss.backward()가 미분을 잘 계산해 주는 것이 맞는가? 손계산으로 검증하여 보자. . $loss(w_0,w_1)=({ bf y}- hat{ bf y})^ top ({ bf y}- hat{ bf y})=({ bf y}-{ bf XW})^ top ({ bf y}-{ bf XW})$ . | $ frac{ partial}{ partial { bf W} }loss(w_0,w_1)=-2{ bf X}^ top { bf y}+2{ bf X}^ top { bf X W}$ . | . - 2 * X.T @ y + 2 * X.T @ X @ What . tensor([-1342.2522, 1188.9308], grad_fn=&lt;AddBackward0&gt;) . - (확인2) loss.backward()가 미분을 잘 계산해 주는 것이 맞는가? 편미분을 간단히 구현하여 검증하여 보자. . $ frac{ partial}{ partial { bf W} } loss(w_0,w_1)= begin{bmatrix} frac{ partial}{ partial w_0} frac{ partial}{ partial w_1} end{bmatrix}loss(w_0,w_1) = begin{bmatrix} frac{ partial}{ partial w_0}loss(w_0,w_1) frac{ partial}{ partial w_1}loss(w_0,w_1) end{bmatrix}$ . | $ frac{ partial}{ partial w_0}loss(w_0,w_1) approx frac{loss(w_0+h,w_1)-loss(w_0,w_1)}{h}$ . | $ frac{ partial}{ partial w_1}loss(w_0,w_1) approx frac{loss(w_0,w_1+h)-loss(w_0,w_1)}{h}$ . | . _lossfn = lambda w0,w1: torch.sum((y-w0-w1*x)**2) . h=0.001 (_lossfn(-5+h,10) - _lossfn(-5,10)) / h , (_lossfn(-5,10+h) - _lossfn(-5,10)) / h . (tensor(-1341.7968), tensor(1190.4297)) . 약간 오차가 있지만 얼추비슷 $ to$ 잘 계산했다는 소리임 | . - 수정전, 수정하는폭, 수정후의 값은 차례로 아래와 같다. . &#39;수정전: &#39; + str(What.data) . &#39;수정전: tensor([-5., 10.])&#39; . alpha=0.001 print(&#39;수정전: &#39; + str(What.data)) # What 에서 미분꼬리표를 떼고 싶다면? What.data or What.detach() print(&#39;수정하는폭: &#39; +str(-alpha * What.grad)) print(&#39;수정후: &#39; +str(What.data-alpha * What.grad)) print(&#39;*참값: (2.5,4)&#39; ) . 수정전: tensor([-5., 10.]) 수정하는폭: tensor([ 1.3423, -1.1889]) 수정후: tensor([-3.6577, 8.8111]) *참값: (2.5,4) . - Wbefore, Wafter 계산 . Wbefore = What.data Wafter = What.data- alpha * What.grad Wbefore, Wafter . (tensor([-5., 10.]), tensor([-3.6577, 8.8111])) . - Wbefore, Wafter의 시각화 . plt.plot(x,y,&#39;o&#39;) plt.plot(x,X@Wbefore,&#39;--&#39;) plt.plot(x,X@Wafter,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fa740053050&gt;] . Stage3: Learn (=estimate $ bf hat{W})$ . - 이 과정은 Stage1,2를 반복하면 된다. . What = torch.tensor([-5.0,10.0],requires_grad=True) What . tensor([-5., 10.], requires_grad=True) . alpha= 1/1000 . for epoc in range(30): yhat = X@What loss = torch.sum((y-yhat)**2) loss.backward() What.data = What.data - alpha * What.grad What.grad = None . What . tensor([2.4290, 4.0144], requires_grad=True) . 원래 철자는 epoch이 맞아요 | . - 반복결과는?! (최종적으로 구해지는 What의 값은?!) . 참고로 true | . What.data ## true인 (2.5,4)와 상당히 비슷함 . tensor([2.4290, 4.0144]) . - 반복결과를 시각화하면? . plt.plot(x,y,&#39;o&#39;) plt.plot(x,X@What.data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fa726f030d0&gt;] . &#54028;&#46972;&#47700;&#53552;&#51032; &#54617;&#49845;&#44284;&#51221; &#51020;&#48120; (&#54617;&#49845;&#44284;&#51221; &#47784;&#45768;&#53552;&#47553;) . &#54617;&#49845;&#44284;&#51221;&#51032; &#44592;&#47197; . - 기록을 해보자. . loss_history = [] # 기록하고 싶은것 1 yhat_history = [] # 기록하고 싶은것 2 What_history = [] # 기록하고 싶은것 3 . What= torch.tensor([-5.0,10.0],requires_grad=True) alpha=0.001 for epoc in range(30): yhat=X@What ; yhat_history.append(yhat.data.tolist()) loss=torch.sum((y-yhat)**2); loss_history.append(loss.item()) loss.backward() What.data = What.data-alpha * What.grad; What_history.append(What.data.tolist()) What.grad=None . - $ hat{y}$ 관찰 (epoch=3, epoch=10, epoch=15) . plt.plot(x,y,&#39;o&#39;) plt.plot(x,yhat_history[2],&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fa744c5bf10&gt;] . plt.plot(x,y,&#39;o&#39;) plt.plot(x,yhat_history[9],&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fa726839790&gt;] . plt.plot(x,y,&#39;o&#39;) plt.plot(x,yhat_history[14],&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fa743197a50&gt;] . - $ hat{ bf W}$ 관찰 . What_history . [[-3.657747745513916, 8.81106948852539], [-2.554811716079712, 7.861191749572754], [-1.649186372756958, 7.101552963256836], [-0.9060714244842529, 6.49347448348999], [-0.29667872190475464, 6.006272315979004], [0.2027742564678192, 5.615575313568115], [0.6119104623794556, 5.302003860473633], [0.9469034075737, 5.0501298904418945], [1.2210698127746582, 4.847658157348633], [1.4453644752502441, 4.684779644012451], [1.6287914514541626, 4.553659915924072], [1.7787461280822754, 4.448036193847656], [1.9012980461120605, 4.3628973960876465], [2.0014259815216064, 4.294229507446289], [2.0832109451293945, 4.238814353942871], [2.149996757507324, 4.194070339202881], [2.204521894454956, 4.157923698425293], [2.249027729034424, 4.128708839416504], [2.285348415374756, 4.105085849761963], [2.31498384475708, 4.0859761238098145], [2.339160442352295, 4.070511341094971], [2.3588807582855225, 4.057991027832031], [2.3749637603759766, 4.0478515625], [2.3880786895751953, 4.039637088775635], [2.3987717628479004, 4.032979965209961], [2.40748929977417, 4.027583599090576], [2.414595603942871, 4.023208141326904], [2.4203879833221436, 4.019659042358398], [2.4251089096069336, 4.016779899597168], [2.4289560317993164, 4.014443874359131]] . - loss 관찰 . plt.plot(loss_history) . [&lt;matplotlib.lines.Line2D at 0x7ffa78e2d350&gt;] . &#54617;&#49845;&#44284;&#51221;&#51012; animation&#51004;&#47196; &#49884;&#44033;&#54868; . from matplotlib import animation . plt.rcParams[&#39;figure.figsize&#39;] = (7.5,2.5) plt.rcParams[&quot;animation.html&quot;] = &quot;jshtml&quot; . - 왼쪽에는 $(x_i,y_i)$ and $(x_i, hat{y}_i)$ 을 그리고 오른쪽에는 $loss(w_0,w_1)$ 을 그릴것임 . fig = plt.figure() ax1 = fig.add_subplot(1, 2, 1) ax2 = fig.add_subplot(1, 2, 2, projection=&#39;3d&#39;) . - 왼쪽그림! . ax1.plot(x,y,&#39;o&#39;) line, = ax1.plot(x,yhat_history[0]) # 나중에 애니메이션 할때 필요해요.. . fig . - 오른쪽 그림1: $loss(w_0,w_1)$ . _w0 = np.arange(-6, 11, 0.5) ## 파란색곡면을 그리는 코드 (시작) _w1 = np.arange(-6, 11, 0.5) w1,w0 = np.meshgrid(_w1,_w0) lss=w0*0 for i in range(len(_w0)): for j in range(len(_w1)): lss[i,j]=torch.sum((y-_w0[i]-_w1[j]*x)**2) ax2.plot_surface(w0, w1, lss, rstride=1, cstride=1, color=&#39;b&#39;,alpha=0.35) ## 파란색곡면을 그리는 코드(끝) ax2.azim = 40 ## 3d plot의 view 조절 ax2.dist = 8 ## 3d plot의 view 조절 ax2.elev = 5 ## 3d plot의 view 조절 . fig . - 오른쪽 그림2: $(w_0,w_1)=(2.5,4)$ 와 $loss(2.5,4)$ 값 &lt;- loss 함수가 최소가 되는 값 (이거 진짜야? ㅋㅋ) . ax2.scatter(2.5,4,torch.sum((y-2.5-4*x)**2),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## 최소점을 표시하는 코드 (붉은색 별) . &lt;mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7fa726c37750&gt; . fig . - 오른쪽 그림3: $(w_0,w_1)=(-3.66, 8.81)$ 와 $loss(-3.66,8.81)$ 값 . What_history[0] . [-3.657747745513916, 8.81106948852539] . ax2.scatter(What_history[0][0],What_history[0][1],loss_history[0],color=&#39;grey&#39;) ## 업데이트되는 What을 표시하는 점 (파란색 동그라미) . &lt;mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7fa726633110&gt; . fig . - 애니메이션 . def animate(epoc): line.set_ydata(yhat_history[epoc]) ax2.scatter(What_history[epoc][0],What_history[epoc][1],loss_history[epoc],color=&#39;grey&#39;) return line ani = animation.FuncAnimation(fig, animate, frames=30) plt.close() ani . Once Loop Reflect - 함수로 만들자.. . def show_lrpr(data,history): x,y = data loss_history,yhat_history,What_history = history fig = plt.figure() ax1 = fig.add_subplot(1, 2, 1) ax2 = fig.add_subplot(1, 2, 2, projection=&#39;3d&#39;) ## ax1: 왼쪽그림 ax1.plot(x,y,&#39;o&#39;) line, = ax1.plot(x,yhat_history[0]) ## ax2: 오른쪽그림 _w0 = np.arange(-6, 11, 0.5) ## 파란색곡면을 그리는 코드 (시작) _w1 = np.arange(-6, 11, 0.5) w1,w0 = np.meshgrid(_w1,_w0) lss=w0*0 for i in range(len(_w0)): for j in range(len(_w1)): lss[i,j]=torch.sum((y-_w0[i]-_w1[j]*x)**2) ax2.plot_surface(w0, w1, lss, rstride=1, cstride=1, color=&#39;b&#39;,alpha=0.35) ## 파란색곡면을 그리는 코드(끝) ax2.scatter(2.5,4,torch.sum((y-2.5-4*x)**2),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## 최소점을 표시하는 코드 (붉은색 별) ax2.scatter(What_history[0][0],What_history[0][1],loss_history[0],color=&#39;b&#39;) ## 업데이트되는 What을 표시하는 점 (파란색 동그라미) ax2.azim = 40 ## 3d plot의 view 조절 ax2.dist = 8 ## 3d plot의 view 조절 ax2.elev = 5 ## 3d plot의 view 조절 def animate(epoc): line.set_ydata(yhat_history[epoc]) ax2.scatter(np.array(What_history)[epoc,0],np.array(What_history)[epoc,1],loss_history[epoc],color=&#39;grey&#39;) return line ani = animation.FuncAnimation(fig, animate, frames=30) plt.close() return ani . show_lrpr([x,y],[loss_history,yhat_history,What_history]) . Once Loop Reflect $ alpha$&#50640; &#45824;&#54616;&#50668; ($ alpha$&#45716; &#54617;&#49845;&#47456;) . (1) $ alpha=0.0001$: $ alpha$ &#44032; &#45320;&#47924; &#51089;&#45796;&#47732;? $ to$ &#48708;&#54952;&#50984;&#51201;&#51060;&#45796;. . loss_history = [] # 기록하고 싶은것 1 yhat_history = [] # 기록하고 싶은것 2 What_history = [] # 기록하고 싶은것 3 . What= torch.tensor([-5.0,10.0],requires_grad=True) alpha=0.0001 for epoc in range(30): yhat=X@What ; yhat_history.append(yhat.data.tolist()) loss=torch.sum((y-yhat)**2); loss_history.append(loss.item()) loss.backward() What.data = What.data-alpha * What.grad; What_history.append(What.data.tolist()) What.grad=None . show_lrpr([x,y],[loss_history,yhat_history,What_history]) . Once Loop Reflect (2) $ alpha=0.0083$: $ alpha$&#44032; &#45320;&#47924; &#53356;&#45796;&#47732;? $ to$ &#45796;&#47480;&#51032;&#48120;&#50640;&#49436; &#48708;&#54952;&#50984;&#51201;&#51060;&#45796; + &#50948;&#54744;&#54616;&#45796;.. . loss_history = [] # 기록하고 싶은것 1 yhat_history = [] # 기록하고 싶은것 2 What_history = [] # 기록하고 싶은것 3 . What= torch.tensor([-5.0,10.0],requires_grad=True) alpha=0.0083 for epoc in range(30): yhat=X@What ; yhat_history.append(yhat.data.tolist()) loss=torch.sum((y-yhat)**2); loss_history.append(loss.item()) loss.backward() What.data = What.data-alpha * What.grad; What_history.append(What.data.tolist()) What.grad=None . show_lrpr([x,y],[loss_history,yhat_history,What_history]) . Once Loop Reflect (3) $ alpha=0.0085$ . loss_history = [] # 기록하고 싶은것 1 yhat_history = [] # 기록하고 싶은것 2 What_history = [] # 기록하고 싶은것 3 . What= torch.tensor([-5.0,10.0],requires_grad=True) alpha=0.0085 for epoc in range(30): yhat=X@What ; yhat_history.append(yhat.data.tolist()) loss=torch.sum((y-yhat)**2); loss_history.append(loss.item()) loss.backward() What.data = What.data-alpha * What.grad.data; What_history.append(What.data.tolist()) What.grad=None . show_lrpr([x,y],[loss_history,yhat_history,What_history]) . Once Loop Reflect (4) $ alpha=0.01$ . loss_history = [] # 기록하고 싶은것 1 yhat_history = [] # 기록하고 싶은것 2 What_history = [] # 기록하고 싶은것 3 . What= torch.tensor([-5.0,10.0],requires_grad=True) alpha=0.01 for epoc in range(30): yhat=X@What ; yhat_history.append(yhat.data.tolist()) loss=torch.sum((y-yhat)**2); loss_history.append(loss.item()) loss.backward() What.data = What.data-alpha * What.grad; What_history.append(What.data.tolist()) What.grad=None . show_lrpr([x,y],[loss_history,yhat_history,What_history]) . Once Loop Reflect &#49689;&#51228; . - 학습률($ alpha$)를 조정하며 실습해보고 스크린샷 제출 .",
            "url": "https://guebin.github.io/STML2022/2022/09/21/(3%EC%A3%BC%EC%B0%A8)-9%EC%9B%9421%EC%9D%BC.html",
            "relUrl": "/2022/09/21/(3%EC%A3%BC%EC%B0%A8)-9%EC%9B%9421%EC%9D%BC.html",
            "date": " • Sep 21, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Assignment 1 (09.19) -- 풀이O",
            "content": "제출은 이름(학번).ipynb 파일과 이름(학번).html파일 2개를 제출할 것. | ipynb 혹은 html 파일을 이용한 제출이 익숙하지 않은 학생은 질문할 것. | . from fastai.vision.all import * from fastai.collab import * from fastai.text.all import * . 1. &#51060;&#48120;&#51648;&#51088;&#47308;&#48516;&#49437; . 아래를 이용하여 MNIST_SAMPLE 이미지 자료를 다운로드 받고 dls오브젝트를 만들어라. . path = untar_data(URLs.MNIST_SAMPLE) . dls = ImageDataLoaders.from_folder(path,suffle=False) . dls.show_batch() . (1) cnn_learner를 이용하여 lrnr 오브젝트를 생성하라. . arch 는 resnet34 로 설정할 것 | metrics 는 error_rate 로 설정할 것 | . (풀이) . lrnr = cnn_learner(dls, arch = resnet34, metrics=error_rate) . /home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/fastai/vision/learner.py:284: UserWarning: `cnn_learner` has been renamed to `vision_learner` -- please update your code warn(&#34;`cnn_learner` has been renamed to `vision_learner` -- please update your code&#34;) /home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter &#39;pretrained&#39; is deprecated since 0.13 and will be removed in 0.15, please use &#39;weights&#39; instead. f&#34;The parameter &#39;{pretrained_param}&#39; is deprecated since 0.13 and will be removed in 0.15, &#34; /home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for &#39;weights&#39; are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights. warnings.warn(msg) . (2) fine_tune 을 이용하여 lrnr 오브젝트를 학습하라. . (풀이) . lrnr.fine_tune(1) . epoch train_loss valid_loss error_rate time . 0 | 0.282870 | 0.150136 | 0.049068 | 00:05 | . epoch train_loss valid_loss error_rate time . 0 | 0.042991 | 0.017522 | 0.006379 | 00:05 | . (3) 아래를 이용하여 X,y를 만들어라. . X,y = dls.one_batch() . X,y의 shape을 조사하라. X에는 몇개의 이미지가 있는가? 이미지의 size는 얼마인가? . (풀이) . X,y = dls.one_batch() X.shape . torch.Size([64, 3, 28, 28]) . X에는 64개의 이미지가 있고 크기는 (28,28) 이다. . (4) 아래의 코드를 이용하여 X의 두번째 이미지가 어떠한 숫자를 의미하는지 확인하라. (그림보고 3인지 7인지 확인하여 답을 쓸 것) . show_image(X[0]) . 그리고 show_image가 정의된 파일의 경로를 확인하고 show_image가 python 내장함수 인지, torch에서 지원하는 함수인지 fastai에서 지원하는 함수인지 파악하라. . (풀이) . show_image(X[1]) # 두번째 이미지 . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . &lt;AxesSubplot:&gt; . show_image? . Signature: show_image( im, ax=None, figsize=None, title=None, ctx=None, cmap=None, norm=None, *, aspect=None, interpolation=None, alpha=None, vmin=None, vmax=None, origin=None, extent=None, interpolation_stage=None, filternorm=True, filterrad=4.0, resample=None, url=None, data=None, **kwargs, ) Docstring: Show a PIL or PyTorch image on `ax`. File: ~/anaconda3/envs/py37/lib/python3.7/site-packages/fastai/torch_core.py Type: function . fastai에서 지원하는 함수 | . (5) lrnr 오브젝트를 이용하여 AI가 X[0]을 어떤 값으로 판단하는지 확인하라. 올바르게 판단하였는가? 올바르게 판단했다면 몇 프로의 확신으로 판단하였는가? &lt;-- 문제가 의도한 것과 다르게 만들어졌어요 . (풀이) . show_image(X[0]) # 첫번째 이미지 . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . &lt;AxesSubplot:&gt; . lrnr.model(X[0].reshape(1,3,28,28)) . TensorBase([[ 3.4148, -5.0356]], device=&#39;cuda:0&#39;, grad_fn=&lt;AliasBackward0&gt;) . import numpy as np a=np.exp(3.4148) b=np.exp(-5.0356) print(&#39;3일확률&#39;,a/(a+b)) print(&#39;7일확률&#39;,b/(a+b)) . 3일확률 0.9997862308347155 7일확률 0.0002137691652844868 . 원래문제의도:lrnr.predict(X[0].to(&quot;cpu&quot;)) . 2. &#52628;&#52380;&#49884;&#49828;&#53596; . 아래를 이용하여 rcmd_anal.csv 를 다운로드 받고 dls오브젝트를 만들어라. . df = pd.read_csv(&#39;https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-09-08-rcmd_anal.csv&#39;) df . user item rating item_name . 0 1 | 15 | 1.084308 | 홍차5 | . 1 1 | 1 | 4.149209 | 커피1 | . 2 1 | 11 | 1.142659 | 홍차1 | . 3 1 | 5 | 4.033415 | 커피5 | . 4 1 | 4 | 4.078139 | 커피4 | . ... ... | ... | ... | ... | . 995 100 | 18 | 4.104276 | 홍차8 | . 996 100 | 17 | 4.164773 | 홍차7 | . 997 100 | 14 | 4.026915 | 홍차4 | . 998 100 | 4 | 0.838720 | 커피4 | . 999 100 | 7 | 1.094826 | 커피7 | . 1000 rows × 4 columns . (1) 73번 유저가 먹은 아이템 및 평점을 출력하는 코드를 작성하라. 이를 기반으로 73번 유저가 어떠한 취향인지 파악하라. . (풀이) . df.query(&#39;user == 73&#39;) . user item rating item_name . 720 73 | 20 | 3.733853 | 홍차10 | . 721 73 | 18 | 3.975004 | 홍차8 | . 722 73 | 9 | 1.119541 | 커피9 | . 723 73 | 13 | 3.840801 | 홍차3 | . 724 73 | 2 | 0.943742 | 커피2 | . 725 73 | 4 | 1.152405 | 커피4 | . 726 73 | 1 | 0.887292 | 커피1 | . 727 73 | 7 | 0.947641 | 커피7 | . 728 73 | 6 | 0.868370 | 커피6 | . 729 73 | 17 | 3.873590 | 홍차7 | . 홍차를 선호 | . (2) dls와 lrnr 오브젝트를 생성하고 lrnr 오브젝트를 학습하라. . (풀이) . dls = CollabDataLoaders.from_df(df) lrnr = collab_learner(dls,y_range=(0,5)) . lrnr.fit(50) . epoch train_loss valid_loss time . 0 | 2.337114 | 2.258755 | 00:00 | . 1 | 2.328897 | 2.254714 | 00:00 | . 2 | 2.320246 | 2.237874 | 00:00 | . 3 | 2.300545 | 2.191783 | 00:00 | . 4 | 2.265857 | 2.104007 | 00:00 | . 5 | 2.207397 | 1.966761 | 00:00 | . 6 | 2.123599 | 1.783263 | 00:00 | . 7 | 2.008980 | 1.562448 | 00:00 | . 8 | 1.865242 | 1.317642 | 00:00 | . 9 | 1.697832 | 1.068948 | 00:00 | . 10 | 1.515044 | 0.833239 | 00:00 | . 11 | 1.326496 | 0.625003 | 00:00 | . 12 | 1.139156 | 0.453686 | 00:00 | . 13 | 0.962462 | 0.320953 | 00:00 | . 14 | 0.802481 | 0.223124 | 00:00 | . 15 | 0.662327 | 0.155420 | 00:00 | . 16 | 0.542384 | 0.110662 | 00:00 | . 17 | 0.442099 | 0.082435 | 00:00 | . 18 | 0.359706 | 0.064858 | 00:00 | . 19 | 0.292656 | 0.054441 | 00:00 | . 20 | 0.238817 | 0.048325 | 00:00 | . 21 | 0.195901 | 0.045092 | 00:00 | . 22 | 0.161955 | 0.043386 | 00:00 | . 23 | 0.135049 | 0.042616 | 00:00 | . 24 | 0.113653 | 0.042549 | 00:00 | . 25 | 0.096877 | 0.042678 | 00:00 | . 26 | 0.083618 | 0.043010 | 00:00 | . 27 | 0.073081 | 0.043308 | 00:00 | . 28 | 0.064768 | 0.043905 | 00:00 | . 29 | 0.058133 | 0.044605 | 00:00 | . 30 | 0.053050 | 0.044990 | 00:00 | . 31 | 0.048904 | 0.045569 | 00:00 | . 32 | 0.045665 | 0.045833 | 00:00 | . 33 | 0.043033 | 0.045906 | 00:00 | . 34 | 0.040883 | 0.046624 | 00:00 | . 35 | 0.039263 | 0.046878 | 00:00 | . 36 | 0.037608 | 0.047040 | 00:00 | . 37 | 0.036450 | 0.047146 | 00:00 | . 38 | 0.035638 | 0.047335 | 00:00 | . 39 | 0.034883 | 0.047623 | 00:00 | . 40 | 0.034177 | 0.048048 | 00:00 | . 41 | 0.033486 | 0.047836 | 00:00 | . 42 | 0.033047 | 0.048263 | 00:00 | . 43 | 0.032634 | 0.048296 | 00:00 | . 44 | 0.032165 | 0.048577 | 00:00 | . 45 | 0.031884 | 0.048578 | 00:00 | . 46 | 0.031517 | 0.048725 | 00:00 | . 47 | 0.031158 | 0.048977 | 00:00 | . 48 | 0.030711 | 0.048955 | 00:00 | . 49 | 0.030465 | 0.049127 | 00:00 | . (3) 아래와 같은 데이터 프레임을 생성하고 df_new 에 저장하라. . import IPython _html=&#39;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt; n &lt;thead&gt; n &lt;tr style=&quot;text-align: right;&quot;&gt; n &lt;th&gt;&lt;/th&gt; n &lt;th&gt;user&lt;/th&gt; n &lt;th&gt;item&lt;/th&gt; n &lt;/tr&gt; n &lt;/thead&gt; n &lt;tbody&gt; n &lt;tr&gt; n &lt;th&gt;0&lt;/th&gt; n &lt;td&gt;73&lt;/td&gt; n &lt;td&gt;1&lt;/td&gt; n &lt;/tr&gt; n &lt;tr&gt; n &lt;th&gt;1&lt;/th&gt; n &lt;td&gt;73&lt;/td&gt; n &lt;td&gt;2&lt;/td&gt; n &lt;/tr&gt; n &lt;tr&gt; n &lt;th&gt;2&lt;/th&gt; n &lt;td&gt;73&lt;/td&gt; n &lt;td&gt;3&lt;/td&gt; n &lt;/tr&gt; n &lt;tr&gt; n &lt;th&gt;3&lt;/th&gt; n &lt;td&gt;73&lt;/td&gt; n &lt;td&gt;4&lt;/td&gt; n &lt;/tr&gt; n &lt;tr&gt; n &lt;th&gt;4&lt;/th&gt; n &lt;td&gt;73&lt;/td&gt; n &lt;td&gt;5&lt;/td&gt; n &lt;/tr&gt; n &lt;tr&gt; n &lt;th&gt;5&lt;/th&gt; n &lt;td&gt;73&lt;/td&gt; n &lt;td&gt;6&lt;/td&gt; n &lt;/tr&gt; n &lt;tr&gt; n &lt;th&gt;6&lt;/th&gt; n &lt;td&gt;73&lt;/td&gt; n &lt;td&gt;7&lt;/td&gt; n &lt;/tr&gt; n &lt;tr&gt; n &lt;th&gt;7&lt;/th&gt; n &lt;td&gt;73&lt;/td&gt; n &lt;td&gt;8&lt;/td&gt; n &lt;/tr&gt; n &lt;tr&gt; n &lt;th&gt;8&lt;/th&gt; n &lt;td&gt;73&lt;/td&gt; n &lt;td&gt;9&lt;/td&gt; n &lt;/tr&gt; n &lt;tr&gt; n &lt;th&gt;9&lt;/th&gt; n &lt;td&gt;73&lt;/td&gt; n &lt;td&gt;10&lt;/td&gt; n &lt;/tr&gt; n &lt;tr&gt; n &lt;th&gt;10&lt;/th&gt; n &lt;td&gt;73&lt;/td&gt; n &lt;td&gt;11&lt;/td&gt; n &lt;/tr&gt; n &lt;tr&gt; n &lt;th&gt;11&lt;/th&gt; n &lt;td&gt;73&lt;/td&gt; n &lt;td&gt;12&lt;/td&gt; n &lt;/tr&gt; n &lt;tr&gt; n &lt;th&gt;12&lt;/th&gt; n &lt;td&gt;73&lt;/td&gt; n &lt;td&gt;13&lt;/td&gt; n &lt;/tr&gt; n &lt;tr&gt; n &lt;th&gt;13&lt;/th&gt; n &lt;td&gt;73&lt;/td&gt; n &lt;td&gt;14&lt;/td&gt; n &lt;/tr&gt; n &lt;tr&gt; n &lt;th&gt;14&lt;/th&gt; n &lt;td&gt;73&lt;/td&gt; n &lt;td&gt;15&lt;/td&gt; n &lt;/tr&gt; n &lt;tr&gt; n &lt;th&gt;15&lt;/th&gt; n &lt;td&gt;73&lt;/td&gt; n &lt;td&gt;16&lt;/td&gt; n &lt;/tr&gt; n &lt;tr&gt; n &lt;th&gt;16&lt;/th&gt; n &lt;td&gt;73&lt;/td&gt; n &lt;td&gt;17&lt;/td&gt; n &lt;/tr&gt; n &lt;tr&gt; n &lt;th&gt;17&lt;/th&gt; n &lt;td&gt;73&lt;/td&gt; n &lt;td&gt;18&lt;/td&gt; n &lt;/tr&gt; n &lt;tr&gt; n &lt;th&gt;18&lt;/th&gt; n &lt;td&gt;73&lt;/td&gt; n &lt;td&gt;19&lt;/td&gt; n &lt;/tr&gt; n &lt;tr&gt; n &lt;th&gt;19&lt;/th&gt; n &lt;td&gt;73&lt;/td&gt; n &lt;td&gt;20&lt;/td&gt; n &lt;/tr&gt; n &lt;/tbody&gt; n&lt;/table&gt;&#39; IPython.display.HTML(_html) . . user item . 0 73 | 1 | . 1 73 | 2 | . 2 73 | 3 | . 3 73 | 4 | . 4 73 | 5 | . 5 73 | 6 | . 6 73 | 7 | . 7 73 | 8 | . 8 73 | 9 | . 9 73 | 10 | . 10 73 | 11 | . 11 73 | 12 | . 12 73 | 13 | . 13 73 | 14 | . 14 73 | 15 | . 15 73 | 16 | . 16 73 | 17 | . 17 73 | 18 | . 18 73 | 19 | . 19 73 | 20 | . (풀이) . df_new=pd.DataFrame({&#39;user&#39;:[73]*20,&#39;item&#39;:range(1,21)}) df_new . user item . 0 73 | 1 | . 1 73 | 2 | . 2 73 | 3 | . 3 73 | 4 | . 4 73 | 5 | . 5 73 | 6 | . 6 73 | 7 | . 7 73 | 8 | . 8 73 | 9 | . 9 73 | 10 | . 10 73 | 11 | . 11 73 | 12 | . 12 73 | 13 | . 13 73 | 14 | . 14 73 | 15 | . 15 73 | 16 | . 16 73 | 17 | . 17 73 | 18 | . 18 73 | 19 | . 19 73 | 20 | . (4) 아래의 코드를 이용하여 73번 유저의 취향을 파악하라. 73번 유저가 커피3, 커피5를 먹는다면 얼마정도의 평점을 줄 것이라 예측되는가? . _dl = dls.test_dl(df_new) lrnr.get_preds(dl=_dl) . (풀이) . _dl = dls.test_dl(df_new) lrnr.get_preds(dl=_dl) . (tensor([0.9698, 1.0314, 1.0191, 1.0177, 1.0122, 0.9323, 1.0513, 1.0184, 1.0316, 0.9842, 3.8255, 3.9591, 3.8640, 3.8937, 3.9437, 3.8947, 3.8272, 3.9503, 3.8117, 3.8603]), None) . 커피3: 1.0191, 커피5: 1.0122 | . 3. &#49884;&#53248;&#49828;&#51088;&#47308;&#48516;&#49437; . 아래를 이용하여 자료를 다운로드 받아라. . df = pd.read_csv(&#39;https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-09-19-human_numbers_100.csv&#39;) df . Unnamed: 0 text . 0 0 | one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve... | . 1 1 | one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve... | . 2 2 | one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve... | . 3 3 | one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve... | . 4 4 | one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve... | . ... ... | ... | . 1995 1995 | one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve... | . 1996 1996 | one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve... | . 1997 1997 | one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve... | . 1998 1998 | one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve... | . 1999 1999 | one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve... | . 2000 rows × 2 columns . (1) TextDataLoaders.from_df을 이용하여 dls오브젝트를 만들어라. . is_lm = True 로 설정할 것 | seq_len = 5 로 설정할 것 | . (풀이) . dls = TextDataLoaders.from_df(df,is_lm=True,seq_len=5,text_col=&#39;text&#39;) dls.show_batch() . text text_ . 0 xxbos one , two , | one , two , three | . 1 hundred xxbos one , two | xxbos one , two , | . 2 one hundred xxbos one , | hundred xxbos one , two | . 3 , one hundred xxbos one | one hundred xxbos one , | . 4 nine , one hundred xxbos | , one hundred xxbos one | . 5 ninety nine , one hundred | nine , one hundred xxbos | . 6 , ninety nine , one | ninety nine , one hundred | . 7 eight , ninety nine , | , ninety nine , one | . 8 ninety eight , ninety nine | eight , ninety nine , | . (2) lrnr 오브젝트를 만들어라. . arch = AWD_LSTM 이용 | metrics = accuracy 이용 | . (풀이) . lrnr = language_model_learner(dls, arch= AWD_LSTM, metrics=accuracy) . (3) lrnr오브젝트에서 fine_tune(3) 메소드를 이용하여 모형을 학습하라. . (풀이) . lrnr.fine_tune(3) . epoch train_loss valid_loss accuracy time . 0 | 0.534681 | 0.168856 | 0.977650 | 00:49 | . epoch train_loss valid_loss accuracy time . 0 | 0.018749 | 0.003256 | 0.999205 | 00:54 | . 1 | 0.001580 | 0.002430 | 0.999324 | 00:54 | . 2 | 0.000651 | 0.002244 | 0.999315 | 00:54 | . (4) &#39;one , two ,&#39; 이후에 이어질 50개의 단어를 생성하라. . (풀이) . lrnr.predict(&#39;one, two,&#39;, n_words=50) . &#39;one , two , three , four , five , six , seven , eight , nine , ten , eleven , twelve , thirteen , fourteen , fifteen , sixteen , seventeen , eighteen , nineteen , twenty , twenty one , twenty two , twenty three , twenty four , twenty five&#39; . (5) &#39;twenty , twenty one , &#39; 이후에 이어질 50개의 단어를 생성하라. . (풀이) . lrnr.predict(&#39;twenty, twenty one,&#39;, n_words=50) . &#39;twenty , twenty one , twenty two , twenty three , twenty four , twenty five , twenty six , twenty seven , twenty eight , twenty nine , thirty , thirty one , thirty two , thirty three , thirty four , thirty five , thirty six , thirty seven , thirty eight ,&#39; . 4. &#47532;&#45573;&#49828;&#47749;&#47161;&#50612; . Collab 에서 (혹은 리눅스기반 서버에서) 아래의 명령어를 순서대로 실행해보라. . !ls !ls -a !ls . !ls .. !ls sample !mkdir asdf !wget https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-09-08-rcmd_anal.csv !cp 2022-09-08-rcmd_anal.csv ./asdf !ls ./asdf !rm 2022-09-08-rcmd_anal.csv !rm -rf asdf . 각 명령들이 무엇을 의미하는지 간단히 서술하라. . (풀이) . !ls . 현재디렉토리 파일+폴더 출력 | !ls . 와 같음 | !ls ./ 와 같음 | . !ls -a . 현재디렉토리 파일+폴더 출력, 숨겨진 항목까지 출력 | . !ls . . 현재디렉토리 파일+폴더 출력 | !ls 와 같음 | !ls ./ 와 같음 | . !ls .. . 현재디렉토리보다 상위디렉토리의 파일+폴더 출력 | . !ls sample . 현재디렉토리에 sample 디렉토리 출력 | !ls ./sample 과 같음 | . !mkdir asdf . 현재디렉토리에 asdf 폴더 생성 | !mkdir ./asdf 와 같음 | . !wget https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-09-08-rcmd_anal.csv . url에 있는 파일 다운로드하여 현재디렉토리에 저장 | . !cp 2022-09-08-rcmd_anal.csv ./asdf . 2022-09-08-rcmd_anal.csv 파일을 ./asdf 로 복사 | . !ls ./asdf . 현재디렉토리에서 asdf 디렉토리의 내용출력 | !ls asdf 와 같음 | . !rm 2022-09-08-rcmd_anal.csv . 현재 디렉토리에서 2022-09-08-rcmd_anal.csv 파일삭제; | rm ./2022-09-08-rcmd_anal.csv 와 같음 | . !rm -rf asdf . 현재 디렉토리에서 asdf 삭제 (asdf 폴더내에 파일이 존재하면 파일도 같이 삭제) | r은 recursively, f는 force의 약자 | . Appendix: ipynb -&gt; html &#48320;&#54872; . .",
            "url": "https://guebin.github.io/STML2022/2022/09/19/Assignment-1-Copy1.html",
            "relUrl": "/2022/09/19/Assignment-1-Copy1.html",
            "date": " • Sep 19, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "(2주차) 9월14일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . imports . from fastai.vision.all import * ## 이미지분석 from fastai.collab import * ## 추천시스템 from fastai.text.all import * ## 텍스트분석 from fastai.vision.gan import * ## GAN (이미지생성) . import pandas as pd . &#51060;&#48120;&#51648; &#51088;&#47308;&#48516;&#49437; &#49892;&#49845; (&#51648;&#45212;&#49884;&#44036; &#48373;&#49845;) . 1&#45800;&#44228;: &#45936;&#51060;&#53552;&#51032; &#51221;&#47532; . path = untar_data(URLs.PETS)/&#39;images&#39; . . 100.00% [811712512/811706944 01:23&lt;00:00] fnames = get_image_files(path) . ?get_image_files . Signature: get_image_files(path, recurse=True, folders=None) Docstring: Get image files in `path` recursively, only in `folders`, if specified. File: ~/anaconda3/envs/py37/lib/python3.7/site-packages/fastai/data/transforms.py Type: function . f = lambda fname: &#39;cat&#39; if fname[0].isupper() else &#39;dog&#39; . dls=ImageDataLoaders.from_name_func( path, fnames, f, item_tfms = Resize(224)) . dls.show_batch() . 2&#45800;&#44228;: lrnr &#50724;&#48652;&#51229;&#53944; &#49373;&#49457; . lrnr = cnn_learner(dls,resnet34,metrics=error_rate) . /home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/fastai/vision/learner.py:284: UserWarning: `cnn_learner` has been renamed to `vision_learner` -- please update your code warn(&#34;`cnn_learner` has been renamed to `vision_learner` -- please update your code&#34;) /home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter &#39;pretrained&#39; is deprecated since 0.13 and will be removed in 0.15, please use &#39;weights&#39; instead. f&#34;The parameter &#39;{pretrained_param}&#39; is deprecated since 0.13 and will be removed in 0.15, &#34; /home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for &#39;weights&#39; are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights. warnings.warn(msg) . 3&#45800;&#44228;: lrnr.&#54617;&#49845;() . fine_tune()은 모든 가중치를 학습하는 것이 아니라 일부만 학습하는 것임. | fine_tune()이외이 방법으로 학습할 수도 있음. | . lrnr.fine_tune(1) . epoch train_loss valid_loss error_rate time . 0 | 0.153413 | 0.019028 | 0.004060 | 00:09 | . epoch train_loss valid_loss error_rate time . 0 | 0.055954 | 0.022035 | 0.008796 | 00:10 | . 4&#45800;&#44228;: lrnr.&#50696;&#52769;() . (방법1) lrnr.predict() 함수를 이용 . lrnr.predict(&#39;/home/cgb4/.fastai/data/oxford-iiit-pet/images/miniature_pinscher_81.jpg&#39;) . (&#39;dog&#39;, TensorBase(1), TensorBase([0.0014, 0.9986])) . (방법2) lrnr.model(X) 를 이용: X의 shape이 (?,3,224,224)의 형태의 텐서이어야함 . X,y = dls.one_batch() . lrnr.model(X[0:1]) . TensorBase([[ 10.0084, -10.8716]], device=&#39;cuda:0&#39;, grad_fn=&lt;AliasBackward0&gt;) . 0번 obs에 대한 결과 | . &#54532;&#47196;&#44536;&#47000;&#48141; &#44284;&#51221; . &#54532;&#47196;&#44536;&#47000;&#48141; &#44284;&#51221; overview . - overview . (1) dls 오브젝트 생성 . (2) lrnr 오브젝트 생성 . (3) lrnr.학습() . (4) lrnr.예측() . &#51060;&#48120;&#51648;&#48516;&#49437;, &#52628;&#52380;&#49884;&#49828;&#53596;, &#53581;&#49828;&#53944;&#48516;&#49437;, GAN &#48516;&#49437;&#44284;&#51221; &#48708;&#44368; . - 비교 . 이미지분석(CNN) 추천시스템 텍스트분석 GAN . 1단계 | ImageDataLoaders | CollabDataLoaders | TextDataLoaders | DataBlock -&gt; dls | . 2단계 | cnn_learner() | collab_learner() | language_model_learner() | GANLearner.wgan() | . 3단계 | lrnr.fine_tune(1) | lrnr.fit() | lrnr.fit() | lrnr.fit() | . 4단계 | lrnr.predict(), lrnr.model(X) | lrnr.model(X) | lrnr.predict() | | . &#52628;&#52380;&#49884;&#49828;&#53596; &#49892;&#49845; . 1&#45800;&#44228; . df_view = pd.read_csv(&#39;https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-09-08-rcmd_view.csv&#39;) df_view . 커피1 커피2 커피3 커피4 커피5 커피6 커피7 커피8 커피9 커피10 홍차1 홍차2 홍차3 홍차4 홍차5 홍차6 홍차7 홍차8 홍차9 홍차10 . 0 4.149209 | NaN | NaN | 4.078139 | 4.033415 | 4.071871 | NaN | NaN | NaN | NaN | 1.142659 | 1.109452 | NaN | 0.603118 | 1.084308 | NaN | 0.906524 | NaN | NaN | 0.903826 | . 1 4.031811 | NaN | NaN | 3.822704 | NaN | NaN | NaN | 4.071410 | 3.996206 | NaN | NaN | 0.839565 | 1.011315 | NaN | 1.120552 | 0.911340 | NaN | 0.860954 | 0.871482 | NaN | . 2 4.082178 | 4.196436 | NaN | 3.956876 | NaN | NaN | NaN | 4.450931 | 3.972090 | NaN | NaN | NaN | NaN | 0.983838 | NaN | 0.918576 | 1.206796 | 0.913116 | NaN | 0.956194 | . 3 NaN | 4.000621 | 3.895570 | NaN | 3.838781 | 3.967183 | NaN | NaN | NaN | 4.105741 | 1.147554 | NaN | 1.346860 | NaN | 0.614099 | 1.297301 | NaN | NaN | NaN | 1.147545 | . 4 NaN | NaN | NaN | NaN | 3.888208 | NaN | 3.970330 | 3.979490 | NaN | 4.010982 | NaN | 0.920995 | 1.081111 | 0.999345 | NaN | 1.195183 | NaN | 0.818332 | 1.236331 | NaN | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 95 0.511905 | 1.066144 | NaN | 1.315430 | NaN | 1.285778 | NaN | 0.678400 | 1.023020 | 0.886803 | NaN | 4.055996 | NaN | NaN | 4.156489 | 4.127622 | NaN | NaN | NaN | NaN | . 96 NaN | 1.035022 | NaN | 1.085834 | NaN | 0.812558 | NaN | 1.074543 | NaN | 0.852806 | 3.894772 | NaN | 4.071385 | 3.935935 | NaN | NaN | 3.989815 | NaN | NaN | 4.267142 | . 97 NaN | 1.115511 | NaN | 1.101395 | 0.878614 | NaN | NaN | NaN | 1.329319 | NaN | 4.125190 | NaN | 4.354638 | 3.811209 | 4.144648 | NaN | NaN | 4.116915 | 3.887823 | NaN | . 98 NaN | 0.850794 | NaN | NaN | 0.927884 | 0.669895 | NaN | NaN | 0.665429 | 1.387329 | NaN | NaN | 4.329404 | 4.111706 | 3.960197 | NaN | NaN | NaN | 3.725288 | 4.122072 | . 99 NaN | NaN | 1.413968 | 0.838720 | NaN | NaN | 1.094826 | 0.987888 | NaN | 1.177387 | 3.957383 | 4.136731 | NaN | 4.026915 | NaN | NaN | 4.164773 | 4.104276 | NaN | NaN | . 100 rows × 20 columns . 컴퓨터가 좋아하는 데이터 타입은 아님 | . - 컴퓨터가 좋아하는 자료 . df=pd.read_csv(&#39;https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-09-08-rcmd_anal.csv&#39;) df . user item rating item_name . 0 1 | 15 | 1.084308 | 홍차5 | . 1 1 | 1 | 4.149209 | 커피1 | . 2 1 | 11 | 1.142659 | 홍차1 | . 3 1 | 5 | 4.033415 | 커피5 | . 4 1 | 4 | 4.078139 | 커피4 | . ... ... | ... | ... | ... | . 995 100 | 18 | 4.104276 | 홍차8 | . 996 100 | 17 | 4.164773 | 홍차7 | . 997 100 | 14 | 4.026915 | 홍차4 | . 998 100 | 4 | 0.838720 | 커피4 | . 999 100 | 7 | 1.094826 | 커피7 | . 1000 rows × 4 columns . - 유저와 아이템의 인덱스 정리 . df.user.unique(), df.item.unique() . (array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100]), array([15, 1, 11, 5, 4, 14, 6, 20, 12, 17, 8, 9, 13, 19, 18, 16, 2, 3, 10, 7])) . 유저는 1-100 까지 번호가 매겨짐 | 아이템은 1-20 까지 번호가 매겨짐 | . dls = CollabDataLoaders.from_df(df) . dls.show_batch() . user item rating . 0 11 | 17 | 0.960800 | . 1 61 | 17 | 3.909415 | . 2 31 | 2 | 4.050969 | . 3 17 | 17 | 1.096392 | . 4 62 | 9 | 0.742943 | . 5 46 | 17 | 0.833476 | . 6 12 | 2 | 3.812937 | . 7 90 | 20 | 3.903833 | . 8 64 | 5 | 1.328967 | . 9 85 | 3 | 0.693932 | . X,y = dls.one_batch() . X[:10] . tensor([[64, 6], [86, 3], [84, 14], [42, 18], [49, 8], [15, 19], [53, 11], [13, 12], [74, 16], [25, 14]]) . y[:5] . tensor([[0.9451], [0.8993], [4.0129], [0.7864], [3.7636]]) . 2&#45800;&#44228; . lrnr = collab_learner(dls, y_range=(0,5)) . 3&#45800;&#44228; . lrnr.fit(10) . epoch train_loss valid_loss time . 0 | 0.035896 | 0.044345 | 00:00 | . 1 | 0.035299 | 0.044548 | 00:00 | . 2 | 0.035174 | 0.044936 | 00:00 | . 3 | 0.035359 | 0.045408 | 00:00 | . 4 | 0.035180 | 0.045514 | 00:00 | . 5 | 0.034807 | 0.046011 | 00:00 | . 6 | 0.034429 | 0.046395 | 00:00 | . 7 | 0.034163 | 0.046817 | 00:00 | . 8 | 0.033940 | 0.047022 | 00:00 | . 9 | 0.033884 | 0.047236 | 00:00 | . 4&#45800;&#44228; . - 이미 있는 데이터를 예측 . lrnr.model(X.to(&quot;cuda:0&quot;)) . tensor([0.9894, 0.9968, 4.1498, 0.9097, 4.0571, 1.0113, 3.8838, 0.8962, 4.0603, 1.1188, 3.9837, 3.9908, 4.0775, 4.1657, 4.0407, 1.0694, 3.8324, 1.0445, 1.0275, 0.9045, 3.8887, 4.0168, 3.8125, 3.9972, 1.0710, 0.9687, 0.9342, 3.9129, 0.8585, 4.1423, 0.9783, 1.0044, 4.1366, 0.8986, 1.2504, 1.0086, 3.9337, 1.1220, 0.9493, 1.0758, 0.9694, 4.0556, 0.9549, 1.0312, 4.0246, 1.1666, 4.0489, 4.1264, 1.0699, 3.7826, 4.2048, 3.9758, 4.1614, 1.0629, 1.0447, 1.0123, 0.8838, 1.0406, 3.9522, 1.0259, 4.0523, 1.0417, 3.9562, 3.9449], device=&#39;cuda:0&#39;, grad_fn=&lt;AddBackward0&gt;) . y.reshape(-1) . tensor([0.9451, 0.8993, 4.0129, 0.7864, 3.7636, 0.9507, 4.1551, 0.6014, 4.0648, 0.9704, 4.0714, 3.8687, 4.0553, 3.8141, 3.9967, 1.0482, 3.7097, 1.0794, 0.8947, 0.8144, 3.5164, 3.8604, 3.7402, 4.0649, 1.0261, 1.0894, 0.8515, 3.9947, 0.8541, 3.7918, 1.0113, 0.7114, 4.0840, 0.8953, 1.3570, 1.2247, 3.7839, 1.2259, 0.8335, 0.9166, 0.8183, 4.0661, 0.9404, 0.6141, 4.0646, 0.9350, 4.3938, 3.8240, 1.1155, 3.8708, 4.3127, 3.7778, 3.9387, 1.0811, 0.9899, 1.0573, 0.8246, 0.8015, 3.9672, 1.1892, 4.3997, 0.9129, 4.2277, 3.7051]) . - 첫번째 유저가 커피2를 먹었을때? -&gt; 예상: 4점근처..? . Xnew = tensor([[1, 2]]) # 첫번째 유저가 2번째 아이템을 먹었을때 . lrnr.model(Xnew.to(&quot;cuda:0&quot;)) # 첫번째 유저가 2번째 아이템을 먹었을때 . tensor([3.9528], device=&#39;cuda:0&#39;, grad_fn=&lt;AddBackward0&gt;) . &#53581;&#49828;&#53944;&#48516;&#49437; &#49892;&#49845; . 1&#45800;&#44228; . df = pd.DataFrame({&#39;text&#39;:[&#39;h e l l o . h e l l o ! h e l l o ? h e l l o !! h e l l o ??&#39;]*20000}) df . text . 0 h e l l o . h e l l o ! h e l l o ? h e l l o !! h e l l o ?? | . 1 h e l l o . h e l l o ! h e l l o ? h e l l o !! h e l l o ?? | . 2 h e l l o . h e l l o ! h e l l o ? h e l l o !! h e l l o ?? | . 3 h e l l o . h e l l o ! h e l l o ? h e l l o !! h e l l o ?? | . 4 h e l l o . h e l l o ! h e l l o ? h e l l o !! h e l l o ?? | . ... ... | . 19995 h e l l o . h e l l o ! h e l l o ? h e l l o !! h e l l o ?? | . 19996 h e l l o . h e l l o ! h e l l o ? h e l l o !! h e l l o ?? | . 19997 h e l l o . h e l l o ! h e l l o ? h e l l o !! h e l l o ?? | . 19998 h e l l o . h e l l o ! h e l l o ? h e l l o !! h e l l o ?? | . 19999 h e l l o . h e l l o ! h e l l o ? h e l l o !! h e l l o ?? | . 20000 rows × 1 columns . dls = TextDataLoaders.from_df(df,text_col=&#39;text&#39;,is_lm=True) . dls.show_batch() . text text_ . 0 xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o | h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . | . 1 ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l | xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o | . 2 ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l | ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l | . 3 o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e | ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l | . 4 l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h | o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e | . 5 l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos | l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h | . 6 e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? | l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos | . 7 h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? | e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? | . 8 ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o | h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? | . xxbos는 하나의 내용이 끝나고 다른 내용이 시작된다는 의미 | . 2&#45800;&#44228; . lrnr = language_model_learner(dls, AWD_LSTM) . 3&#45800;&#44228; . lrnr.fit(1) . epoch train_loss valid_loss time . 0 | 0.609508 | 0.249602 | 00:11 | . 4&#45800;&#44228; . lrnr.predict(&#39;h e&#39;,n_words=30) . &#39;h e l l o . h e l l l o ? h e l l l o ? ? h e l l o ! h e l l&#39; . GAN intro . - 저자: 이안굿펠로우 . 천재임 | 지도교수가 요수아 벤지오 | . - 논문 NIPS, 저는 이 논문 읽고 소름돋았어요.. . https://arxiv.org/abs/1406.2661 (현재시점, 38751회 인용되었음 $ to$ 48978회 인용..) | . - 최근 10년간 머신러닝 분야에서 가장 혁신적인 아이디어이다. (얀르쿤, 2014년 시점..) . - 무슨내용? 생성모형 . &#49373;&#49457;&#47784;&#54805;&#51060;&#46976;? (&#49772;&#50868; &#49444;&#47749;) . 만들수 없다면 이해하지 못한 것이다, 리처드 파인만 (천재 물리학자) . - 사진속에 들어있는 동물이 개인지 고양이인지 맞출수 있는 기계와 개와 고양이를 그릴수 있는 기계중 어떤것이 더 시각적보에 대한 이해가 깊다고 볼수 있는가? . - 진정으로 인공지능이 이미지를 이해했다면, 이미지를 만들수도 있어야 한다. $ to$ 이미지를 생성하는 모형을 만들어보자 $ to$ 성공 . . GAN&#51032; &#51025;&#50857;&#48516;&#50556; . - 내가 찍은 사진이 피카소의 화풍으로 표현된다면? . - 퀸의 라이브에이드가 4k로 나온다면? . - 1920년대 서울의 모습이 칼라로 복원된다면? . - 딥페이크: 유명인의 가짜 포르노, 가짜뉴스, 협박(거짓기소) . - 게임영상 (파이널판타지) . - 거북이의 커버.. . - 너무 많아요..... . &#49373;&#49457;&#47784;&#54805;&#51060;&#46976;? &#53685;&#44228;&#54617;&#44284; &#48260;&#51204;&#51032; &#49444;&#47749; . 제한된 정보만으로 어떤 문제를 풀 때, 그 과정에서 원래의 문제보다 일반적인 문제를 풀지 말고, 가능한 원래의 문제를 직접 풀어야한다. 배프닉 (SVM 창시자) . - 이미지 $ boldsymbol{x}$가 주어졌을 경우 라벨을 $y$라고 하자. . - 이미지를 보고 라벨을 맞추는 일은 $p(y| boldsymbol{x})$에 관심이 있다. . - 이미지를 생성하는 일은 $p( boldsymbol{x},y)$에 관심이 있는것이다. . - 데이터의 생성확률 $p( boldsymbol{x},y)$을 알면 클래스의 사후확률 $p(y| boldsymbol{x})$를 알 수 있음. (아래의 수식 참고) 하지만 역은 불가능 . $$p(y|x) = frac{p(x,y)}{p(x)} = frac{p(x,y)}{ sum_{y}p(x,y)} $$ . 즉 이미지를 생성하는일은 분류문제보다 더 어려운 일이라 해석가능 | . - 따라서 배프닉의 원리에 의하면 식별적 분류가 생성적 분류보다 바람직한 접근법이라 할 수 있음. . - 하지만 다양한 현실문제에서 생성모형이 유용할때가 많다. . GAN&#51032; &#50896;&#47532; . - GAN은 생성모형중 하나임 . - GAN의 원리는 경찰과 위조지폐범이 서로 선의의(?) 경쟁을 통하여 서로 발전하는 모형으로 설명할 수 있다. . The generative model can be thought of as analogous to a team of counterfeiters, trying to produce fake currency and use it without detection, while the discriminative model is analogous to the police, trying to detect the counterfeit currency. Competition in this game drives both teams to improve their methods until the counterfeits are indistiguishable from the genuine articles. . - 서로 적대적인(adversarial) 네트워크(network)를 동시에 학습시켜 가짜이미지를 만든다(generate) . - 무식한 상황극.. . 위조범: 가짜돈을 만들어서 부자가 되어야지! (가짜돈을 그림) | 경찰: (위조범이 만든 돈을 보고) 이건 가짜다! | 위조범: 걸렸군.. 더 정교하게 만들어야지.. | 경찰: 이건 진짠가?... --&gt; 상사에게 혼남. 그것도 구분못하냐고 | 위조범: 더 정교하게 만들자.. | 경찰: 더 판별능력을 업그레이드 하자! | 반복.. | . - 굉장히 우수한 경찰조차도 진짜와 가짜를 구분하지 못할때(=진짜 이미지를 0.5의 확률로만 진짜라고 말할때 = 가짜 이미지를 0.5의 확률로만 가짜라고 말할때) 학습을 멈춘다. . GAN &#49892;&#49845; . 1&#45800;&#44228; . path = untar_data(URLs.MNIST_SAMPLE) . dblock = DataBlock(blocks=(TransformBlock,ImageBlock), get_x = generate_noise, get_items=get_image_files, item_tfms=Resize(32)) dls = dblock.dataloaders(path) . dls.show_batch() . 2&#45800;&#44228; . counterfeiter = basic_generator(32,n_channels=3,n_extra_layers=1) police = basic_critic(32,n_channels=3,n_extra_layers=1) . lrnr = GANLearner.wgan(dls,counterfeiter,police) . 3&#45800;&#44228; . - lrnr.fit(10) 진행 . lrnr.fit(10) . /home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/fastai/callback/core.py:69: UserWarning: You are shadowing an attribute (generator) that exists in the learner. Use `self.learn.generator` to avoid this warn(f&#34;You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this&#34;) /home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/fastai/callback/core.py:69: UserWarning: You are shadowing an attribute (critic) that exists in the learner. Use `self.learn.critic` to avoid this warn(f&#34;You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this&#34;) /home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/fastai/callback/core.py:69: UserWarning: You are shadowing an attribute (gen_mode) that exists in the learner. Use `self.learn.gen_mode` to avoid this warn(f&#34;You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this&#34;) . epoch train_loss valid_loss gen_loss crit_loss time . 0 | -0.532201 | 0.440801 | 0.440801 | -0.726502 | 00:02 | . 1 | -0.572863 | 0.292201 | 0.292201 | -0.762671 | 00:02 | . 2 | -0.580544 | 0.365955 | 0.365955 | -0.764500 | 00:02 | . 3 | -0.578211 | 0.283310 | 0.283310 | -0.760000 | 00:02 | . 4 | -0.574836 | 0.292477 | 0.292477 | -0.760881 | 00:02 | . 5 | -0.559277 | 0.333989 | 0.333989 | -0.720299 | 00:02 | . 6 | -0.477380 | 0.213629 | 0.213629 | -0.643177 | 00:02 | . 7 | -0.531273 | 0.235818 | 0.235818 | -0.432077 | 00:02 | . 8 | -0.551595 | 0.282958 | 0.282958 | -0.723118 | 00:02 | . 9 | -0.494749 | 0.308528 | 0.308528 | -0.711895 | 00:03 | . lrnr.show_results() . - lrnr.fit(10) 추가로 진행 // 총20회 . lrnr.fit(10) . epoch train_loss valid_loss gen_loss crit_loss time . 0 | -0.419668 | 0.087052 | 0.087052 | -0.237243 | 00:02 | . 1 | -0.543500 | 0.329471 | 0.329471 | -0.734705 | 00:02 | . 2 | -0.529323 | 0.215306 | 0.215306 | -0.719122 | 00:02 | . 3 | -0.533282 | 0.299712 | 0.299712 | -0.719879 | 00:02 | . 4 | -0.492712 | 0.277884 | 0.277884 | -0.696638 | 00:02 | . 5 | -0.524883 | 0.277088 | 0.277088 | -0.720916 | 00:02 | . 6 | -0.543268 | 0.260537 | 0.260537 | -0.726659 | 00:02 | . 7 | -0.436168 | 0.298683 | 0.298683 | -0.533639 | 00:02 | . 8 | -0.398121 | 0.386021 | 0.386021 | -0.575905 | 00:02 | . 9 | -0.473424 | 0.259428 | 0.259428 | -0.680931 | 00:02 | . lrnr.show_results() . - lrnr.fit(10) 추가로 진행 // 총30회 . lrnr.fit(10) . epoch train_loss valid_loss gen_loss crit_loss time . 0 | -0.460563 | 0.232739 | 0.232739 | -0.672230 | 00:02 | . 1 | -0.444508 | 0.213786 | 0.213786 | -0.622139 | 00:02 | . 2 | -0.388439 | 0.238852 | 0.238852 | -0.307610 | 00:02 | . 3 | -0.426316 | 0.354313 | 0.354313 | -0.563799 | 00:02 | . 4 | -0.414099 | 0.322855 | 0.322855 | -0.595307 | 00:02 | . 5 | -0.388728 | 0.184913 | 0.184913 | -0.584780 | 00:02 | . 6 | -0.370009 | 0.174675 | 0.174675 | -0.578049 | 00:02 | . 7 | -0.351684 | 0.153075 | 0.153075 | -0.500779 | 00:02 | . 8 | -0.272257 | 0.293494 | 0.293494 | -0.026133 | 00:02 | . 9 | -0.147323 | 0.130281 | 0.130281 | -0.062606 | 00:02 | . lrnr.show_results() . - lrnr.fit(30) 추가로 진행 // 총60회 . lrnr.fit(30) . epoch train_loss valid_loss gen_loss crit_loss time . 0 | -0.266713 | 0.326697 | 0.326697 | -0.360351 | 00:02 | . 1 | -0.204794 | -0.024827 | -0.024827 | -0.078930 | 00:02 | . 2 | -0.182044 | 0.218660 | 0.218660 | -0.217780 | 00:02 | . 3 | -0.159330 | -0.333503 | -0.333503 | -0.023529 | 00:02 | . 4 | -0.175804 | 0.490546 | 0.490546 | -0.316850 | 00:02 | . 5 | -0.060805 | 0.017926 | 0.017926 | -0.039713 | 00:02 | . 6 | -0.157659 | 0.368752 | 0.368752 | -0.572124 | 00:02 | . 7 | -0.014943 | 0.071575 | 0.071575 | -0.003522 | 00:02 | . 8 | -0.066916 | 0.074913 | 0.074913 | -0.086654 | 00:02 | . 9 | -0.044177 | 0.315838 | 0.315838 | -0.045339 | 00:02 | . 10 | -0.033223 | 0.011908 | 0.011908 | -0.075365 | 00:02 | . 11 | -0.046094 | 0.430091 | 0.430091 | -0.120811 | 00:02 | . 12 | -0.077078 | 0.026633 | 0.026633 | -0.145710 | 00:02 | . 13 | -0.057786 | 0.120790 | 0.120790 | -0.020656 | 00:02 | . 14 | -0.078144 | -0.212062 | -0.212062 | -0.078650 | 00:02 | . 15 | -0.071753 | 0.190012 | 0.190012 | -0.377307 | 00:02 | . 16 | -0.046076 | -0.214998 | -0.214998 | 0.011535 | 00:02 | . 17 | -0.029732 | -0.085049 | -0.085049 | -0.021675 | 00:02 | . 18 | -0.038285 | 0.106664 | 0.106664 | -0.088883 | 00:02 | . 19 | -0.016875 | -0.099856 | -0.099856 | -0.034580 | 00:02 | . 20 | -0.008266 | 0.152775 | 0.152775 | -0.025161 | 00:02 | . 21 | -0.024998 | 0.002830 | 0.002830 | -0.048660 | 00:02 | . 22 | -0.039477 | -0.267699 | -0.267699 | 0.003434 | 00:02 | . 23 | -0.002846 | 0.028195 | 0.028195 | -0.033808 | 00:02 | . 24 | -0.017525 | -0.034921 | -0.034921 | -0.025195 | 00:02 | . 25 | -0.022841 | 0.010736 | 0.010736 | -0.006746 | 00:02 | . 26 | -0.040269 | 0.132911 | 0.132911 | -0.139113 | 00:02 | . 27 | -0.014493 | -0.010516 | -0.010516 | -0.037637 | 00:02 | . 28 | -0.052681 | 0.244386 | 0.244386 | -0.034104 | 00:02 | . 29 | -0.059834 | -0.129023 | -0.129023 | -0.052899 | 00:02 | . lrnr.show_results() . 4&#45800;&#44228; (&#50630;&#51020;) .",
            "url": "https://guebin.github.io/STML2022/2022/09/14/(2%EC%A3%BC%EC%B0%A8)-9%EC%9B%9414%EC%9D%BC.html",
            "relUrl": "/2022/09/14/(2%EC%A3%BC%EC%B0%A8)-9%EC%9B%9414%EC%9D%BC.html",
            "date": " • Sep 14, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "(1주차) 9월6일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . Import . from fastai.vision.all import * . &#45936;&#51060;&#53552;&#51200;&#51109; . path = untar_data(URLs.PETS)/&#39;images&#39; # URLs.PETS: 스트링 -&gt; 주소가 저장되어 있음.. -&gt; 주소로 들어가보니 어떠한 압축파일이 자동으로 다운 받아짐, 이게 데이터 # untar_data: (1) URLs.PETS에 저장된 주소로 찾아가서 (2) 압축을 풀어서 (3) 어떠한 폴더에 저장, 그 폴더의 위치는 path 에 저장 . . 100.00% [811712512/811706944 00:10&lt;00:00] path # 여기에 그림이 있다는 말이지?? . Path(&#39;/root/.fastai/data/oxford-iiit-pet/images&#39;) . # 탐색... 여러파일들이 있기는함.. # Abyssinian_1.jpg 를 보고싶다면? PILImage.create(&#39;/root/.fastai/data/oxford-iiit-pet/images/Abyssinian_1.jpg&#39;) . # Abyssinian_100.jpg 를 보고싶다면? PILImage.create(&#39;/root/.fastai/data/oxford-iiit-pet/images/Abyssinian_100.jpg&#39;) . - 그림을 확인 할 수 있는건 좋은데 이렇게 확인하니까 조금 귀찮음.. . _lst = [&#39;/root/.fastai/data/oxford-iiit-pet/images/Abyssinian_1.jpg&#39;,&#39;/root/.fastai/data/oxford-iiit-pet/images/Abyssinian_10.jpg&#39;] . _lst[0] . &#39;/root/.fastai/data/oxford-iiit-pet/images/Abyssinian_1.jpg&#39; . PILImage.create(_lst[0]) . files= get_image_files(path) files . (#7390) [Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/scottish_terrier_92.jpg&#39;),Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/leonberger_173.jpg&#39;),Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/shiba_inu_120.jpg&#39;),Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/Persian_26.jpg&#39;),Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/yorkshire_terrier_86.jpg&#39;),Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/Ragdoll_56.jpg&#39;),Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/german_shorthaired_2.jpg&#39;),Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/Egyptian_Mau_34.jpg&#39;),Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/japanese_chin_169.jpg&#39;),Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/Abyssinian_192.jpg&#39;)...] . files[0] . Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/scottish_terrier_92.jpg&#39;) . PILImage.create(files[0]) . print(files[2]) PILImage.create(files[2]) . /root/.fastai/data/oxford-iiit-pet/images/shiba_inu_120.jpg . print(files[3]) PILImage.create(files[3]) . /root/.fastai/data/oxford-iiit-pet/images/Persian_26.jpg . print(files[4]) PILImage.create(files[4]) . /root/.fastai/data/oxford-iiit-pet/images/yorkshire_terrier_86.jpg . print(files[5]) PILImage.create(files[5]) . /root/.fastai/data/oxford-iiit-pet/images/Ragdoll_56.jpg . print(files[6]) PILImage.create(files[6]) . /root/.fastai/data/oxford-iiit-pet/images/german_shorthaired_2.jpg . print(files[7]) PILImage.create(files[7]) . /root/.fastai/data/oxford-iiit-pet/images/Egyptian_Mau_34.jpg . print(files[8]) PILImage.create(files[8]) . /root/.fastai/data/oxford-iiit-pet/images/japanese_chin_169.jpg . # 특1: 대문자이면 고양이, 소문자이면 강아지그림이다!! (천재적인 저장방식) # 특2: 이미지크기가 서로 다르다.. . def label_func(fname): if fname[0].isupper(): return &#39;cat&#39; else: return &#39;dog&#39; . dls = ImageDataLoaders.from_name_func(path,files,label_func,item_tfms=Resize(224)) # path 경로에서 # files 에 해당하는 파일들을 불러와서 X를 만들고 # item_tfms 에 정의된 방식으로 X를 변환하여 저장한다. 그리고 # label_func: &quot;파일이름&quot; -&gt; &quot;라벨&quot;, 에 저장된 함수내용을 바탕으로 y를 만들어 저장한다. # 이 모든것이 저장된 자료는 변수 dls에 저장한다. . dls.show_batch(max_n=16) . &#54617;&#49845; . # 우리의 1차 목표: 이미지 -&gt; 개/고양이 판단하는 모형을 채용하고, 그 모형에 데이터를 넣어서 학습하고, 그 모형의 결과를 판단하고 싶다. (즉 클래시파이어를 만든다는 소리) # 우리의 2차 목표: 그 모형에 &quot;새로운&quot; 자료를 전달하여 이미지를 분류할 것이다. (즉 클래시파이어를 쓴다는 소리) # cnn_learner 라는 함수를 이용해서 1차목표와 2차목표를 달성할 &quot;썸띵(Object)&quot;을 만들것임. ## 오브젝트란? 정보와 함수를 동시에 가지는 어떠한 집합체 # - 오브젝트.명사이름: 이것 통채로 하나의 변수처럼 쓸 수 있음. # - 오브젝트.동사이름: 이것 통채로 하나의 함수처럼 쓸 수 있음. (이때 함수의 첫번째 입력은 명시하지 않아도 오브젝트 그 자체가 된다) ## clafr에 필요한 명사(=정보) &lt;-- 우리가 넣어줘야하는 것들이 대부분 # (1) 모델정보: 클래시파이어로 누구를 뽑을것인가 (유명한 모델이 무엇인가? 잘 맞추는 모델이 무엇인가) # (2) 데이터: 데이터를 줘야함 # (3) 평가기준표: 채점을 할 지표 ## clafr에 필요한 동사(=함수) &lt;-- 이미 구현이 되어있음.. # (1) 학습 # (2) 결과를 판단 # (3) 예측 clsfr = cnn_learner(dls,resnet34,metrics=error_rate) # clsfr 라는 오브젝트를 만들건데.. # 그 오브젝트의 재료로 dls (데이터), resnet34 (데이터를 분석할 모형이름), metrics (모형의 성능을 평가할 기준) 를 넣음. . /usr/local/lib/python3.7/dist-packages/fastai/vision/learner.py:284: UserWarning: `cnn_learner` has been renamed to `vision_learner` -- please update your code warn(&#34;`cnn_learner` has been renamed to `vision_learner` -- please update your code&#34;) /usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter &#39;pretrained&#39; is deprecated since 0.13 and will be removed in 0.15, please use &#39;weights&#39; instead. f&#34;The parameter &#39;{pretrained_param}&#39; is deprecated since 0.13 and will be removed in 0.15, &#34; /usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for &#39;weights&#39; are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights. warnings.warn(msg) Downloading: &#34;https://download.pytorch.org/models/resnet34-b627a593.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth . clsfr.fine_tune(1) # 학습을 하는 함수 . epoch train_loss valid_loss error_rate time . 0 | 0.189062 | 0.012517 | 0.006089 | 01:01 | . epoch train_loss valid_loss error_rate time . 0 | 0.051309 | 0.010439 | 0.003383 | 00:57 | . &#44592;&#51316; &#45936;&#51060;&#53552;&#47484; &#51096; &#47582;&#52628;&#45716;&#51648; &#54869;&#51064; . files[0] # 강아지 . Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/scottish_terrier_92.jpg&#39;) . clsfr.predict(files[0]) . (&#39;dog&#39;, TensorBase(1), TensorBase([6.8846e-07, 1.0000e+00])) . files[7] # 고양이 . Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/Egyptian_Mau_34.jpg&#39;) . clsfr.predict(files[7]) . (&#39;cat&#39;, TensorBase(0), TensorBase([1.0000e+00, 1.3773e-08])) . clsfr.show_results() . &#50724;&#45813;&#48516;&#49437; . interpreter = Interpretation.from_learner(clsfr) # 오답을 분석하는 오브젝트를 만듬.. 재료는 클래시파이어! . interpreter.plot_top_losses(16) # 오답을 분석하는 오브젝트는 가장 오류가 높은 이미지를 정렬하여 보여주는 기능이 있음.. . &#51652;&#51676; &#51096;&#46104;&#45716;&#44172; &#47582;&#45716;&#44148;&#44032;? . clsfr.predict(files[7]) . (&#39;cat&#39;, TensorBase(0), TensorBase([1.0000e+00, 1.3773e-08])) . clsfr.predict(&#39;/root/.fastai/data/oxford-iiit-pet/images/Egyptian_Mau_34.jpg&#39;) . (&#39;cat&#39;, TensorBase(0), TensorBase([1.0000e+00, 1.3773e-08])) . clsfr.predict(PILImage.create(&#39;/root/.fastai/data/oxford-iiit-pet/images/Egyptian_Mau_34.jpg&#39;)) . (&#39;cat&#39;, TensorBase(0), TensorBase([1.0000e+00, 1.3773e-08])) . PILImage.create(&#39;2022-09-06-cat1.png&#39;) . clsfr.predict(PILImage.create(&#39;2022-09-06-cat1.png&#39;)) . (&#39;cat&#39;, TensorBase(0), TensorBase([1.0000e+00, 1.5662e-10])) . PILImage.create(&#39;2022-09-06-cat2.jpeg&#39;) . clsfr.predict(PILImage.create(&#39;2022-09-06-cat2.jpeg&#39;)) . (&#39;cat&#39;, TensorBase(0), TensorBase([0.9809, 0.0191])) . clsfr.predict(PILImage.create(&#39;2022-09-06-hani01.jpeg&#39;)) . (&#39;dog&#39;, TensorBase(1), TensorBase([3.2573e-10, 1.0000e+00])) . clsfr.predict(PILImage.create(&#39;2022-09-06-hani02.jpeg&#39;)) . (&#39;dog&#39;, TensorBase(1), TensorBase([7.0723e-07, 1.0000e+00])) . clsfr.predict(PILImage.create(&#39;2022-09-06-hani03.jpg&#39;)) . (&#39;dog&#39;, TensorBase(1), TensorBase([0.1814, 0.8186])) . &#49689;&#51228; . - 인터넷에 존재하는 개 혹은 고양이 이미지를 임의로 하나 불러온뒤 clsfr에 넣어보고 결과를 관찰하라. 관찰결과를 스크린샷하여 제출하라. . 숙제를 위한 예시코드 # https://dimg.donga.com/ugc/CDB/SHINDONGA/Article/5e/0d/9f/01/5e0d9f011a9ad2738de6.jpg &lt;-- 인터넷의 이미지 주소 img=PILImage.create(requests.get(&#39;https://dimg.donga.com/ugc/CDB/SHINDONGA/Article/5e/0d/9f/01/5e0d9f011a9ad2738de6.jpg&#39;).content) clsfr.predict(img) . | . - 숙제 못하겠으면 카톡으로 물어보세요! 답 알려드립니다. . - 숙제는 간단하게 편한 형식으로 제출하세요. (저는 스크린샷 선호해요..) pdf나 hwp로 만드실 필요 없습니다. .",
            "url": "https://guebin.github.io/STML2022/2022/09/07/(1%EC%A3%BC%EC%B0%A8)-9%EC%9B%946%EC%9D%BC.html",
            "relUrl": "/2022/09/07/(1%EC%A3%BC%EC%B0%A8)-9%EC%9B%946%EC%9D%BC.html",
            "date": " • Sep 7, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "(A1) 깃허브와 fastpages를 이용하여 블로그 개설하기",
            "content": "About this doc . - 본 포스트는 2021년 1학기 Python 입문 강의내용중 일부를 업로드 하였음. . - Github, fastpages를 사용하여 블로그를 개설하고 관리하는 방법에 대한 설명임. . .",
            "url": "https://guebin.github.io/STML2022/2021/08/17/(A1)-%EA%B9%83%ED%97%88%EB%B8%8C%EC%99%80-fastpages%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%98%EC%97%AC-%EB%B8%94%EB%A1%9C%EA%B7%B8-%EA%B0%9C%EC%84%A4%ED%95%98%EA%B8%B0.html",
            "relUrl": "/2021/08/17/(A1)-%EA%B9%83%ED%97%88%EB%B8%8C%EC%99%80-fastpages%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%98%EC%97%AC-%EB%B8%94%EB%A1%9C%EA%B7%B8-%EA%B0%9C%EC%84%A4%ED%95%98%EA%B8%B0.html",
            "date": " • Aug 17, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "최규빈 . guebin@jbnu.ac.kr | 자연과학대학교 본관 205호 | 카카오톡 오픈채널1 | . 2022년 2학기 종료 후 폐쇄 예정 &#8617; . |",
          "url": "https://guebin.github.io/STML2022/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://guebin.github.io/STML2022/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}